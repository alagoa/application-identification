{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyshark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema, find_peaks_cwt\n",
    "from scipy.stats import stats\n",
    "import numpy as np\n",
    "import re\n",
    "import pprint as pp\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict, defaultdict, deque\n",
    "from scalogram import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_info(up_pkts, down_pkts):\n",
    "    up_ordered = OrderedDict(sorted(up_pkts.items(), key=lambda t: t[0]))\n",
    "    down_ordered= OrderedDict(sorted(down_pkts.items(), key=lambda t: t[0]))    \n",
    "\n",
    "    result = {}\n",
    "    up = defaultdict(list)\n",
    "    down = defaultdict(list)\n",
    "    byte_count = defaultdict(list)\n",
    "\n",
    "    for pkts in up_ordered.values():\n",
    "        up['byte_count'].append(sum(int(pkt.ip.len) for pkt in pkts))\n",
    "        up['packet_count'].append(len(pkts))\n",
    "    for pkts in down_ordered.values():\n",
    "        down['byte_count'].append(sum(int(pkt.ip.len) for pkt in pkts))\n",
    "        down['packet_count'].append(len(pkts))\n",
    "    result['up'] = up\n",
    "    result['down'] = down\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redirect_packets(pkt):\n",
    "    print(\"Timestamp: \" + str(pkt.sniff_time.timestamp()) + \"\\tLen: \" + str(pkt.ip.len))\n",
    "    if hasattr(pkt, 'ip'):\n",
    "        global download, upload, batch_time, last_timestamp_down, last_timestamp_up, info\n",
    "        if (int(pkt.sniff_time.timestamp() * window) - batch_time) >= step:\n",
    "            info.append(get_info(upload, download))\n",
    "            batch_time = int(pkt.sniff_time.timestamp() * window)\n",
    "            download = defaultdict(list)\n",
    "            upload = defaultdict(list)\n",
    "            download[last_timestamp_up] = []\n",
    "            upload[last_timestamp_up] = []\n",
    "            last_timestamp_up = int(pkt.sniff_time.timestamp() * window)\n",
    "            last_timestamp_down = int(pkt.sniff_time.timestamp() * window)\n",
    "        if private_ip_pattern.match(pkt.ip.src.get_default_value()):\n",
    "            time_diff = int(pkt.sniff_time.timestamp() * window) - last_timestamp_up\n",
    "            if time_diff > 1:\n",
    "                for i in range(1, time_diff):\n",
    "                    upload[last_timestamp_up + i] = []\n",
    "            last_timestamp_up = int(pkt.sniff_time.timestamp() * window)\n",
    "            upload[int(pkt.sniff_time.timestamp() * window)].append(pkt)\n",
    "        elif private_ip_pattern.match(pkt.ip.dst.get_default_value()):\n",
    "            time_diff = int(pkt.sniff_time.timestamp() * window) - last_timestamp_down\n",
    "            if time_diff > 1:\n",
    "                for i in range(1, time_diff):\n",
    "                    download[last_timestamp_down + i] = []\n",
    "            last_timestamp_down = int(pkt.sniff_time.timestamp() * window)\n",
    "            download[int(pkt.sniff_time.timestamp() * window)].append(pkt)\n",
    "        else:\n",
    "            print(\"Curious!\\n\", pkt)\n",
    "    elif hasattr(pkt, 'ipv6'):\n",
    "        print(\"not yet implemented\")\n",
    "        # TODO\n",
    "    global count\n",
    "    print(count, end=\"\\r\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scalogram(data, scales):\n",
    "    \n",
    "    S,scales= scalogramCWT(data,scales)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_scalo(data, scales, colors):\n",
    "    for i in range (0, len(data)):\n",
    "        plt.plot(scales, data[i], colors[i], lw=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top X spikes from scalogram, sorted by value\n",
    "def get_spikes(scalo, comparator):\n",
    "    len(scalo)\n",
    "    spikes = deque([(-1,-1)] * 5, maxlen=5)\n",
    "    #aux = argrelextrema(scalo, comparator, order=int(len(scalo)/10))\n",
    "    aux = argrelextrema(scalo, comparator)\n",
    "    if aux[0].size:\n",
    "        for x in np.nditer(aux) or []:\n",
    "            spikes.append((scalo[x], scales[x]))\n",
    "    ordered = sorted(spikes, key=lambda x: x[1], reverse=True)\n",
    "    values = np.hstack(zip(*ordered))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "def get_stats_numpy(data):\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    std = np.std(data)\n",
    "    var = np.var(data)\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    pc = [25,50,75,90]\n",
    "    percentiles = np.array(np.percentile(data, pc))\n",
    "    silences = np.count_nonzero(np.asarray(data)==0.0)\n",
    "    silence_mean = np.mean(list(sum(1 for _ in g) for k, g in groupby(data) if k==0))\n",
    "    longest_silence = max(sum(1 for _ in g) for k, g in groupby(data) if k==0) if silences > 0 else 0\n",
    "    shortest_silence = min(sum(1 for _ in g) for k, g in groupby(data) if k==0) if silences > 0 else 0\n",
    "    \n",
    "    #print(\"Mean: \" + str(mean))\n",
    "    #print(\"Media: \" + str(median))\n",
    "    #print(\"StdDev: \" + str(std))\n",
    "    #print(\"Variance: \" + str(var))\n",
    "    #print(\"Skewness: \" + str(skew))\n",
    "    #print(\"Kurtosis: \" + str(kurt))\n",
    "    #print(\"Pc25: \" + str(percentiles[0]))\n",
    "    #print(\"Pc50: \" + str(percentiles[1]))\n",
    "    #print(\"Pc75: \" + str(percentiles[2]))\n",
    "    \n",
    "    features = np.hstack((mean, median, std, var, skew, kurt, percentiles, silences, silence_mean, longest_silence, shortest_silence))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_json(data):\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    std = np.std(data)\n",
    "    var = np.var(data)\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    pc = [25,50,75]\n",
    "    percentiles = np.array(np.percentile(data, pc))\n",
    "    silences = np.count_nonzero(np.asarray(data)==0.0)\n",
    "    longest_silence = max(sum(1 for _ in g) for k, g in groupby(data) if k==0) if silences > 0 else 0\n",
    "    shortest_silence = min(sum(1 for _ in g) for k, g in groupby(data) if k==0) if silences > 0 else 0\n",
    "    #print(\"Mean: \" + str(mean))\n",
    "    #print(\"Media: \" + str(median))\n",
    "    #print(\"StdDev: \" + str(std))\n",
    "    #print(\"Variance: \" + str(var))\n",
    "    #print(\"Skewness: \" + str(skew))\n",
    "    #print(\"Kurtosis: \" + str(kurt))\n",
    "    #print(\"Pc25: \" + str(percentiles[0]))\n",
    "    #print(\"Pc50: \" + str(percentiles[1]))\n",
    "    #print(\"Pc75: \" + str(percentiles[2]))\n",
    "    \n",
    "    statistiscs = {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'std': std,\n",
    "        'var': var,\n",
    "        'skew': skew,\n",
    "        'kurt': kurt,\n",
    "        'pc25': percentiles[0],\n",
    "        'pc50': percentiles[1],\n",
    "        'pc75': percentiles[2],\n",
    "    }\n",
    "    \n",
    "    return statistiscs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all on a numpy array\n",
    "def get_features_numpy(info, local_max_up_bytes, local_min_up_bytes, local_max_up_packet, local_min_up_packet, \n",
    "                       local_max_down_bytes, local_min_down_bytes, local_max_down_packet, local_min_down_packet):\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for idx in range(0, len(info)):\n",
    "        result.append(\n",
    "            np.hstack(\n",
    "                (\n",
    "                 get_stats_numpy(info[idx]['up']['byte_count']),\n",
    "                 get_stats_numpy(info[idx]['up']['packet_count']),\n",
    "                 local_max_up_bytes[idx], local_min_up_bytes[idx],\n",
    "                 local_max_up_packet[idx], local_min_up_packet[idx],\n",
    "                 get_stats_numpy(info[idx]['down']['byte_count']),\n",
    "                 get_stats_numpy(info[idx]['down']['packet_count']),\n",
    "                 local_max_down_bytes[idx], local_min_down_bytes[idx],\n",
    "                 local_max_down_packet[idx], local_min_down_packet[idx],\n",
    "\n",
    "            ))\n",
    "        )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all on a json\n",
    "def get_features_json(info):\n",
    "    stat = {\n",
    "        'down': defaultdict(list),\n",
    "        'up': defaultdict(list)\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "    for idx in range(0, len(info)):\n",
    "        stat['down']['byte_count'] = get_stats_json(info[idx]['down']['byte_count'])\n",
    "        stat['down']['packet_count'] = get_stats_json(info[idx]['down']['packet_count'])\n",
    "        stat['down']['byte_count']['scalo_spikes_max'] =  local_max_down_bytes[idx]\n",
    "        stat['down']['byte_count']['scalo_spikes_min'] =  local_min_down_bytes[idx]\n",
    "        stat['down']['packet_count']['scalo_spikes_max'] =  local_max_down_packet[idx]\n",
    "        stat['down']['packet_count']['scalo_spikes_min'] =  local_min_down_packet[idx]\n",
    "\n",
    "        stat['up']['byte_count'] = get_stats_json(info[idx]['up']['byte_count'])\n",
    "        stat['up']['packet_count'] = get_stats_json(info[idx]['up']['packet_count'])\n",
    "        stat['up']['byte_count']['scalo_spikes_max'] =  local_max_up_bytes[idx]\n",
    "        stat['up']['byte_count']['scalo_spikes_min'] =  local_min_up_packet[idx]\n",
    "        stat['up']['packet_count']['scalo_spikes_max'] =  local_max_up_packet[idx]\n",
    "        stat['up']['packet_count']['scalo_spikes_min'] =  local_min_up_packet[idx]\n",
    "\n",
    "\n",
    "        result.append(stat)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcap_to_csv(path, filename, cap, batch_time, last_timestamp_up, last_timestamp_down, scales):\n",
    "    global download, upload\n",
    "    download = defaultdict(list)\n",
    "    upload = defaultdict(list)\n",
    "    download[last_timestamp_up] = []\n",
    "    upload[last_timestamp_up] = []\n",
    "    \n",
    "    cap.apply_on_packets(redirect_packets)\n",
    "    global info\n",
    "    pp.pprint(info)\n",
    "    \n",
    "    scalos_up = []\n",
    "    scalos_down = []\n",
    "\n",
    "    for idx, sample in enumerate(info):\n",
    "        scalos_up.append(\n",
    "            (calc_scalogram(np.asarray(sample['up']['byte_count']), scales),\n",
    "             calc_scalogram(np.asarray(sample['up']['packet_count']), scales))\n",
    "        )\n",
    "        scalos_down.append(\n",
    "            (calc_scalogram(np.asarray(sample['down']['byte_count']), scales),\n",
    "             calc_scalogram(np.asarray(sample['down']['packet_count']), scales))\n",
    "        )\n",
    "    #    show_scalo([scalos_down[idx], scalos_up[idx]], scales, ['r', 'b'])\n",
    "    #smooth_down = np.convolve(scalo_down, np.ones(len(scalo_down)), mode='same')\n",
    "    #smooth_up = np.convolve(scalo_up, np.ones(len(scalo_up)), mode='same')\n",
    "    #show_scalo([smooth_down, smooth_up], scales, ['r', 'b'])\n",
    "\n",
    "    #scalo, scales = calc_and_show(np.asarray(stats['down']['packet_count']), 'r')\n",
    "    #scalo, scales = calc_and_show(np.asarray(stats['up']['packet_count']), 'b')\n",
    "    \n",
    "    local_max_up_bytes = []\n",
    "    local_min_up_bytes = []\n",
    "    local_max_up_packet = []\n",
    "    local_min_up_packet = []\n",
    "    local_max_down_bytes = []\n",
    "    local_min_down_bytes = []\n",
    "    local_max_down_packet = []\n",
    "    local_min_down_packet = []\n",
    "\n",
    "\n",
    "    for scalo in scalos_up:\n",
    "        local_max_up_bytes.append(get_spikes(scalo[0], np.greater))\n",
    "        local_min_up_bytes.append(get_spikes(scalo[0], np.less))\n",
    "        local_max_up_packet.append(get_spikes(scalo[1], np.greater))\n",
    "        local_min_up_packet.append(get_spikes(scalo[1], np.less))\n",
    "\n",
    "    for scalo in scalos_down:\n",
    "        local_max_down_bytes.append(get_spikes(scalo[0], np.greater))\n",
    "        local_min_down_bytes.append(get_spikes(scalo[0], np.less))\n",
    "        local_max_down_packet.append(get_spikes(scalo[1], np.greater))\n",
    "        local_min_down_packet.append(get_spikes(scalo[1], np.less))\n",
    "        \n",
    "    import pandas as pd\n",
    "\n",
    "    samples = get_features_numpy(info, local_max_up_bytes, local_min_up_bytes, local_max_up_packet, local_min_up_packet,\n",
    "                                local_max_down_bytes, local_min_down_bytes, local_max_down_packet, local_min_down_packet)\n",
    "\n",
    "    names = [\n",
    "        'up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var', 'up_bytes_skew', 'up_bytes_kurt',\n",
    "        'up_bytes_perc25', 'up_bytes_perc50', 'up_bytes_perc75', 'up_bytes_perc90',\n",
    "        'up_bytes_silences', 'up_bytes_silence_mean', 'up_bytes_longest_silence', 'up_bytes_shortest_silence',\n",
    "        'up_packet_mean', 'up_packet_median', 'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
    "        'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75', 'up_packet_perc90',\n",
    "        'up_packet_silences', 'up_packet_silence_mean', 'up_packet_longest_silence', 'up_packet_shortest_silence',\n",
    "        'up_bytes_1max_y', 'up_bytes_2max_y', 'up_bytes_3max_y', 'up_bytes_4max_y', 'up_bytes_5max_y',\n",
    "        'up_bytes_1max_x', 'up_bytes_2max_x', 'up_bytes_3max_x', 'up_bytes_4max_x', 'up_bytes_5max_x',\n",
    "        'up_bytes_1min_y', 'up_bytes_2min_y', 'up_bytes_3min_y', 'up_bytes_4min_y', 'up_bytes_5min_y',\n",
    "        'up_bytes_1min_x', 'up_bytes_2min_x', 'up_bytes_3min_x', 'up_bytes_4min_x', 'up_bytes_5min_x',\n",
    "        'up_packet_1max_y', 'up_packet_2max_y', 'up_packet_3max_y', 'up_packet_4max_y', 'up_packet_5max_y',\n",
    "        'up_packet_1max_x', 'up_packet_2max_x', 'up_packet_3max_x', 'up_packet_4max_x', 'up_packet_5max_x',\n",
    "        'up_packet_1min_y', 'up_packet_2min_y', 'up_packet_3min_y', 'up_packet_4min_y', 'up_packet_5min_y',\n",
    "        'up_packet_1min_x', 'up_packet_2min_x', 'up_packet_3min_x', 'up_packet_4min_x', 'up_packet_5min_x',\n",
    "\n",
    "        'down_bytes_mean', 'down_bytes_median', 'down_bytes_std', 'down_bytes_var', 'down_bytes_skew', 'down_bytes_kurt',\n",
    "        'down_bytes_perc25', 'down_bytes_perc50', 'down_bytes_perc75', 'down_bytes_perc90',\n",
    "        'down_bytes_silences', 'down_bytes_silence_mean', 'down_bytes_longest_silence', 'down_bytes_shortest_silence',\n",
    "        'down_packet_mean', 'down_packet_median', 'down_packet_std', 'down_packet_var', 'down_packet_skew', 'down_packet_kurt',\n",
    "        'down_packet_perc25', 'down_packet_perc50', 'down_packet_perc75', 'down_packet_perc90',\n",
    "        'down_packet_silences', 'down_packet_silence_mean', 'down_packet_longest_silence', 'down_packet_shortest_silence',\n",
    "        'down_bytes_1max_y', 'down_bytes_2max_y', 'down_bytes_3max_y', 'down_bytes_4max_y', 'down_bytes_5max_y',\n",
    "        'down_bytes_1max_x', 'down_bytes_2max_x', 'down_bytes_3max_x', 'down_bytes_4max_x', 'down_bytes_5max_x',\n",
    "        'down_bytes_1min_y', 'down_bytes_2min_y', 'down_bytes_3min_y', 'down_bytes_4min_y', 'down_bytes_5min_y',\n",
    "        'down_bytes_1min_x', 'down_bytes_2min_x', 'down_bytes_3min_x', 'down_bytes_4min_x', 'down_bytes_5min_x',\n",
    "        'down_packet_1max_y', 'down_packet_2max_y', 'down_packet_3max_y', 'down_packet_4max_y', 'down_packet_5max_y',\n",
    "        'down_packet_1max_x', 'down_packet_2max_x', 'down_packet_3max_x', 'down_packet_4max_x', 'down_packet_5max_x',\n",
    "        'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_3min_y', 'down_packet_4min_y', 'down_packet_5min_y',\n",
    "        'down_packet_1min_x', 'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x'\n",
    "    ]\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(samples, columns=names)\n",
    "    \n",
    "    # Not necessary to have silences in both 'bytes' and 'packet'\n",
    "    df.drop(columns=['down_packet_silences', 'up_packet_silences', 'up_packet_longest_silence', 'up_packet_shortest_silence',\n",
    "                    'up_packet_silences', 'down_packet_longest_silence', 'down_packet_shortest_silence'], inplace=True)\n",
    "    #df.describe()\n",
    "    \n",
    "    #f['label'] = os.path.basename(path)\n",
    "    #ut = 'csv/30s1s/' + filename.split('.')[0] + '.csv'\n",
    "    #f.to_csv(out, sep=',', encoding='utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9096b7f63e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_pcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../../shared/normal/video/twitch/pcap/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprivate_ip_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(^127\\.)|(^10\\.)|(^172\\.1[6-9]\\.)|(^172\\.2[0-9]\\.)|(^172\\.3[0-1]\\.)|(^192\\.168\\.)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;31m# batches of 30 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# 1 second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_pcap = \"../../../shared/normal/video/twitch/pcap/\"\n",
    "private_ip_pattern = re.compile(\"(^127\\.)|(^10\\.)|(^172\\.1[6-9]\\.)|(^172\\.2[0-9]\\.)|(^172\\.3[0-1]\\.)|(^192\\.168\\.)\")\n",
    "step = 30 # batches of 30 seconds\n",
    "window = 1 # 1 second\n",
    "window = 1/window\n",
    "\n",
    "#Scalogram\n",
    "N = step\n",
    "dj=1/128\n",
    "s0=2\n",
    "J=1/dj * np.log2(0.5*N/s0)\n",
    "scales=s0*2**(np.arange(J)*dj)\n",
    "\n",
    "for path, subdirs, files in os.walk(base_pcap):\n",
    "    for name in files:\n",
    "        if name != 'twitch1.pcapng':\n",
    "            continue\n",
    "        info = []\n",
    "        count = 0\n",
    "\n",
    "        cap = ps.FileCapture(os.path.join(path, name))\n",
    "        \n",
    "        batch_time = int(cap[0].sniff_time.timestamp() * window)\n",
    "        last_timestamp_up = int(cap[0].sniff_time.timestamp() * window)\n",
    "        last_timestamp_down = int(cap[0].sniff_time.timestamp() * window)\n",
    "        \n",
    "        download = defaultdict(list)\n",
    "        upload = defaultdict(list)\n",
    "        download[last_timestamp_up] = []\n",
    "        upload[last_timestamp_up] = []\n",
    "        \n",
    "        print(os.path.join(str(path), str(name)))\n",
    "        pcap_to_csv(path, name, cap, batch_time, last_timestamp_up, last_timestamp_down, scales)\n",
    "#cap = ps.FileCapture(base_pcap + \"netflix4.pcapng\")\n",
    "#cap.load_packets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51,\n",
       " 34,\n",
       " 68,\n",
       " 1,\n",
       " 20,\n",
       " 59,\n",
       " 30,\n",
       " 46,\n",
       " 9,\n",
       " 61,\n",
       " 22,\n",
       " 62,\n",
       " 18,\n",
       " 69,\n",
       " 36,\n",
       " 23,\n",
       " 21,\n",
       " 112,\n",
       " 28,\n",
       " 62,\n",
       " 65,\n",
       " 118,\n",
       " 18,\n",
       " 53,\n",
       " 78,\n",
       " 170,\n",
       " 1,\n",
       " 110,\n",
       " 22,\n",
       " 56]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " info[0]['down']['packet_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
