{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_number(s):\n",
    "    return list(filter(None, re.split(r'(\\d+)', s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_stats = [\n",
    "    'up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var', 'up_bytes_skew', 'up_bytes_kurt',\n",
    "    'up_bytes_perc25', 'up_bytes_perc50', 'up_bytes_perc75', 'up_bytes_perc90',\n",
    "    'up_packet_mean', 'up_packet_median', 'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
    "    'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75', 'up_packet_perc90',\n",
    "    'down_bytes_mean', 'down_bytes_median', 'down_bytes_std', 'down_bytes_var', 'down_bytes_skew', 'down_bytes_kurt',\n",
    "    'down_bytes_perc25', 'down_bytes_perc50', 'down_bytes_perc75', 'down_bytes_perc90',\n",
    "    'down_packet_mean', 'down_packet_median', 'down_packet_std', 'down_packet_var', 'down_packet_skew', 'down_packet_kurt',\n",
    "    'down_packet_perc25', 'down_packet_perc50', 'down_packet_perc75', 'down_packet_perc90']\n",
    "\n",
    "silences = ['down_bytes_silences', 'down_bytes_silence_mean', 'down_bytes_longest_silence', 'down_bytes_shortest_silence',\n",
    "           'up_bytes_silences', 'up_bytes_silence_mean', 'up_bytes_longest_silence', 'up_bytes_shortest_silence']\n",
    "\n",
    "scalogram = ['up_bytes_1max_y', 'up_bytes_2max_y', 'up_bytes_3max_y', 'up_bytes_4max_y', 'up_bytes_5max_y',\n",
    "    'up_bytes_1max_x', 'up_bytes_2max_x', 'up_bytes_3max_x', 'up_bytes_4max_x', 'up_bytes_5max_x',\n",
    "    'up_bytes_1min_y', 'up_bytes_2min_y', 'up_bytes_3min_y', 'up_bytes_4min_y', 'up_bytes_5min_y',\n",
    "    'up_bytes_1min_x', 'up_bytes_2min_x', 'up_bytes_3min_x', 'up_bytes_4min_x', 'up_bytes_5min_x',\n",
    "    'up_packet_1max_y', 'up_packet_2max_y', 'up_packet_3max_y', 'up_packet_4max_y', 'up_packet_5max_y',\n",
    "    'up_packet_1max_x', 'up_packet_2max_x', 'up_packet_3max_x', 'up_packet_4max_x', 'up_packet_5max_x',\n",
    "    'up_packet_1min_y', 'up_packet_2min_y', 'up_packet_2min_y', 'up_packet_4min_y', 'up_packet_5min_y',\n",
    "    'up_packet_1min_x', 'up_packet_2min_x', 'up_packet_3min_x', 'up_packet_4min_x', 'up_packet_5min_x',\n",
    "    'down_bytes_1max_y', 'down_bytes_2max_y', 'down_bytes_3max_y', 'down_bytes_4max_y', 'down_bytes_5max_y',\n",
    "    'down_bytes_1max_x', 'down_bytes_2max_x', 'down_bytes_3max_x', 'down_bytes_4max_x', 'down_bytes_5max_x',\n",
    "    'down_bytes_1min_y', 'down_bytes_2min_y', 'down_bytes_3min_y', 'down_bytes_4min_y', 'down_bytes_5min_y',\n",
    "    'down_bytes_1min_x', 'down_bytes_2min_x', 'down_bytes_3min_x', 'down_bytes_4min_x', 'down_bytes_5min_x',\n",
    "    'down_packet_1max_y', 'down_packet_2max_y', 'down_packet_3max_y', 'down_packet_4max_y', 'down_packet_5max_y',\n",
    "    'down_packet_1max_x', 'down_packet_2max_x', 'down_packet_3max_x', 'down_packet_4max_x', 'down_packet_5max_x',\n",
    "    'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_2min_y', 'down_packet_4min_y', 'down_packet_5min_y',\n",
    "    'down_packet_1min_x', 'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var',\n",
      "       'up_bytes_skew', 'up_bytes_kurt', 'up_bytes_perc25', 'up_bytes_perc50',\n",
      "       'up_bytes_perc75', 'up_bytes_perc90',\n",
      "       ...\n",
      "       'down_packet_2min_y', 'down_packet_3min_y', 'down_packet_4min_y',\n",
      "       'down_packet_5min_y', 'down_packet_1min_x', 'down_packet_2min_x',\n",
      "       'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x',\n",
      "       'label'],\n",
      "      dtype='object', length=133)\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_folder = \"csv/30s0.10s/\"\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for path, subdirs, files in os.walk(base_folder):\n",
    "    for name in files:\n",
    "        data = pd.read_csv(os.path.join(str(path), str(name)))\n",
    "        if(split_number(name)[0] == 'cap'):\n",
    "            continue\n",
    "        data['label'] = split_number(name)[0]\n",
    "        dataset = pd.concat([dataset, data])\n",
    "dataset = dataset.drop(columns=['Unnamed: 0']).reset_index()\n",
    "dataset.drop(columns=['index', 'up_packet_silence_mean', 'down_packet_silence_mean',\n",
    "                      'down_packet_longest_silence', 'down_packet_shortest_silence'], inplace=True)\n",
    "features = dataset.columns\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "netflix      5731\n",
       "youtube      4653\n",
       "acestream    2514\n",
       "twitch       2294\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset['label'] == 'netflix-ssh', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'youtube-ssh', 'label'] = 'youtube'\n",
    "dataset.loc[dataset['label'] == 'twitch-ssh.csv', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'netflix-openvpn.csv', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'youtube-openvpn.csv', 'label'] = 'youtube'\n",
    "dataset.loc[dataset['label'] == 'twitch-openvpn.csv', 'label'] = 'twitch'\n",
    "dataset.loc[dataset['label'] == 'acestream-openvpn.csv', 'label'] = 'acestream'\n",
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var',\n",
       "       'up_bytes_skew', 'up_bytes_kurt', 'up_bytes_perc25', 'up_bytes_perc50',\n",
       "       'up_bytes_perc75', 'up_bytes_perc90',\n",
       "       ...\n",
       "       'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_3min_y',\n",
       "       'down_packet_4min_y', 'down_packet_5min_y', 'down_packet_1min_x',\n",
       "       'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x',\n",
       "       'down_packet_5min_x'],\n",
       "      dtype='object', length=132)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test some stuff\n",
    "#dataset.drop(columns=silences, inplace=True)\n",
    "dataset.columns\n",
    "features = dataset.columns[:-1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "5        0\n",
       "6        0\n",
       "7        0\n",
       "8        0\n",
       "9        0\n",
       "10       0\n",
       "11       0\n",
       "12       0\n",
       "13       0\n",
       "14       0\n",
       "15       0\n",
       "16       0\n",
       "17       0\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       0\n",
       "29       0\n",
       "        ..\n",
       "15162    2\n",
       "15163    2\n",
       "15164    2\n",
       "15165    2\n",
       "15166    2\n",
       "15167    2\n",
       "15168    2\n",
       "15169    2\n",
       "15170    2\n",
       "15171    2\n",
       "15172    2\n",
       "15173    2\n",
       "15174    2\n",
       "15175    2\n",
       "15176    2\n",
       "15177    2\n",
       "15178    2\n",
       "15179    2\n",
       "15180    2\n",
       "15181    2\n",
       "15182    2\n",
       "15183    2\n",
       "15184    2\n",
       "15185    2\n",
       "15186    2\n",
       "15187    2\n",
       "15188    2\n",
       "15189    2\n",
       "15190    2\n",
       "15191    2\n",
       "Name: label, Length: 15192, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevlabel = dataset['label']\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "labels = dataset['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                          1.000000\n",
       "down_bytes_silence_mean        0.434830\n",
       "up_bytes_silence_mean          0.432630\n",
       "up_bytes_shortest_silence      0.287114\n",
       "up_packet_shortest_silence     0.287114\n",
       "down_bytes_shortest_silence    0.282086\n",
       "up_bytes_perc25                0.236005\n",
       "up_bytes_median                0.197101\n",
       "up_bytes_perc50                0.197101\n",
       "up_packet_longest_silence      0.189311\n",
       "up_bytes_longest_silence       0.189311\n",
       "down_packet_perc25             0.184737\n",
       "down_bytes_longest_silence     0.178033\n",
       "down_bytes_perc25              0.176851\n",
       "up_packet_perc25               0.159010\n",
       "down_packet_median             0.136892\n",
       "down_packet_perc50             0.136892\n",
       "up_packet_perc50               0.132694\n",
       "up_packet_median               0.132694\n",
       "down_bytes_median              0.114756\n",
       "down_bytes_perc50              0.114756\n",
       "up_bytes_perc75                0.023395\n",
       "up_packet_perc75               0.022153\n",
       "up_bytes_silences              0.014750\n",
       "up_packet_silences             0.014750\n",
       "down_bytes_perc75              0.005765\n",
       "down_packet_perc75             0.003739\n",
       "down_packet_mean               0.000014\n",
       "down_bytes_mean                0.000006\n",
       "up_packet_mean                -0.000030\n",
       "                                 ...   \n",
       "down_bytes_2max_x             -0.344978\n",
       "up_bytes_2max_x               -0.350711\n",
       "up_packet_2max_x              -0.360655\n",
       "down_bytes_2min_x             -0.362276\n",
       "up_bytes_2min_x               -0.378229\n",
       "up_packet_2min_x              -0.382569\n",
       "down_packet_1min_x            -0.403007\n",
       "down_packet_2max_y            -0.404585\n",
       "down_packet_2min_y            -0.405459\n",
       "down_packet_1max_x            -0.429387\n",
       "down_packet_1max_y            -0.449365\n",
       "down_packet_1min_y            -0.452590\n",
       "up_packet_1min_x              -0.454912\n",
       "down_bytes_2max_y             -0.457920\n",
       "down_bytes_2min_y             -0.459395\n",
       "up_bytes_1max_x               -0.463259\n",
       "up_packet_2max_y              -0.463604\n",
       "up_packet_2min_y              -0.464564\n",
       "up_bytes_1min_x               -0.464727\n",
       "down_bytes_1min_x             -0.465396\n",
       "up_packet_1max_x              -0.474831\n",
       "up_bytes_2max_y               -0.475410\n",
       "up_bytes_2min_y               -0.475946\n",
       "down_bytes_1max_x             -0.494420\n",
       "up_bytes_1max_y               -0.502138\n",
       "up_bytes_1min_y               -0.502409\n",
       "up_packet_1max_y              -0.503076\n",
       "up_packet_1min_y              -0.505477\n",
       "down_bytes_1max_y             -0.519025\n",
       "down_bytes_1min_y             -0.522577\n",
       "Name: label, Length: 133, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()['label'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer for NaN\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(dataset)\n",
    "dataset = pd.DataFrame(imputer.transform(dataset), columns=dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.preprocessing import normalize\\n# Normalize data\\ndataset_no_label = dataset.loc[:, dataset.columns != 'label']\\n\\n#dataset = (dataset_no_label - dataset_no_label.mean()) / (dataset_no_label.max() - dataset_no_label.min())\\ndataset_normalized = normalize(dataset_no_label)\\ndataset = pd.DataFrame(dataset_normalized, columns=features)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import normalize\n",
    "# Normalize data\n",
    "dataset_no_label = dataset.loc[:, dataset.columns != 'label']\n",
    "\n",
    "#dataset = (dataset_no_label - dataset_no_label.mean()) / (dataset_no_label.max() - dataset_no_label.min())\n",
    "dataset_normalized = normalize(dataset_no_label)\n",
    "dataset = pd.DataFrame(dataset_normalized, columns=features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components=10)\\nmain_components = pca.fit_transform(dataset)\\ndataset = pd.DataFrame(data = main_components)\\ndataset['label'] = labels\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "main_components = pca.fit_transform(dataset)\n",
    "dataset = pd.DataFrame(data = main_components)\n",
    "dataset['label'] = labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = train['label']\n",
    "x_train = train.drop(columns=['label'])\n",
    "\n",
    "y_test = test['label']\n",
    "x_test = test.drop(columns=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_youtube = (y_train == 0)\n",
    "y_train_netflix = (y_train == 1)\n",
    "y_train_twitch = (y_train == 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_bytes_mean</th>\n",
       "      <th>up_bytes_median</th>\n",
       "      <th>up_bytes_std</th>\n",
       "      <th>up_bytes_var</th>\n",
       "      <th>up_bytes_skew</th>\n",
       "      <th>up_bytes_kurt</th>\n",
       "      <th>up_bytes_perc25</th>\n",
       "      <th>up_bytes_perc50</th>\n",
       "      <th>up_bytes_perc75</th>\n",
       "      <th>up_bytes_perc90</th>\n",
       "      <th>...</th>\n",
       "      <th>down_packet_1min_y</th>\n",
       "      <th>down_packet_2min_y</th>\n",
       "      <th>down_packet_3min_y</th>\n",
       "      <th>down_packet_4min_y</th>\n",
       "      <th>down_packet_5min_y</th>\n",
       "      <th>down_packet_1min_x</th>\n",
       "      <th>down_packet_2min_x</th>\n",
       "      <th>down_packet_3min_x</th>\n",
       "      <th>down_packet_4min_x</th>\n",
       "      <th>down_packet_5min_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10200</th>\n",
       "      <td>-0.079130</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>3.386415e-01</td>\n",
       "      <td>1.146781e-01</td>\n",
       "      <td>5.189428</td>\n",
       "      <td>24.965753</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.008896</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.539757</td>\n",
       "      <td>8.630086</td>\n",
       "      <td>6.800425</td>\n",
       "      <td>3.935542</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13412</th>\n",
       "      <td>-0.107161</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>4.092421e-01</td>\n",
       "      <td>1.674791e-01</td>\n",
       "      <td>2.671848</td>\n",
       "      <td>6.338245</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.092536</td>\n",
       "      <td>0.121833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013330</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.561446</td>\n",
       "      <td>7.336032</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14671</th>\n",
       "      <td>0.226701</td>\n",
       "      <td>-0.174761</td>\n",
       "      <td>9.336432e-01</td>\n",
       "      <td>8.716896e-01</td>\n",
       "      <td>0.789465</td>\n",
       "      <td>-0.249575</td>\n",
       "      <td>-0.601404</td>\n",
       "      <td>-0.174761</td>\n",
       "      <td>0.802801</td>\n",
       "      <td>1.218680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>4.604757</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>-0.578880</td>\n",
       "      <td>-0.706428</td>\n",
       "      <td>2.063335e-01</td>\n",
       "      <td>4.257351e-02</td>\n",
       "      <td>1.247258</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>-0.726231</td>\n",
       "      <td>-0.706428</td>\n",
       "      <td>-0.450378</td>\n",
       "      <td>-0.322992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.814342</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>4.433474</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12451</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>1.446792</td>\n",
       "      <td>1.243354</td>\n",
       "      <td>1.704337e+00</td>\n",
       "      <td>2.904765e+00</td>\n",
       "      <td>1.428606</td>\n",
       "      <td>1.797063</td>\n",
       "      <td>0.027553</td>\n",
       "      <td>1.243354</td>\n",
       "      <td>2.139456</td>\n",
       "      <td>3.579851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>12.745137</td>\n",
       "      <td>8.354190</td>\n",
       "      <td>6.441961</td>\n",
       "      <td>4.199776</td>\n",
       "      <td>2.579672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6824</th>\n",
       "      <td>-0.172218</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>5.152611e-02</td>\n",
       "      <td>2.654940e-03</td>\n",
       "      <td>3.478627</td>\n",
       "      <td>10.112128</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>3.809727</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>-0.106012</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>9.486510e-02</td>\n",
       "      <td>8.999387e-03</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.106012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>0.017239</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>5.482904e-01</td>\n",
       "      <td>3.006224e-01</td>\n",
       "      <td>3.517940</td>\n",
       "      <td>10.605252</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.115549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>-0.052365</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>2.674398e-01</td>\n",
       "      <td>7.152402e-02</td>\n",
       "      <td>3.278747</td>\n",
       "      <td>9.655424</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.091012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.583479</td>\n",
       "      <td>7.140066</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6423</th>\n",
       "      <td>0.096918</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>1.036407e+00</td>\n",
       "      <td>1.074139e+00</td>\n",
       "      <td>2.357437</td>\n",
       "      <td>4.352739</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>1.741536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>1.035536</td>\n",
       "      <td>1.204041</td>\n",
       "      <td>1.126177e+00</td>\n",
       "      <td>1.268274e+00</td>\n",
       "      <td>0.534846</td>\n",
       "      <td>-0.127047</td>\n",
       "      <td>-0.111013</td>\n",
       "      <td>1.204041</td>\n",
       "      <td>1.581805</td>\n",
       "      <td>2.092415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.138872</td>\n",
       "      <td>9.209514</td>\n",
       "      <td>6.004056</td>\n",
       "      <td>2.813152</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>0.744933</td>\n",
       "      <td>0.393486</td>\n",
       "      <td>1.138062e+00</td>\n",
       "      <td>1.295185e+00</td>\n",
       "      <td>1.284652</td>\n",
       "      <td>1.732834</td>\n",
       "      <td>-0.129568</td>\n",
       "      <td>0.393486</td>\n",
       "      <td>1.352132</td>\n",
       "      <td>2.018318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.018744</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>9.259521</td>\n",
       "      <td>5.417022</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>-0.165610</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.672593e-01</td>\n",
       "      <td>7.142754e-02</td>\n",
       "      <td>5.199469</td>\n",
       "      <td>25.034483</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.915094</td>\n",
       "      <td>7.257009</td>\n",
       "      <td>3.648207</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>-0.235707</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>4.944251e-02</td>\n",
       "      <td>2.444562e-03</td>\n",
       "      <td>5.199469</td>\n",
       "      <td>25.034483</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.009141</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>9.110309</td>\n",
       "      <td>6.168843</td>\n",
       "      <td>4.362031</td>\n",
       "      <td>3.327353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>-0.066480</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>2.760103e-01</td>\n",
       "      <td>7.618166e-02</td>\n",
       "      <td>3.511800</td>\n",
       "      <td>10.430744</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>8.630086</td>\n",
       "      <td>4.705588</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10434</th>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>-0.111469</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>7.586769e-01</td>\n",
       "      <td>5.755906e-01</td>\n",
       "      <td>1.375917</td>\n",
       "      <td>0.486632</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>0.107275</td>\n",
       "      <td>1.351853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.934862</td>\n",
       "      <td>3.002028</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8299</th>\n",
       "      <td>-0.036418</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>4.516375e-01</td>\n",
       "      <td>2.039765e-01</td>\n",
       "      <td>3.644875</td>\n",
       "      <td>11.706503</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.152899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006217</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.819061</td>\n",
       "      <td>7.063154</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12194</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13254</th>\n",
       "      <td>0.021080</td>\n",
       "      <td>-0.278323</td>\n",
       "      <td>5.107240e-01</td>\n",
       "      <td>2.608390e-01</td>\n",
       "      <td>1.529409</td>\n",
       "      <td>0.755960</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.278323</td>\n",
       "      <td>0.093251</td>\n",
       "      <td>1.037905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011902</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.375141</td>\n",
       "      <td>5.687570</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13133</th>\n",
       "      <td>-0.125453</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>4.474782e-01</td>\n",
       "      <td>2.002367e-01</td>\n",
       "      <td>2.388625</td>\n",
       "      <td>4.032556</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>0.398656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009781</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>-0.297805</td>\n",
       "      <td>-0.587556</td>\n",
       "      <td>7.111195e-01</td>\n",
       "      <td>5.056909e-01</td>\n",
       "      <td>2.733056</td>\n",
       "      <td>7.711204</td>\n",
       "      <td>-0.676266</td>\n",
       "      <td>-0.587556</td>\n",
       "      <td>-0.403423</td>\n",
       "      <td>0.498857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10.318687</td>\n",
       "      <td>4.604757</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>0.919651</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>2.930435e+00</td>\n",
       "      <td>8.587452e+00</td>\n",
       "      <td>3.312911</td>\n",
       "      <td>10.782022</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>0.389587</td>\n",
       "      <td>2.315089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.138872</td>\n",
       "      <td>9.061108</td>\n",
       "      <td>4.994404</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>1.232595e-32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>-0.072641</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>2.898558e-01</td>\n",
       "      <td>8.401641e-02</td>\n",
       "      <td>3.874849</td>\n",
       "      <td>13.857175</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>4.457547</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>-0.046688</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>2.979990e-01</td>\n",
       "      <td>8.880342e-02</td>\n",
       "      <td>3.155160</td>\n",
       "      <td>8.574877</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.089774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>8.771433</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>1.047699</td>\n",
       "      <td>1.115511</td>\n",
       "      <td>1.031005e+00</td>\n",
       "      <td>1.062970e+00</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>-0.763709</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>1.115511</td>\n",
       "      <td>1.736664</td>\n",
       "      <td>2.367135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.823610</td>\n",
       "      <td>5.103767</td>\n",
       "      <td>2.443692</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>8.326673e-17</td>\n",
       "      <td>6.933348e-33</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>-0.106554</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>8.765227e-01</td>\n",
       "      <td>7.682920e-01</td>\n",
       "      <td>3.630779</td>\n",
       "      <td>11.662082</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.318167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>8.676947</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>-0.024967</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>6.356027e-01</td>\n",
       "      <td>4.039908e-01</td>\n",
       "      <td>3.580091</td>\n",
       "      <td>12.177900</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.122374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>8.491018</td>\n",
       "      <td>7.178836</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>-0.149123</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>3.281929e-01</td>\n",
       "      <td>1.077106e-01</td>\n",
       "      <td>5.141715</td>\n",
       "      <td>24.626476</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>5.048789</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>-0.042417</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>3.901545e-01</td>\n",
       "      <td>1.522206e-01</td>\n",
       "      <td>3.977671</td>\n",
       "      <td>14.858090</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>-0.096588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>7.025009</td>\n",
       "      <td>4.043557</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6949</th>\n",
       "      <td>-0.011085</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>9.418646e-01</td>\n",
       "      <td>8.871090e-01</td>\n",
       "      <td>5.199469</td>\n",
       "      <td>25.034483</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.008995</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>8.537123</td>\n",
       "      <td>7.296414</td>\n",
       "      <td>4.433474</td>\n",
       "      <td>3.118009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13803</th>\n",
       "      <td>0.101709</td>\n",
       "      <td>0.175556</td>\n",
       "      <td>6.146590e-01</td>\n",
       "      <td>3.778057e-01</td>\n",
       "      <td>0.632431</td>\n",
       "      <td>0.481458</td>\n",
       "      <td>-0.601404</td>\n",
       "      <td>0.175556</td>\n",
       "      <td>0.559144</td>\n",
       "      <td>0.690269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7.702424</td>\n",
       "      <td>4.132100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10583</th>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0.071715</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>2.285503e+00</td>\n",
       "      <td>5.223523e+00</td>\n",
       "      <td>4.647858</td>\n",
       "      <td>20.905451</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.409953</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.073315</td>\n",
       "      <td>3.002028</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>0.915837</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>4.873022e+00</td>\n",
       "      <td>2.374634e+01</td>\n",
       "      <td>5.123177</td>\n",
       "      <td>24.503634</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>0.475555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010680</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.204785</td>\n",
       "      <td>8.866947</td>\n",
       "      <td>3.101170</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11363</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11636</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14423</th>\n",
       "      <td>0.211697</td>\n",
       "      <td>-0.211945</td>\n",
       "      <td>8.647401e-01</td>\n",
       "      <td>7.477754e-01</td>\n",
       "      <td>0.439924</td>\n",
       "      <td>-1.482853</td>\n",
       "      <td>-0.601404</td>\n",
       "      <td>-0.211945</td>\n",
       "      <td>1.129633</td>\n",
       "      <td>1.391295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018997</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.943262</td>\n",
       "      <td>5.300947</td>\n",
       "      <td>2.378414</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>0.645961</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>8.055966e-01</td>\n",
       "      <td>6.489859e-01</td>\n",
       "      <td>0.563289</td>\n",
       "      <td>-0.225174</td>\n",
       "      <td>0.036065</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>1.145215</td>\n",
       "      <td>1.781550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>14.357673</td>\n",
       "      <td>6.069436</td>\n",
       "      <td>2.607763</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>-0.231659</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>4.860568e-01</td>\n",
       "      <td>2.362513e-01</td>\n",
       "      <td>2.544847</td>\n",
       "      <td>4.962687</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>0.343230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>8.630086</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.409919</td>\n",
       "      <td>0.285072</td>\n",
       "      <td>9.661747e-01</td>\n",
       "      <td>9.334936e-01</td>\n",
       "      <td>0.438157</td>\n",
       "      <td>-1.240765</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>0.285072</td>\n",
       "      <td>1.174056</td>\n",
       "      <td>1.875546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.411176</td>\n",
       "      <td>6.202340</td>\n",
       "      <td>3.668016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0.135417</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>9.367235e-01</td>\n",
       "      <td>8.774509e-01</td>\n",
       "      <td>3.217303</td>\n",
       "      <td>9.663538</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>0.605631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.166044</td>\n",
       "      <td>7.101506</td>\n",
       "      <td>3.978398</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>-0.192527</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>6.847011e-02</td>\n",
       "      <td>4.688156e-03</td>\n",
       "      <td>3.154413</td>\n",
       "      <td>8.639927</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.160226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.021523</td>\n",
       "      <td>3.668016</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>0.806106</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.507614e+00</td>\n",
       "      <td>6.288129e+00</td>\n",
       "      <td>3.525821</td>\n",
       "      <td>13.080674</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>0.370213</td>\n",
       "      <td>3.237917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.607847</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13418</th>\n",
       "      <td>-0.116688</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>3.896848e-01</td>\n",
       "      <td>1.518542e-01</td>\n",
       "      <td>2.567905</td>\n",
       "      <td>5.847283</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.063953</td>\n",
       "      <td>0.207581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.005751</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10.097579</td>\n",
       "      <td>5.780723</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.273103</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>4.494021e-01</td>\n",
       "      <td>2.019623e-01</td>\n",
       "      <td>1.360759</td>\n",
       "      <td>0.914041</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>0.278606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.313708</td>\n",
       "      <td>7.496671</td>\n",
       "      <td>2.874752</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12153 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       up_bytes_mean  up_bytes_median  up_bytes_std  up_bytes_var  \\\n",
       "10200      -0.079130        -0.144257  3.386415e-01  1.146781e-01   \n",
       "13412      -0.107161        -0.306905  4.092421e-01  1.674791e-01   \n",
       "14671       0.226701        -0.174761  9.336432e-01  8.716896e-01   \n",
       "4073       -0.578880        -0.706428  2.063335e-01  4.257351e-02   \n",
       "12451       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "8074        1.446792         1.243354  1.704337e+00  2.904765e+00   \n",
       "6824       -0.172218        -0.185985  5.152611e-02  2.654940e-03   \n",
       "7387       -0.106012        -0.137634  9.486510e-02  8.999387e-03   \n",
       "2055        0.017239        -0.135179  5.482904e-01  3.006224e-01   \n",
       "11597       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "2058       -0.052365        -0.135179  2.674398e-01  7.152402e-02   \n",
       "11902       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "6423        0.096918        -0.347224  1.036407e+00  1.074139e+00   \n",
       "4546        1.035536         1.204041  1.126177e+00  1.268274e+00   \n",
       "4755        0.744933         0.393486  1.138062e+00  1.295185e+00   \n",
       "5556       -0.165610        -0.215238  2.672593e-01  7.142754e-02   \n",
       "9564       -0.235707        -0.244888  4.944251e-02  2.444562e-03   \n",
       "3509       -0.066480        -0.140016  2.760103e-01  7.618166e-02   \n",
       "10434      -0.144257        -0.144257  2.775558e-17  7.703720e-34   \n",
       "7337       -0.137634        -0.137634  0.000000e+00  0.000000e+00   \n",
       "483        -0.111469        -0.587750  7.586769e-01  5.755906e-01   \n",
       "8299       -0.036418        -0.156688  4.516375e-01  2.039765e-01   \n",
       "12048       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "12194       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "13254       0.021080        -0.278323  5.107240e-01  2.608390e-01   \n",
       "13133      -0.125453        -0.306905  4.474782e-01  2.002367e-01   \n",
       "4296       -0.297805        -0.587556  7.111195e-01  5.056909e-01   \n",
       "6306        0.919651        -0.211623  2.930435e+00  8.587452e+00   \n",
       "6507       -0.347224        -0.347224  1.110223e-16  1.232595e-32   \n",
       "2984       -0.072641        -0.147340  2.898558e-01  8.401641e-02   \n",
       "...              ...              ...           ...           ...   \n",
       "3385       -0.046688        -0.140016  2.979990e-01  8.880342e-02   \n",
       "4555        1.047699         1.115511  1.031005e+00  1.062970e+00   \n",
       "1184       -0.200795        -0.200795  8.326673e-17  6.933348e-33   \n",
       "6420       -0.106554        -0.347224  8.765227e-01  7.682920e-01   \n",
       "5051       -0.024967        -0.215238  6.356027e-01  4.039908e-01   \n",
       "5311       -0.149123        -0.215238  3.281929e-01  1.077106e-01   \n",
       "2433       -0.042417        -0.146523  3.901545e-01  1.522206e-01   \n",
       "6949       -0.011085        -0.185985  9.418646e-01  8.871090e-01   \n",
       "13803       0.101709         0.175556  6.146590e-01  3.778057e-01   \n",
       "10583      -0.144257        -0.144257  2.775558e-17  7.703720e-34   \n",
       "769         0.071715        -0.587750  2.285503e+00  5.223523e+00   \n",
       "1685        0.915837        -0.135179  4.873022e+00  2.374634e+01   \n",
       "8322       -0.156688        -0.156688  2.775558e-17  7.703720e-34   \n",
       "11111       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "11363       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "11636       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "14423       0.211697        -0.211945  8.647401e-01  7.477754e-01   \n",
       "5578       -0.215238        -0.215238  2.775558e-17  7.703720e-34   \n",
       "4426        0.645961         0.473058  8.055966e-01  6.489859e-01   \n",
       "13526      -0.231659        -0.412035  4.860568e-01  2.362513e-01   \n",
       "466         0.409919         0.285072  9.661747e-01  9.334936e-01   \n",
       "6265        0.135417        -0.211623  9.367235e-01  8.774509e-01   \n",
       "5734       -0.192527        -0.215238  6.847011e-02  4.688156e-03   \n",
       "11284       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "11964       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "5191        0.806106        -0.215238  2.507614e+00  6.288129e+00   \n",
       "13418      -0.116688        -0.306905  3.896848e-01  1.518542e-01   \n",
       "5390       -0.215238        -0.215238  2.775558e-17  7.703720e-34   \n",
       "860        -0.273103        -0.587750  4.494021e-01  2.019623e-01   \n",
       "7270       -0.137634        -0.137634  0.000000e+00  0.000000e+00   \n",
       "\n",
       "       up_bytes_skew  up_bytes_kurt  up_bytes_perc25  up_bytes_perc50  \\\n",
       "10200       5.189428      24.965753        -0.144257        -0.144257   \n",
       "13412       2.671848       6.338245        -0.306905        -0.306905   \n",
       "14671       0.789465      -0.249575        -0.601404        -0.174761   \n",
       "4073        1.247258       0.331092        -0.726231        -0.706428   \n",
       "12451       0.000000      -3.000000         0.000000         0.000000   \n",
       "8074        1.428606       1.797063         0.027553         1.243354   \n",
       "6824        3.478627      10.112128        -0.185985        -0.185985   \n",
       "7387        2.666667       5.111111        -0.137634        -0.137634   \n",
       "2055        3.517940      10.605252        -0.135179        -0.135179   \n",
       "11597       0.000000      -3.000000         0.000000         0.000000   \n",
       "2058        3.278747       9.655424        -0.135179        -0.135179   \n",
       "11902       0.000000      -3.000000         0.000000         0.000000   \n",
       "6423        2.357437       4.352739        -0.347224        -0.347224   \n",
       "4546        0.534846      -0.127047        -0.111013         1.204041   \n",
       "4755        1.284652       1.732834        -0.129568         0.393486   \n",
       "5556        5.199469      25.034483        -0.215238        -0.215238   \n",
       "9564        5.199469      25.034483        -0.244888        -0.244888   \n",
       "3509        3.511800      10.430744        -0.140016        -0.140016   \n",
       "10434       1.000000      -2.000000        -0.144257        -0.144257   \n",
       "7337        0.000000      -3.000000        -0.137634        -0.137634   \n",
       "483         1.375917       0.486632        -0.587750        -0.587750   \n",
       "8299        3.644875      11.706503        -0.156688        -0.156688   \n",
       "12048       0.000000      -3.000000         0.000000         0.000000   \n",
       "12194       0.000000      -3.000000         0.000000         0.000000   \n",
       "13254       1.529409       0.755960        -0.306905        -0.278323   \n",
       "13133       2.388625       4.032556        -0.306905        -0.306905   \n",
       "4296        2.733056       7.711204        -0.676266        -0.587556   \n",
       "6306        3.312911      10.782022        -0.211623        -0.211623   \n",
       "6507        1.000000      -2.000000        -0.347224        -0.347224   \n",
       "2984        3.874849      13.857175        -0.147340        -0.147340   \n",
       "...              ...            ...              ...              ...   \n",
       "3385        3.155160       8.574877        -0.140016        -0.140016   \n",
       "4555        0.058908      -0.763709         0.199106         1.115511   \n",
       "1184       -1.000000      -2.000000        -0.200795        -0.200795   \n",
       "6420        3.630779      11.662082        -0.347224        -0.347224   \n",
       "5051        3.580091      12.177900        -0.215238        -0.215238   \n",
       "5311        5.141715      24.626476        -0.215238        -0.215238   \n",
       "2433        3.977671      14.858090        -0.146523        -0.146523   \n",
       "6949        5.199469      25.034483        -0.185985        -0.185985   \n",
       "13803       0.632431       0.481458        -0.601404         0.175556   \n",
       "10583       1.000000      -2.000000        -0.144257        -0.144257   \n",
       "769         4.647858      20.905451        -0.587750        -0.587750   \n",
       "1685        5.123177      24.503634        -0.135179        -0.135179   \n",
       "8322        1.000000      -2.000000        -0.156688        -0.156688   \n",
       "11111       0.000000      -3.000000         0.000000         0.000000   \n",
       "11363       0.000000      -3.000000         0.000000         0.000000   \n",
       "11636       0.000000      -3.000000         0.000000         0.000000   \n",
       "14423       0.439924      -1.482853        -0.601404        -0.211945   \n",
       "5578        1.000000      -2.000000        -0.215238        -0.215238   \n",
       "4426        0.563289      -0.225174         0.036065         0.473058   \n",
       "13526       2.544847       4.962687        -0.412035        -0.412035   \n",
       "466         0.438157      -1.240765        -0.587750         0.285072   \n",
       "6265        3.217303       9.663538        -0.211623        -0.211623   \n",
       "5734        3.154413       8.639927        -0.215238        -0.215238   \n",
       "11284       0.000000      -3.000000         0.000000         0.000000   \n",
       "11964       0.000000      -3.000000         0.000000         0.000000   \n",
       "5191        3.525821      13.080674        -0.215238        -0.215238   \n",
       "13418       2.567905       5.847283        -0.306905        -0.306905   \n",
       "5390        1.000000      -2.000000        -0.215238        -0.215238   \n",
       "860         1.360759       0.914041        -0.587750        -0.587750   \n",
       "7270        0.000000      -3.000000        -0.137634        -0.137634   \n",
       "\n",
       "       up_bytes_perc75  up_bytes_perc90         ...          \\\n",
       "10200        -0.144257        -0.144257         ...           \n",
       "13412        -0.092536         0.121833         ...           \n",
       "14671         0.802801         1.218680         ...           \n",
       "4073         -0.450378        -0.322992         ...           \n",
       "12451         0.000000         0.000000         ...           \n",
       "8074          2.139456         3.579851         ...           \n",
       "6824         -0.185985        -0.185985         ...           \n",
       "7387         -0.137634        -0.106012         ...           \n",
       "2055         -0.135179        -0.115549         ...           \n",
       "11597         0.000000         0.000000         ...           \n",
       "2058         -0.135179        -0.091012         ...           \n",
       "11902         0.000000         0.000000         ...           \n",
       "6423         -0.347224         1.741536         ...           \n",
       "4546          1.581805         2.092415         ...           \n",
       "4755          1.352132         2.018318         ...           \n",
       "5556         -0.215238        -0.215238         ...           \n",
       "9564         -0.244888        -0.244888         ...           \n",
       "3509         -0.140016        -0.140016         ...           \n",
       "10434        -0.144257        -0.144257         ...           \n",
       "7337         -0.137634        -0.137634         ...           \n",
       "483           0.107275         1.351853         ...           \n",
       "8299         -0.156688        -0.152899         ...           \n",
       "12048         0.000000         0.000000         ...           \n",
       "12194         0.000000         0.000000         ...           \n",
       "13254         0.093251         1.037905         ...           \n",
       "13133        -0.306905         0.398656         ...           \n",
       "4296         -0.403423         0.498857         ...           \n",
       "6306          0.389587         2.315089         ...           \n",
       "6507         -0.347224        -0.347224         ...           \n",
       "2984         -0.147340        -0.147340         ...           \n",
       "...                ...              ...         ...           \n",
       "3385         -0.140016        -0.089774         ...           \n",
       "4555          1.736664         2.367135         ...           \n",
       "1184         -0.200795        -0.200795         ...           \n",
       "6420         -0.347224        -0.318167         ...           \n",
       "5051         -0.215238        -0.122374         ...           \n",
       "5311         -0.215238        -0.215238         ...           \n",
       "2433         -0.146523        -0.096588         ...           \n",
       "6949         -0.185985        -0.185985         ...           \n",
       "13803         0.559144         0.690269         ...           \n",
       "10583        -0.144257        -0.144257         ...           \n",
       "769          -0.409953         0.110507         ...           \n",
       "1685         -0.135179         0.475555         ...           \n",
       "8322         -0.156688        -0.156688         ...           \n",
       "11111         0.000000         0.000000         ...           \n",
       "11363         0.000000         0.000000         ...           \n",
       "11636         0.000000         0.000000         ...           \n",
       "14423         1.129633         1.391295         ...           \n",
       "5578         -0.215238        -0.215238         ...           \n",
       "4426          1.145215         1.781550         ...           \n",
       "13526        -0.412035         0.343230         ...           \n",
       "466           1.174056         1.875546         ...           \n",
       "6265         -0.211623         0.605631         ...           \n",
       "5734         -0.215238        -0.160226         ...           \n",
       "11284         0.000000         0.000000         ...           \n",
       "11964         0.000000         0.000000         ...           \n",
       "5191          0.370213         3.237917         ...           \n",
       "13418        -0.063953         0.207581         ...           \n",
       "5390         -0.215238        -0.215238         ...           \n",
       "860           0.026458         0.278606         ...           \n",
       "7270         -0.137634        -0.137634         ...           \n",
       "\n",
       "       down_packet_1min_y  down_packet_2min_y  down_packet_3min_y  \\\n",
       "10200            0.005860            0.008896            0.008737   \n",
       "13412            0.013330            0.001572           -1.000000   \n",
       "14671            0.005574            0.001867           -1.000000   \n",
       "4073             0.003252            0.008237            0.003024   \n",
       "12451           -1.000000           -1.000000           -1.000000   \n",
       "8074             0.005365            0.009584            0.005290   \n",
       "6824             0.008514            0.011564            0.002802   \n",
       "7387             0.008994            0.012208           -1.000000   \n",
       "2055             0.007299            0.009431           -1.000000   \n",
       "11597           -1.000000           -1.000000           -1.000000   \n",
       "2058             0.009107            0.011621            0.011696   \n",
       "11902           -1.000000           -1.000000           -1.000000   \n",
       "6423             0.010421           -1.000000           -1.000000   \n",
       "4546             0.009793            0.012066            0.007865   \n",
       "4755             0.002496            0.018744            0.005859   \n",
       "5556             0.005173            0.007579            0.008852   \n",
       "9564             0.005896            0.008164            0.008838   \n",
       "3509             0.006175            0.008511            0.009006   \n",
       "10434            0.008994            0.012208           -1.000000   \n",
       "7337             0.008994            0.012208           -1.000000   \n",
       "483              0.001784            0.001820           -1.000000   \n",
       "8299             0.006217            0.008375            0.009037   \n",
       "12048           -1.000000           -1.000000           -1.000000   \n",
       "12194           -1.000000           -1.000000           -1.000000   \n",
       "13254            0.011902            0.003610            0.001850   \n",
       "13133            0.009781           -1.000000           -1.000000   \n",
       "4296             0.001411            0.004333           -1.000000   \n",
       "6306             0.007444            0.009868            0.002316   \n",
       "6507             0.008994            0.012208           -1.000000   \n",
       "2984             0.006574            0.008910            0.008079   \n",
       "...                   ...                 ...                 ...   \n",
       "3385             0.007261            0.011334           -1.000000   \n",
       "4555             0.000537            0.004518            0.001292   \n",
       "1184             0.008994            0.012208           -1.000000   \n",
       "6420             0.008899            0.011104           -1.000000   \n",
       "5051             0.007093            0.009288            0.009181   \n",
       "5311             0.006422            0.008797            0.009226   \n",
       "2433             0.005691            0.008660            0.010063   \n",
       "6949             0.005804            0.008341            0.008488   \n",
       "13803            0.001880            0.000771           -1.000000   \n",
       "10583            0.008994            0.012208           -1.000000   \n",
       "769              0.013393            0.000425           -1.000000   \n",
       "1685             0.010680            0.012899            0.001598   \n",
       "8322             0.008994            0.012208           -1.000000   \n",
       "11111           -1.000000           -1.000000           -1.000000   \n",
       "11363           -1.000000           -1.000000           -1.000000   \n",
       "11636           -1.000000           -1.000000           -1.000000   \n",
       "14423            0.018997            0.001050            0.000263   \n",
       "5578             0.008994            0.012208           -1.000000   \n",
       "4426             0.000967            0.005850            0.001068   \n",
       "13526            0.009898            0.011989           -1.000000   \n",
       "466              0.002531            0.001531            0.001629   \n",
       "6265             0.006037            0.005097            0.006484   \n",
       "5734             0.006585            0.003787            0.007560   \n",
       "11284           -1.000000           -1.000000           -1.000000   \n",
       "11964           -1.000000           -1.000000           -1.000000   \n",
       "5191             0.011995           -1.000000           -1.000000   \n",
       "13418            0.003703            0.005751           -1.000000   \n",
       "5390             0.008994            0.012208           -1.000000   \n",
       "860              0.015683            0.002318            0.000409   \n",
       "7270             0.008994            0.012208           -1.000000   \n",
       "\n",
       "       down_packet_4min_y  down_packet_5min_y  down_packet_1min_x  \\\n",
       "10200            0.008919           -1.000000           12.539757   \n",
       "13412           -1.000000           -1.000000           11.561446   \n",
       "14671           -1.000000           -1.000000           13.454343   \n",
       "4073            -1.000000           -1.000000           12.814342   \n",
       "12451           -1.000000           -1.000000           -1.000000   \n",
       "8074             0.008255            0.008069           12.745137   \n",
       "6824            -1.000000           -1.000000           12.337687   \n",
       "7387            -1.000000           -1.000000           12.337687   \n",
       "2055            -1.000000           -1.000000           12.337687   \n",
       "11597           -1.000000           -1.000000           -1.000000   \n",
       "2058            -1.000000           -1.000000           12.337687   \n",
       "11902           -1.000000           -1.000000           -1.000000   \n",
       "6423            -1.000000           -1.000000           12.472035   \n",
       "4546             0.002583           -1.000000           12.138872   \n",
       "4755            -1.000000           -1.000000           13.454343   \n",
       "5556             0.009160           -1.000000           12.337687   \n",
       "9564             0.009101            0.009141           12.271056   \n",
       "3509            -1.000000           -1.000000           12.404679   \n",
       "10434           -1.000000           -1.000000           12.337687   \n",
       "7337            -1.000000           -1.000000           12.337687   \n",
       "483             -1.000000           -1.000000            9.934862   \n",
       "8299            -1.000000           -1.000000           12.337687   \n",
       "12048           -1.000000           -1.000000           -1.000000   \n",
       "12194           -1.000000           -1.000000           -1.000000   \n",
       "13254           -1.000000           -1.000000           11.375141   \n",
       "13133           -1.000000           -1.000000           12.404679   \n",
       "4296            -1.000000           -1.000000           10.318687   \n",
       "6306            -1.000000           -1.000000           12.138872   \n",
       "6507            -1.000000           -1.000000           12.337687   \n",
       "2984            -1.000000           -1.000000           12.337687   \n",
       "...                   ...                 ...                 ...   \n",
       "3385            -1.000000           -1.000000           12.472035   \n",
       "4555            -1.000000           -1.000000           13.823610   \n",
       "1184            -1.000000           -1.000000           12.337687   \n",
       "6420            -1.000000           -1.000000           12.271056   \n",
       "5051            -1.000000           -1.000000           12.404679   \n",
       "5311            -1.000000           -1.000000           12.271056   \n",
       "2433             0.008434           -1.000000           12.404679   \n",
       "6949             0.008995            0.009132           12.472035   \n",
       "13803           -1.000000           -1.000000            7.702424   \n",
       "10583           -1.000000           -1.000000           12.337687   \n",
       "769             -1.000000           -1.000000           12.073315   \n",
       "1685            -1.000000           -1.000000           12.204785   \n",
       "8322            -1.000000           -1.000000           12.337687   \n",
       "11111           -1.000000           -1.000000           -1.000000   \n",
       "11363           -1.000000           -1.000000           -1.000000   \n",
       "11636           -1.000000           -1.000000           -1.000000   \n",
       "14423           -1.000000           -1.000000           11.943262   \n",
       "5578            -1.000000           -1.000000           12.337687   \n",
       "4426            -1.000000           -1.000000           14.357673   \n",
       "13526           -1.000000           -1.000000           12.271056   \n",
       "466             -1.000000           -1.000000            9.411176   \n",
       "6265            -1.000000           -1.000000           13.166044   \n",
       "5734             0.007705           -1.000000           12.472035   \n",
       "11284           -1.000000           -1.000000           -1.000000   \n",
       "11964           -1.000000           -1.000000           -1.000000   \n",
       "5191            -1.000000           -1.000000           12.607847   \n",
       "13418           -1.000000           -1.000000           10.097579   \n",
       "5390            -1.000000           -1.000000           12.337687   \n",
       "860             -1.000000           -1.000000           11.313708   \n",
       "7270            -1.000000           -1.000000           12.337687   \n",
       "\n",
       "       down_packet_2min_x  down_packet_3min_x  down_packet_4min_x  \\\n",
       "10200            8.630086            6.800425            3.935542   \n",
       "13412            7.336032           -1.000000           -1.000000   \n",
       "14671            4.604757           -1.000000           -1.000000   \n",
       "4073             8.963502            4.433474           -1.000000   \n",
       "12451           -1.000000           -1.000000           -1.000000   \n",
       "8074             8.354190            6.441961            4.199776   \n",
       "6824             8.724062            3.809727           -1.000000   \n",
       "7387             8.724062           -1.000000           -1.000000   \n",
       "2055             8.724062           -1.000000           -1.000000   \n",
       "11597           -1.000000           -1.000000           -1.000000   \n",
       "2058             8.583479            7.140066           -1.000000   \n",
       "11902           -1.000000           -1.000000           -1.000000   \n",
       "6423            -1.000000           -1.000000           -1.000000   \n",
       "4546             9.209514            6.004056            2.813152   \n",
       "4755             9.259521            5.417022           -1.000000   \n",
       "5556             8.915094            7.257009            3.648207   \n",
       "9564             9.110309            6.168843            4.362031   \n",
       "3509             8.630086            4.705588           -1.000000   \n",
       "10434            8.724062           -1.000000           -1.000000   \n",
       "7337             8.724062           -1.000000           -1.000000   \n",
       "483              3.002028           -1.000000           -1.000000   \n",
       "8299             8.819061            7.063154           -1.000000   \n",
       "12048           -1.000000           -1.000000           -1.000000   \n",
       "12194           -1.000000           -1.000000           -1.000000   \n",
       "13254            5.687570            2.828427           -1.000000   \n",
       "13133           -1.000000           -1.000000           -1.000000   \n",
       "4296             4.604757           -1.000000           -1.000000   \n",
       "6306             9.061108            4.994404           -1.000000   \n",
       "6507             8.724062           -1.000000           -1.000000   \n",
       "2984             8.724062            4.457547           -1.000000   \n",
       "...                   ...                 ...                 ...   \n",
       "3385             8.771433           -1.000000           -1.000000   \n",
       "4555             5.103767            2.443692           -1.000000   \n",
       "1184             8.724062           -1.000000           -1.000000   \n",
       "6420             8.676947           -1.000000           -1.000000   \n",
       "5051             8.491018            7.178836           -1.000000   \n",
       "5311             8.963502            5.048789           -1.000000   \n",
       "2433             8.963502            7.025009            4.043557   \n",
       "6949             8.537123            7.296414            4.433474   \n",
       "13803            4.132100           -1.000000           -1.000000   \n",
       "10583            8.724062           -1.000000           -1.000000   \n",
       "769              3.002028           -1.000000           -1.000000   \n",
       "1685             8.866947            3.101170           -1.000000   \n",
       "8322             8.724062           -1.000000           -1.000000   \n",
       "11111           -1.000000           -1.000000           -1.000000   \n",
       "11363           -1.000000           -1.000000           -1.000000   \n",
       "11636           -1.000000           -1.000000           -1.000000   \n",
       "14423            5.300947            2.378414           -1.000000   \n",
       "5578             8.724062           -1.000000           -1.000000   \n",
       "4426             6.069436            2.607763           -1.000000   \n",
       "13526            8.630086           -1.000000           -1.000000   \n",
       "466              6.202340            3.668016           -1.000000   \n",
       "6265             7.101506            3.978398           -1.000000   \n",
       "5734             8.000000            5.021523            3.668016   \n",
       "11284           -1.000000           -1.000000           -1.000000   \n",
       "11964           -1.000000           -1.000000           -1.000000   \n",
       "5191            -1.000000           -1.000000           -1.000000   \n",
       "13418            5.780723           -1.000000           -1.000000   \n",
       "5390             8.724062           -1.000000           -1.000000   \n",
       "860              7.496671            2.874752           -1.000000   \n",
       "7270             8.724062           -1.000000           -1.000000   \n",
       "\n",
       "       down_packet_5min_x  \n",
       "10200           -1.000000  \n",
       "13412           -1.000000  \n",
       "14671           -1.000000  \n",
       "4073            -1.000000  \n",
       "12451           -1.000000  \n",
       "8074             2.579672  \n",
       "6824            -1.000000  \n",
       "7387            -1.000000  \n",
       "2055            -1.000000  \n",
       "11597           -1.000000  \n",
       "2058            -1.000000  \n",
       "11902           -1.000000  \n",
       "6423            -1.000000  \n",
       "4546            -1.000000  \n",
       "4755            -1.000000  \n",
       "5556            -1.000000  \n",
       "9564             3.327353  \n",
       "3509            -1.000000  \n",
       "10434           -1.000000  \n",
       "7337            -1.000000  \n",
       "483             -1.000000  \n",
       "8299            -1.000000  \n",
       "12048           -1.000000  \n",
       "12194           -1.000000  \n",
       "13254           -1.000000  \n",
       "13133           -1.000000  \n",
       "4296            -1.000000  \n",
       "6306            -1.000000  \n",
       "6507            -1.000000  \n",
       "2984            -1.000000  \n",
       "...                   ...  \n",
       "3385            -1.000000  \n",
       "4555            -1.000000  \n",
       "1184            -1.000000  \n",
       "6420            -1.000000  \n",
       "5051            -1.000000  \n",
       "5311            -1.000000  \n",
       "2433            -1.000000  \n",
       "6949             3.118009  \n",
       "13803           -1.000000  \n",
       "10583           -1.000000  \n",
       "769             -1.000000  \n",
       "1685            -1.000000  \n",
       "8322            -1.000000  \n",
       "11111           -1.000000  \n",
       "11363           -1.000000  \n",
       "11636           -1.000000  \n",
       "14423           -1.000000  \n",
       "5578            -1.000000  \n",
       "4426            -1.000000  \n",
       "13526           -1.000000  \n",
       "466             -1.000000  \n",
       "6265            -1.000000  \n",
       "5734            -1.000000  \n",
       "11284           -1.000000  \n",
       "11964           -1.000000  \n",
       "5191            -1.000000  \n",
       "13418           -1.000000  \n",
       "5390            -1.000000  \n",
       "860             -1.000000  \n",
       "7270            -1.000000  \n",
       "\n",
       "[12153 rows x 132 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\\nimport matplotlib.pyplot as plt\\n\\nrandom_forest = RandomForestClassifier(random_state=42)\\ny_probas_forest_y = cross_val_predict(random_forest, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\\ny_probas_forest_n = cross_val_predict(random_forest, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\\ny_probas_forest_t = cross_val_predict(random_forest, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\\ny_probas_forest = cross_val_predict(random_forest, x_train, y_train, cv=10, method=\"predict_proba\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest_y = cross_val_predict(random_forest, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest_n = cross_val_predict(random_forest, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest_t = cross_val_predict(random_forest, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest = cross_val_predict(random_forest, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_scores_forest_y = y_probas_forest_y[:, 1]\\ny_scores_forest_n = y_probas_forest_n[:, 1]\\ny_scores_forest_t = y_probas_forest_t[:, 1]\\n\\nfpr_forest_y, tpr_forest_y, thresholds_forest_y = roc_curve(y_train_youtube, y_scores_forest_y)\\nfpr_forest_n, tpr_forest_n, thresholds_forest_n = roc_curve(y_train_netflix, y_scores_forest_n)\\nfpr_forest_t, tpr_forest_t, thresholds_forest_t = roc_curve(y_train_twitch, y_scores_forest_t)\\n#fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_probas_forest)\\n\\n\\nplot_roc_curve(fpr_forest_y, tpr_forest_y, \"YouTube\")\\nplot_roc_curve(fpr_forest_n, tpr_forest_n, \"Netflix\")\\nplot_roc_curve(fpr_forest_t, tpr_forest_t, \"Twitch\")\\n#plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\\n\\nplt.legend(loc=\"lower right\")\\nplt.show()\\n#cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring=\\'accuracy\\')\\n#accuracy = sum(cvs)/len(cvs)\\n#print(\"Accuracy: \" + str(accuracy))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "y_scores_forest_y = y_probas_forest_y[:, 1]\n",
    "y_scores_forest_n = y_probas_forest_n[:, 1]\n",
    "y_scores_forest_t = y_probas_forest_t[:, 1]\n",
    "\n",
    "fpr_forest_y, tpr_forest_y, thresholds_forest_y = roc_curve(y_train_youtube, y_scores_forest_y)\n",
    "fpr_forest_n, tpr_forest_n, thresholds_forest_n = roc_curve(y_train_netflix, y_scores_forest_n)\n",
    "fpr_forest_t, tpr_forest_t, thresholds_forest_t = roc_curve(y_train_twitch, y_scores_forest_t)\n",
    "#fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_probas_forest)\n",
    "\n",
    "\n",
    "plot_roc_curve(fpr_forest_y, tpr_forest_y, \"YouTube\")\n",
    "plot_roc_curve(fpr_forest_n, tpr_forest_n, \"Netflix\")\n",
    "plot_roc_curve(fpr_forest_t, tpr_forest_t, \"Twitch\")\n",
    "#plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "#cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "#accuracy = sum(cvs)/len(cvs)\n",
    "#print(\"Accuracy: \" + str(accuracy))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrandom_forest.fit(x_train, y_train)\\npredictions = random_forest.predict(x_test)\\nconf_mx = confusion_matrix(y_test, predictions)\\nplt.matshow(conf_mx, cmap=plt.cm.gray)\\nconf_mx\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "random_forest.fit(x_train, y_train)\n",
    "predictions = random_forest.predict(x_test)\n",
    "conf_mx = confusion_matrix(y_test, predictions)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "conf_mx\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search_acc = GridSearchCV(random_forest, params, cv=10, scoring='accuracy')\n",
    "#grid_search_acc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best2 = grid_search_acc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(zip(grid_search.best_estimator_.feature_importances_, basic_stats), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Final evaluation\\nrf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\\'entropy\\', max_depth=9, max_features=\\'log2\\', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=42, verbose=0, warm_start=False)\\nrf.fit(x_train, y_train)\\nprint(\"Accuracy train set: \" + str(sum(rf.predict(x_train) == y_train)/float(len(y_train))))\\nprint(\"Accuracy test set: \" + str(sum(rf.predict(x_test) == y_test)/float(len(y_test))))\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Final evaluation\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "rf.fit(x_train, y_train)\n",
    "print(\"Accuracy train set: \" + str(sum(rf.predict(x_train) == y_train)/float(len(y_train))))\n",
    "print(\"Accuracy test set: \" + str(sum(rf.predict(x_test) == y_test)/float(len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncrossval = cross_val_predict(rf, x_train, y_train, cv=10, method=\"predict_proba\")\\ncrossvalscore = cross_val_score(rf, x_train, y_train, cv=10, scoring=\"accuracy\")\\nprint(\"\\tCrossValScore: \" + str(sum(crossvalscore)/len(cv_yt)))\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "crossval = cross_val_predict(rf, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "crossvalscore = cross_val_score(rf, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "print(\"\\tCrossValScore: \" + str(sum(crossvalscore)/len(cv_yt)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_test_forest(model):\n",
    "    '''\n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    \n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    '''\n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    '''\n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "    '''\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def do_test_svm(model):    \n",
    "    '''\n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "    \n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    '''\n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    '''\n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    '''\n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test_knn(model):\n",
    "    \n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "    \n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    \n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    \n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Random Forest------\n",
      "[[ 435    0   17    0]\n",
      " [   0  922    3    0]\n",
      " [   1    1 1158    1]\n",
      " [   0    0    1  500]]\n",
      "\n",
      "Accuracy train set: 0.9991771579033983\n",
      "Accuracy test set: 0.9921026653504442\n",
      "-----OvO Classifier Random Forest------\n",
      "[[ 436    0   16    0]\n",
      " [   0  920    5    0]\n",
      " [   0    3 1158    0]\n",
      " [   0    0    0  501]]\n",
      "\n",
      "Accuracy train set: 0.9992594421130585\n",
      "Accuracy test set: 0.9921026653504442\n",
      "----------SVM-----------\n",
      "Multiclass: \n",
      "\tCrossValScore: 0.8294267515517344\n",
      "[[376   7  62   7]\n",
      " [ 17 747 160   1]\n",
      " [ 63 183 912   3]\n",
      " [  1   0   3 497]]\n",
      "\n",
      "Accuracy train set: 0.9191969061137167\n",
      "Accuracy test set: 0.8331688055281342\n",
      "----------Knn-----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube: \n",
      "\tCrossValScore: 0.9253690736838722\n",
      "\tRocAucScore:  0.949505214116756\n",
      "Netflix: \n",
      "\tCrossValScore: 0.8915495426837428\n",
      "\tRocAucScore:  0.9345713885457394\n",
      "Twitch: \n",
      "\tCrossValScore: 0.8451434914446609\n",
      "\tRocAucScore:  0.9173747219321348\n",
      "Multiclass: \n",
      "\tCrossValScore: 0.8240823006185675\n",
      "[[356  11  76   9]\n",
      " [ 50 716 157   2]\n",
      " [122 118 915   6]\n",
      " [  0   0   3 498]]\n",
      "\n",
      "Accuracy train set: 0.8813461696700403\n",
      "Accuracy test set: 0.8177031918394209\n",
      "-----Neural Network-----\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube: \n",
      "\tCrossValScore: 0.9775362083923529\n",
      "\tRocAucScore:  0.9932482002975644\n",
      "Netflix: \n",
      "\tCrossValScore: 0.9164020675461966\n",
      "\tRocAucScore:  0.9717317341857592\n",
      "Twitch: \n",
      "\tCrossValScore: 0.8801145901017977\n",
      "\tRocAucScore:  0.9364205779887119\n",
      "Multiclass: \n",
      "\tCrossValScore: 0.901265107129757\n",
      "[[ 425    5   22    0]\n",
      " [  11  845   69    0]\n",
      " [  15   72 1074    0]\n",
      " [   1    0    0  500]]\n",
      "\n",
      "Accuracy train set: 0.9563070846704518\n",
      "Accuracy test set: 0.9358341559723593\n",
      "--------AdaBoost Random Forest--------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"-----Random Forest------\")\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=9, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "do_test_forest(rf)\n",
    "\n",
    "\n",
    "print(\"-----OvO Classifier Random Forest------\")\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9,\n",
    "                            max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\n",
    "                            random_state=42, verbose=0, warm_start=False)\n",
    "rf = OneVsOneClassifier(rf)\n",
    "do_test_forest(rf)\n",
    "\n",
    "print(\"----------SVM-----------\")\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "do_test_svm(svm)\n",
    "\n",
    "print(\"----------Knn-----------\")\n",
    "knn = KNeighborsClassifier()\n",
    "do_test_knn(knn)\n",
    "\n",
    "print(\"-----Neural Network-----\")\n",
    "nn = MLPClassifier()\n",
    "do_test_knn(nn)\n",
    "\n",
    "\n",
    "print(\"--------AdaBoost Random Forest--------\")\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9,\n",
    "                            max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\n",
    "                            random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    rf,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n",
    "\n",
    "print(\"---------Decision Tree------\")\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "do_test_forest(dt)\n",
    "\n",
    "print(\"--------AdaBoost Decision Tree--------\")\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    dt,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = dict()\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "params['forest'] = {\n",
    "    'max_depth' : [6,7,8,9],\n",
    "    'n_estimators': [30,100,300],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf' : [1, 2, 4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 3, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(random_forest, params['forest'], cv=10, scoring='accuracy')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "\n",
    "predictions = best_random_forest.predict(x_test)\n",
    "conf_mx = confusion_matrix(y_test, predictions)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "print(conf_mx)\n",
    "print(\"\\nAccuracy train set: \" + str(sum(best_random_forest.predict(x_train) == y_train)/float(len(y_train))))\n",
    "print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Random Forest------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3c6869eccbb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             warm_start=False)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdo_test_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-b767a046dc29>\u001b[0m in \u001b[0;36mdo_test_forest\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcv_nf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_netflix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcv_tw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_twitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mcv_mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     '''\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YouTube: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"-----Random Forest------\")\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=9, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "do_test_forest(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/rf_model_01s.sav']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf, '../models/rf_model_01s.sav') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.65946734 -1.73477919 -1.2318347  -4.22145108]\n",
      " [-0.34487931 -4.11526472 -1.37651383 -3.77594527]\n",
      " [-0.16042623 -3.82198385 -2.0832087  -6.31718965]\n",
      " [-0.49440497 -2.76821807 -1.18112638 -3.8941985 ]\n",
      " [-0.13421028 -4.403363   -2.19051664 -6.50311376]\n",
      " [-0.30400206 -3.16321299 -1.53384557 -5.48550383]\n",
      " [-0.17031451 -4.36427963 -1.9777458  -5.20376361]\n",
      " [-0.1332101  -4.05725409 -2.246051   -6.43394457]\n",
      " [-0.10791961 -3.83569323 -2.52406313 -7.45159518]\n",
      " [-0.25609308 -3.09667795 -1.73380679 -5.49186039]\n",
      " [-0.06928215 -4.4673744  -2.91726127 -6.58740486]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for path, subdirs, files in os.walk(base_folder):\n",
    "    for name in files:\n",
    "        if name == 'cap3.csv':\n",
    "            test = pd.read_csv(os.path.join(str(path), str(name)))\n",
    "test = test.drop(columns=['Unnamed: 0']).reset_index()\n",
    "test.drop(columns=['index', 'up_packet_silence_mean', 'down_packet_silence_mean',\n",
    "                      'down_packet_longest_silence', 'down_packet_shortest_silence','label'], inplace=True)\n",
    "# Imputer for NaN\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(test)\n",
    "test = pd.DataFrame(imputer.transform(test), columns=test.columns)\n",
    "print(rf.predict_log_proba(test))\n",
    "print(rf.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "rf = joblib.load('../models/rf_model_01s.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acestream\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prevlabel[4000])\n",
    "dataset['label'][4000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_bytes_mean</th>\n",
       "      <th>up_bytes_median</th>\n",
       "      <th>up_bytes_std</th>\n",
       "      <th>up_bytes_var</th>\n",
       "      <th>up_bytes_skew</th>\n",
       "      <th>up_bytes_kurt</th>\n",
       "      <th>up_bytes_perc25</th>\n",
       "      <th>up_bytes_perc50</th>\n",
       "      <th>up_bytes_perc75</th>\n",
       "      <th>up_bytes_perc90</th>\n",
       "      <th>...</th>\n",
       "      <th>down_packet_1min_y</th>\n",
       "      <th>down_packet_2min_y</th>\n",
       "      <th>down_packet_3min_y</th>\n",
       "      <th>down_packet_4min_y</th>\n",
       "      <th>down_packet_5min_y</th>\n",
       "      <th>down_packet_1min_x</th>\n",
       "      <th>down_packet_2min_x</th>\n",
       "      <th>down_packet_3min_x</th>\n",
       "      <th>down_packet_4min_x</th>\n",
       "      <th>down_packet_5min_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.049954</td>\n",
       "      <td>-0.403884</td>\n",
       "      <td>0.903263</td>\n",
       "      <td>0.815885</td>\n",
       "      <td>0.936888</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.777905</td>\n",
       "      <td>-0.403884</td>\n",
       "      <td>0.496817</td>\n",
       "      <td>1.335181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.071279</td>\n",
       "      <td>3.978398</td>\n",
       "      <td>2.497202</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.555438</td>\n",
       "      <td>-1.038336</td>\n",
       "      <td>0.953788</td>\n",
       "      <td>0.909712</td>\n",
       "      <td>2.042770</td>\n",
       "      <td>3.353170</td>\n",
       "      <td>-1.076309</td>\n",
       "      <td>-1.038336</td>\n",
       "      <td>-0.688625</td>\n",
       "      <td>0.728165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015163</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.814609</td>\n",
       "      <td>6.727171</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.429697</td>\n",
       "      <td>-0.797330</td>\n",
       "      <td>1.049213</td>\n",
       "      <td>1.100848</td>\n",
       "      <td>1.963251</td>\n",
       "      <td>2.620611</td>\n",
       "      <td>-1.040970</td>\n",
       "      <td>-0.797330</td>\n",
       "      <td>-0.604283</td>\n",
       "      <td>1.413431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10.262960</td>\n",
       "      <td>3.220981</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.290295</td>\n",
       "      <td>-0.852862</td>\n",
       "      <td>1.105632</td>\n",
       "      <td>1.222422</td>\n",
       "      <td>1.520949</td>\n",
       "      <td>1.379564</td>\n",
       "      <td>-1.076309</td>\n",
       "      <td>-0.852862</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>1.490957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.676306</td>\n",
       "      <td>7.336032</td>\n",
       "      <td>4.808627</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.490188</td>\n",
       "      <td>0.728495</td>\n",
       "      <td>1.230044</td>\n",
       "      <td>1.513008</td>\n",
       "      <td>0.312373</td>\n",
       "      <td>-0.971798</td>\n",
       "      <td>-0.726433</td>\n",
       "      <td>0.728495</td>\n",
       "      <td>1.389561</td>\n",
       "      <td>1.860390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.669475</td>\n",
       "      <td>4.087589</td>\n",
       "      <td>2.650473</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.412808</td>\n",
       "      <td>0.602065</td>\n",
       "      <td>0.860344</td>\n",
       "      <td>0.740192</td>\n",
       "      <td>-0.417224</td>\n",
       "      <td>-1.152910</td>\n",
       "      <td>-0.262419</td>\n",
       "      <td>0.602065</td>\n",
       "      <td>1.011425</td>\n",
       "      <td>1.422233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.237534</td>\n",
       "      <td>6.036658</td>\n",
       "      <td>2.524395</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.350947</td>\n",
       "      <td>0.470368</td>\n",
       "      <td>0.871598</td>\n",
       "      <td>0.759682</td>\n",
       "      <td>0.216610</td>\n",
       "      <td>-0.865017</td>\n",
       "      <td>-0.559836</td>\n",
       "      <td>0.470368</td>\n",
       "      <td>0.849711</td>\n",
       "      <td>1.411894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.006488</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.674702</td>\n",
       "      <td>7.140066</td>\n",
       "      <td>3.768688</td>\n",
       "      <td>2.538102</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.268645</td>\n",
       "      <td>-0.620746</td>\n",
       "      <td>1.062333</td>\n",
       "      <td>1.128552</td>\n",
       "      <td>1.905095</td>\n",
       "      <td>3.268406</td>\n",
       "      <td>-1.053646</td>\n",
       "      <td>-0.620746</td>\n",
       "      <td>-0.213910</td>\n",
       "      <td>1.118297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.660826</td>\n",
       "      <td>3.291511</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.061178</td>\n",
       "      <td>-0.247054</td>\n",
       "      <td>0.810107</td>\n",
       "      <td>0.656273</td>\n",
       "      <td>0.315406</td>\n",
       "      <td>-1.275492</td>\n",
       "      <td>-0.870641</td>\n",
       "      <td>-0.247054</td>\n",
       "      <td>0.627636</td>\n",
       "      <td>1.170910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.988808</td>\n",
       "      <td>5.358670</td>\n",
       "      <td>2.497202</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.236846</td>\n",
       "      <td>0.320672</td>\n",
       "      <td>0.891537</td>\n",
       "      <td>0.794837</td>\n",
       "      <td>-0.037296</td>\n",
       "      <td>-1.229062</td>\n",
       "      <td>-0.767698</td>\n",
       "      <td>0.320672</td>\n",
       "      <td>0.949033</td>\n",
       "      <td>1.227913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>6.949335</td>\n",
       "      <td>3.118009</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.144833</td>\n",
       "      <td>0.340536</td>\n",
       "      <td>0.743961</td>\n",
       "      <td>0.553478</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-1.121968</td>\n",
       "      <td>-0.596491</td>\n",
       "      <td>0.340536</td>\n",
       "      <td>0.604644</td>\n",
       "      <td>1.026987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.600851</td>\n",
       "      <td>7.063154</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.087574</td>\n",
       "      <td>0.163513</td>\n",
       "      <td>0.785633</td>\n",
       "      <td>0.617219</td>\n",
       "      <td>-0.085402</td>\n",
       "      <td>-1.405019</td>\n",
       "      <td>-0.775600</td>\n",
       "      <td>0.163513</td>\n",
       "      <td>0.701606</td>\n",
       "      <td>1.137678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>5.812112</td>\n",
       "      <td>3.151962</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    up_bytes_mean  up_bytes_median  up_bytes_std  up_bytes_var  up_bytes_skew  \\\n",
       "0       -0.049954        -0.403884      0.903263      0.815885       0.936888   \n",
       "1       -0.555438        -1.038336      0.953788      0.909712       2.042770   \n",
       "2       -0.429697        -0.797330      1.049213      1.100848       1.963251   \n",
       "3       -0.290295        -0.852862      1.105632      1.222422       1.520949   \n",
       "4        0.490188         0.728495      1.230044      1.513008       0.312373   \n",
       "5        0.412808         0.602065      0.860344      0.740192      -0.417224   \n",
       "6        0.350947         0.470368      0.871598      0.759682       0.216610   \n",
       "7       -0.268645        -0.620746      1.062333      1.128552       1.905095   \n",
       "8       -0.061178        -0.247054      0.810107      0.656273       0.315406   \n",
       "9        0.236846         0.320672      0.891537      0.794837      -0.037296   \n",
       "10       0.144833         0.340536      0.743961      0.553478      -0.272444   \n",
       "11       0.087574         0.163513      0.785633      0.617219      -0.085402   \n",
       "\n",
       "    up_bytes_kurt  up_bytes_perc25  up_bytes_perc50  up_bytes_perc75  \\\n",
       "0       -0.152483        -0.777905        -0.403884         0.496817   \n",
       "1        3.353170        -1.076309        -1.038336        -0.688625   \n",
       "2        2.620611        -1.040970        -0.797330        -0.604283   \n",
       "3        1.379564        -1.076309        -0.852862         0.033406   \n",
       "4       -0.971798        -0.726433         0.728495         1.389561   \n",
       "5       -1.152910        -0.262419         0.602065         1.011425   \n",
       "6       -0.865017        -0.559836         0.470368         0.849711   \n",
       "7        3.268406        -1.053646        -0.620746        -0.213910   \n",
       "8       -1.275492        -0.870641        -0.247054         0.627636   \n",
       "9       -1.229062        -0.767698         0.320672         0.949033   \n",
       "10      -1.121968        -0.596491         0.340536         0.604644   \n",
       "11      -1.405019        -0.775600         0.163513         0.701606   \n",
       "\n",
       "    up_bytes_perc90         ...          down_packet_1min_y  \\\n",
       "0          1.335181         ...                    0.002346   \n",
       "1          0.728165         ...                    0.015163   \n",
       "2          1.413431         ...                    0.000837   \n",
       "3          1.490957         ...                    0.013611   \n",
       "4          1.860390         ...                    0.001233   \n",
       "5          1.422233         ...                    0.008492   \n",
       "6          1.411894         ...                    0.003404   \n",
       "7          1.118297         ...                    0.002405   \n",
       "8          1.170910         ...                    0.006532   \n",
       "9          1.227913         ...                    0.016661   \n",
       "10         1.026987         ...                    0.001585   \n",
       "11         1.137678         ...                    0.001604   \n",
       "\n",
       "    down_packet_2min_y  down_packet_3min_y  down_packet_4min_y  \\\n",
       "0             0.001808            0.001815           -1.000000   \n",
       "1             0.000547           -1.000000           -1.000000   \n",
       "2             0.002284           -1.000000           -1.000000   \n",
       "3             0.002952            0.002953           -1.000000   \n",
       "4             0.004270            0.003137           -1.000000   \n",
       "5             0.002504            0.000737           -1.000000   \n",
       "6             0.006488            0.001413            0.003516   \n",
       "7             0.002918           -1.000000           -1.000000   \n",
       "8             0.004598            0.000632           -1.000000   \n",
       "9             0.001908            0.000587           -1.000000   \n",
       "10            0.002463           -1.000000           -1.000000   \n",
       "11            0.004513            0.010919           -1.000000   \n",
       "\n",
       "    down_packet_5min_y  down_packet_1min_x  down_packet_2min_x  \\\n",
       "0                 -1.0           11.071279            3.978398   \n",
       "1                 -1.0           11.814609            6.727171   \n",
       "2                 -1.0           10.262960            3.220981   \n",
       "3                 -1.0           12.676306            7.336032   \n",
       "4                 -1.0            9.669475            4.087589   \n",
       "5                 -1.0           13.237534            6.036658   \n",
       "6                 -1.0           13.674702            7.140066   \n",
       "7                 -1.0            7.660826            3.291511   \n",
       "8                 -1.0            9.988808            5.358670   \n",
       "9                 -1.0           12.337687            6.949335   \n",
       "10                -1.0           13.600851            7.063154   \n",
       "11                -1.0           13.454343            5.812112   \n",
       "\n",
       "    down_packet_3min_x  down_packet_4min_x  down_packet_5min_x  \n",
       "0             2.497202           -1.000000                -1.0  \n",
       "1            -1.000000           -1.000000                -1.0  \n",
       "2            -1.000000           -1.000000                -1.0  \n",
       "3             4.808627           -1.000000                -1.0  \n",
       "4             2.650473           -1.000000                -1.0  \n",
       "5             2.524395           -1.000000                -1.0  \n",
       "6             3.768688            2.538102                -1.0  \n",
       "7            -1.000000           -1.000000                -1.0  \n",
       "8             2.497202           -1.000000                -1.0  \n",
       "9             3.118009           -1.000000                -1.0  \n",
       "10           -1.000000           -1.000000                -1.0  \n",
       "11            3.151962           -1.000000                -1.0  \n",
       "\n",
       "[12 rows x 132 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
