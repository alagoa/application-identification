{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "\n",
    "def calc_scalogram(data, scales):\n",
    "    S,scales= scalogramCWT(data,scales)\n",
    "    return S\n",
    "def show_scalo(data, scales, colors):\n",
    "    for i in range (0, len(data)):\n",
    "        plt.plot(scales, data[i], colors[i], lw=3)\n",
    "    plt.show()\n",
    "    \n",
    "# Get top X spikes from scalogram, sorted by value\n",
    "def get_spikes(scalo, comparator):\n",
    "    len(scalo)\n",
    "    spikes = deque([(-1,-1)] * 5, maxlen=5)\n",
    "    #aux = argrelextrema(scalo, comparator, order=int(len(scalo)/10))\n",
    "    aux = argrelextrema(scalo, comparator)\n",
    "    if aux[0].size:\n",
    "        for x in np.nditer(aux) or []:\n",
    "            spikes.append((scalo[x], scales[x]))\n",
    "    ordered = sorted(spikes, key=lambda x: x[1], reverse=True)\n",
    "    values = np.hstack(zip(*ordered))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from scipy import stats\n",
    "def get_stats_numpy(data):\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    std = np.std(data)\n",
    "    var = np.var(data)\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    pc = [25,50,75,90]\n",
    "    percentiles = np.array(np.percentile(data, pc))\n",
    "    silences = np.count_nonzero(np.asarray(data)==0.0)\n",
    "    silence_mean = np.mean(list(sum(1 for _ in g) for k, g in groupby(data) if k==0))\n",
    "    longest_silence = max(sum(1 for _ in g) for k, g in groupby(data) if k==0) if silences > 0 else 0\n",
    "    shortest_silence = min(sum(1 for _ in g) for k, g in groupby(data) if k==0) if silences > 0 else 0\n",
    "    \n",
    "    #print(\"Mean: \" + str(mean))\n",
    "    #print(\"Media: \" + str(median))\n",
    "    #print(\"StdDev: \" + str(std))\n",
    "    #print(\"Variance: \" + str(var))\n",
    "    #print(\"Skewness: \" + str(skew))\n",
    "    #print(\"Kurtosis: \" + str(kurt))\n",
    "    #print(\"Pc25: \" + str(percentiles[0]))\n",
    "    #print(\"Pc50: \" + str(percentiles[1]))\n",
    "    #print(\"Pc75: \" + str(percentiles[2]))\n",
    "    \n",
    "    features = np.hstack((mean, median, std, var, skew, kurt, percentiles, silences, silence_mean, longest_silence, shortest_silence))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all on a numpy array\n",
    "def get_features_numpy(info, local_max_up_bytes, local_min_up_bytes, local_max_up_packet, local_min_up_packet, \n",
    "                       local_max_down_bytes, local_min_down_bytes, local_max_down_packet, local_min_down_packet):\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for idx in range(0, len(info)):\n",
    "        result.append(\n",
    "            np.hstack(\n",
    "                (\n",
    "                 get_stats_numpy(info[idx]['up']['byte_count']),\n",
    "                 get_stats_numpy(info[idx]['up']['packet_count']),\n",
    "                 local_max_up_bytes[idx], local_min_up_bytes[idx],\n",
    "                 local_max_up_packet[idx], local_min_up_packet[idx],\n",
    "                 get_stats_numpy(info[idx]['down']['byte_count']),\n",
    "                 get_stats_numpy(info[idx]['down']['packet_count']),\n",
    "                 local_max_down_bytes[idx], local_min_down_bytes[idx],\n",
    "                 local_max_down_packet[idx], local_min_down_packet[idx],\n",
    "\n",
    "            ))\n",
    "        )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agro/work/thesis/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/agro/work/thesis/env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "base_url = '../../../shared/normal/video/netflix/dat/1s'\n",
    "file = 'netflix10.pcapng_1.00s.dat'\n",
    "sample_size = 30 # number of intervals for each sample\n",
    "#[up_n_packets, up_n_bytes, up_flag, down_n_packets, down_n_bytes, down_flag]\n",
    "data = np.loadtxt(os.path.join(base_url, file))\n",
    "data = np.delete(data, [2,5], 1)\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "info = []\n",
    "up = defaultdict(list)\n",
    "down = defaultdict(list)\n",
    "result = {}\n",
    "count = 0\n",
    "for second in data:\n",
    "    up['packet_count'].append(second[0])\n",
    "    up['byte_count'].append(second[1])\n",
    "    down['packet_count'].append(second[2])\n",
    "    down['byte_count'].append(second[3])\n",
    "    count+=1\n",
    "    if count >= sample_size:\n",
    "        result['up'] = up\n",
    "        result['down'] = down\n",
    "        info.append(result)\n",
    "        up = defaultdict(list)\n",
    "        down = defaultdict(list)\n",
    "        result = {}\n",
    "        count = 0\n",
    "    \n",
    "from scalogram import *\n",
    "\n",
    "#Scalogram\n",
    "N = sample_size\n",
    "dj=1/128\n",
    "s0=2\n",
    "J=1/dj * np.log2(0.5*N/s0)\n",
    "scales=s0*2**(np.arange(J)*dj)\n",
    "\n",
    "scalos_up = []\n",
    "scalos_down = []\n",
    "for idx, sample in enumerate(info):\n",
    "        scalos_up.append(\n",
    "            (calc_scalogram(np.asarray(sample['up']['byte_count']), scales),\n",
    "             calc_scalogram(np.asarray(sample['up']['packet_count']), scales))\n",
    "        )\n",
    "        scalos_down.append(\n",
    "            (calc_scalogram(np.asarray(sample['down']['byte_count']), scales),\n",
    "             calc_scalogram(np.asarray(sample['down']['packet_count']), scales))\n",
    "        )\n",
    "#    show_scalo([scalos_down[idx], scalos_up[idx]], scales, ['r', 'b'])\n",
    "#smooth_down = np.convolve(scalo_down, np.ones(len(scalo_down)), mode='same')\n",
    "#smooth_up = np.convolve(scalo_up, np.ones(len(scalo_up)), mode='same')\n",
    "#show_scalo([smooth_down, smooth_up], scales, ['r', 'b'])\n",
    "\n",
    "#scalo, scales = calc_and_show(np.asarray(stats['down']['packet_count']), 'r')\n",
    "#scalo, scales = calc_and_show(np.asarray(stats['up']['packet_count']), 'b')\n",
    "\n",
    "\n",
    "local_max_up_bytes = []\n",
    "local_min_up_bytes = []\n",
    "local_max_up_packet = []\n",
    "local_min_up_packet = []\n",
    "local_max_down_bytes = []\n",
    "local_min_down_bytes = []\n",
    "local_max_down_packet = []\n",
    "local_min_down_packet = []\n",
    "\n",
    "\n",
    "for scalo in scalos_up:\n",
    "    local_max_up_bytes.append(get_spikes(scalo[0], np.greater))\n",
    "    local_min_up_bytes.append(get_spikes(scalo[0], np.less))\n",
    "    local_max_up_packet.append(get_spikes(scalo[1], np.greater))\n",
    "    local_min_up_packet.append(get_spikes(scalo[1], np.less))\n",
    "\n",
    "for scalo in scalos_down:\n",
    "    local_max_down_bytes.append(get_spikes(scalo[0], np.greater))\n",
    "    local_min_down_bytes.append(get_spikes(scalo[0], np.less))\n",
    "    local_max_down_packet.append(get_spikes(scalo[1], np.greater))\n",
    "    local_min_down_packet.append(get_spikes(scalo[1], np.less))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "samples = get_features_numpy(info, local_max_up_bytes, local_min_up_bytes, local_max_up_packet, local_min_up_packet,\n",
    "                            local_max_down_bytes, local_min_down_bytes, local_max_down_packet, local_min_down_packet)\n",
    "\n",
    "\n",
    "names = [\n",
    "    'up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var', 'up_bytes_skew', 'up_bytes_kurt',\n",
    "    'up_bytes_perc25', 'up_bytes_perc50', 'up_bytes_perc75', 'up_bytes_perc90',\n",
    "    'up_bytes_silences', 'up_bytes_silence_mean', 'up_bytes_longest_silence', 'up_bytes_shortest_silence',\n",
    "    'up_packet_mean', 'up_packet_median', 'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
    "    'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75', 'up_packet_perc90',\n",
    "    'up_packet_silences', 'up_packet_silence_mean', 'up_packet_longest_silence', 'up_packet_shortest_silence',\n",
    "    'up_bytes_1max_y', 'up_bytes_2max_y', 'up_bytes_3max_y', 'up_bytes_4max_y', 'up_bytes_5max_y',\n",
    "    'up_bytes_1max_x', 'up_bytes_2max_x', 'up_bytes_3max_x', 'up_bytes_4max_x', 'up_bytes_5max_x',\n",
    "    'up_bytes_1min_y', 'up_bytes_2min_y', 'up_bytes_3min_y', 'up_bytes_4min_y', 'up_bytes_5min_y',\n",
    "    'up_bytes_1min_x', 'up_bytes_2min_x', 'up_bytes_3min_x', 'up_bytes_4min_x', 'up_bytes_5min_x',\n",
    "    'up_packet_1max_y', 'up_packet_2max_y', 'up_packet_3max_y', 'up_packet_4max_y', 'up_packet_5max_y',\n",
    "    'up_packet_1max_x', 'up_packet_2max_x', 'up_packet_3max_x', 'up_packet_4max_x', 'up_packet_5max_x',\n",
    "    'up_packet_1min_y', 'up_packet_2min_y', 'up_packet_3min_y', 'up_packet_4min_y', 'up_packet_5min_y',\n",
    "    'up_packet_1min_x', 'up_packet_2min_x', 'up_packet_3min_x', 'up_packet_4min_x', 'up_packet_5min_x',\n",
    "\n",
    "    'down_bytes_mean', 'down_bytes_median', 'down_bytes_std', 'down_bytes_var', 'down_bytes_skew', 'down_bytes_kurt',\n",
    "    'down_bytes_perc25', 'down_bytes_perc50', 'down_bytes_perc75', 'down_bytes_perc90',\n",
    "    'down_bytes_silences', 'down_bytes_silence_mean', 'down_bytes_longest_silence', 'down_bytes_shortest_silence',\n",
    "    'down_packet_mean', 'down_packet_median', 'down_packet_std', 'down_packet_var', 'down_packet_skew', 'down_packet_kurt',\n",
    "    'down_packet_perc25', 'down_packet_perc50', 'down_packet_perc75', 'down_packet_perc90',\n",
    "    'down_packet_silences', 'down_packet_silence_mean', 'down_packet_longest_silence', 'down_packet_shortest_silence',\n",
    "    'down_bytes_1max_y', 'down_bytes_2max_y', 'down_bytes_3max_y', 'down_bytes_4max_y', 'down_bytes_5max_y',\n",
    "    'down_bytes_1max_x', 'down_bytes_2max_x', 'down_bytes_3max_x', 'down_bytes_4max_x', 'down_bytes_5max_x',\n",
    "    'down_bytes_1min_y', 'down_bytes_2min_y', 'down_bytes_3min_y', 'down_bytes_4min_y', 'down_bytes_5min_y',\n",
    "    'down_bytes_1min_x', 'down_bytes_2min_x', 'down_bytes_3min_x', 'down_bytes_4min_x', 'down_bytes_5min_x',\n",
    "    'down_packet_1max_y', 'down_packet_2max_y', 'down_packet_3max_y', 'down_packet_4max_y', 'down_packet_5max_y',\n",
    "    'down_packet_1max_x', 'down_packet_2max_x', 'down_packet_3max_x', 'down_packet_4max_x', 'down_packet_5max_x',\n",
    "    'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_3min_y', 'down_packet_4min_y', 'down_packet_5min_y',\n",
    "    'down_packet_1min_x', 'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x'\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(samples, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
