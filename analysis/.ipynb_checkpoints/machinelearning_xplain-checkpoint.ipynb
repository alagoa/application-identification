{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_number(s):\n",
    "    return list(filter(None, re.split(r'(\\d+)', s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_stats = [\n",
    "    'up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var', 'up_bytes_skew', 'up_bytes_kurt',\n",
    "    'up_bytes_perc25', 'up_bytes_perc50', 'up_bytes_perc75', 'up_bytes_perc90',\n",
    "    'up_packet_mean', 'up_packet_median', 'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
    "    'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75', 'up_packet_perc90',\n",
    "    'down_bytes_mean', 'down_bytes_median', 'down_bytes_std', 'down_bytes_var', 'down_bytes_skew', 'down_bytes_kurt',\n",
    "    'down_bytes_perc25', 'down_bytes_perc50', 'down_bytes_perc75', 'down_bytes_perc90',\n",
    "    'down_packet_mean', 'down_packet_median', 'down_packet_std', 'down_packet_var', 'down_packet_skew', 'down_packet_kurt',\n",
    "    'down_packet_perc25', 'down_packet_perc50', 'down_packet_perc75', 'down_packet_perc90']\n",
    "\n",
    "silences = ['down_bytes_silences', 'down_bytes_silence_mean', 'down_bytes_longest_silence', 'down_bytes_shortest_silence',\n",
    "           'up_bytes_silences', 'up_bytes_silence_mean', 'up_bytes_longest_silence', 'up_bytes_shortest_silence']\n",
    "\n",
    "scalogram = ['up_bytes_1max_y', 'up_bytes_2max_y', 'up_bytes_3max_y', 'up_bytes_4max_y', 'up_bytes_5max_y',\n",
    "    'up_bytes_1max_x', 'up_bytes_2max_x', 'up_bytes_3max_x', 'up_bytes_4max_x', 'up_bytes_5max_x',\n",
    "    'up_bytes_1min_y', 'up_bytes_2min_y', 'up_bytes_3min_y', 'up_bytes_4min_y', 'up_bytes_5min_y',\n",
    "    'up_bytes_1min_x', 'up_bytes_2min_x', 'up_bytes_3min_x', 'up_bytes_4min_x', 'up_bytes_5min_x',\n",
    "    'up_packet_1max_y', 'up_packet_2max_y', 'up_packet_3max_y', 'up_packet_4max_y', 'up_packet_5max_y',\n",
    "    'up_packet_1max_x', 'up_packet_2max_x', 'up_packet_3max_x', 'up_packet_4max_x', 'up_packet_5max_x',\n",
    "    'up_packet_1min_y', 'up_packet_2min_y', 'up_packet_2min_y', 'up_packet_4min_y', 'up_packet_5min_y',\n",
    "    'up_packet_1min_x', 'up_packet_2min_x', 'up_packet_3min_x', 'up_packet_4min_x', 'up_packet_5min_x',\n",
    "    'down_bytes_1max_y', 'down_bytes_2max_y', 'down_bytes_3max_y', 'down_bytes_4max_y', 'down_bytes_5max_y',\n",
    "    'down_bytes_1max_x', 'down_bytes_2max_x', 'down_bytes_3max_x', 'down_bytes_4max_x', 'down_bytes_5max_x',\n",
    "    'down_bytes_1min_y', 'down_bytes_2min_y', 'down_bytes_3min_y', 'down_bytes_4min_y', 'down_bytes_5min_y',\n",
    "    'down_bytes_1min_x', 'down_bytes_2min_x', 'down_bytes_3min_x', 'down_bytes_4min_x', 'down_bytes_5min_x',\n",
    "    'down_packet_1max_y', 'down_packet_2max_y', 'down_packet_3max_y', 'down_packet_4max_y', 'down_packet_5max_y',\n",
    "    'down_packet_1max_x', 'down_packet_2max_x', 'down_packet_3max_x', 'down_packet_4max_x', 'down_packet_5max_x',\n",
    "    'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_2min_y', 'down_packet_4min_y', 'down_packet_5min_y',\n",
    "    'down_packet_1min_x', 'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var',\n",
      "       'up_bytes_skew', 'up_bytes_kurt', 'up_bytes_perc25', 'up_bytes_perc50',\n",
      "       'up_bytes_perc75', 'up_bytes_perc90',\n",
      "       ...\n",
      "       'down_packet_2min_y', 'down_packet_3min_y', 'down_packet_4min_y',\n",
      "       'down_packet_5min_y', 'down_packet_1min_x', 'down_packet_2min_x',\n",
      "       'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x',\n",
      "       'label'],\n",
      "      dtype='object', length=133)\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_folder = \"csv/30s1.00s/\"\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for path, subdirs, files in os.walk(base_folder):\n",
    "    for name in files:\n",
    "        data = pd.read_csv(os.path.join(str(path), str(name)))\n",
    "        if(split_number(name)[0] == 'cap'):\n",
    "            continue\n",
    "        data['label'] = split_number(name)[0]\n",
    "        dataset = pd.concat([dataset, data])\n",
    "dataset = dataset.drop(columns=['Unnamed: 0']).reset_index()\n",
    "dataset.drop(columns=['index', 'up_packet_silence_mean', 'down_packet_silence_mean',\n",
    "                      'down_packet_longest_silence', 'down_packet_shortest_silence'], inplace=True)\n",
    "features = dataset.columns\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "netflix      569\n",
       "youtube      461\n",
       "acestream    248\n",
       "twitch       225\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset['label'] == 'netflix-ssh', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'youtube-ssh', 'label'] = 'youtube'\n",
    "dataset.loc[dataset['label'] == 'twitch-ssh.csv', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'netflix-openvpn.csv', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'youtube-openvpn.csv', 'label'] = 'youtube'\n",
    "dataset.loc[dataset['label'] == 'twitch-openvpn.csv', 'label'] = 'twitch'\n",
    "dataset.loc[dataset['label'] == 'acestream-openvpn.csv', 'label'] = 'acestream'\n",
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var',\n",
       "       'up_bytes_skew', 'up_bytes_kurt', 'up_bytes_perc25', 'up_bytes_perc50',\n",
       "       'up_bytes_perc75', 'up_bytes_perc90',\n",
       "       ...\n",
       "       'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_3min_y',\n",
       "       'down_packet_4min_y', 'down_packet_5min_y', 'down_packet_1min_x',\n",
       "       'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x',\n",
       "       'down_packet_5min_x'],\n",
       "      dtype='object', length=132)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test some stuff\n",
    "#dataset.drop(columns=silences, inplace=True)\n",
    "dataset.columns\n",
    "features = dataset.columns[:-1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "5       0\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      0\n",
       "11      0\n",
       "12      0\n",
       "13      0\n",
       "14      0\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      0\n",
       "19      0\n",
       "20      0\n",
       "21      0\n",
       "22      0\n",
       "23      0\n",
       "24      0\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      0\n",
       "       ..\n",
       "1473    2\n",
       "1474    2\n",
       "1475    2\n",
       "1476    2\n",
       "1477    2\n",
       "1478    2\n",
       "1479    2\n",
       "1480    2\n",
       "1481    2\n",
       "1482    2\n",
       "1483    2\n",
       "1484    2\n",
       "1485    2\n",
       "1486    2\n",
       "1487    2\n",
       "1488    2\n",
       "1489    2\n",
       "1490    2\n",
       "1491    2\n",
       "1492    2\n",
       "1493    2\n",
       "1494    2\n",
       "1495    2\n",
       "1496    2\n",
       "1497    2\n",
       "1498    2\n",
       "1499    2\n",
       "1500    2\n",
       "1501    2\n",
       "1502    2\n",
       "Name: label, Length: 1503, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevlabel = dataset['label']\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "labels = dataset['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                          1.000000\n",
       "up_bytes_silence_mean          0.642040\n",
       "down_bytes_silence_mean        0.641925\n",
       "up_bytes_shortest_silence      0.533203\n",
       "up_packet_shortest_silence     0.533203\n",
       "down_bytes_shortest_silence    0.533203\n",
       "down_bytes_perc25              0.500448\n",
       "up_packet_longest_silence      0.460539\n",
       "up_bytes_longest_silence       0.460539\n",
       "down_packet_perc25             0.460033\n",
       "down_bytes_longest_silence     0.455390\n",
       "up_bytes_perc25                0.404766\n",
       "up_packet_perc25               0.391466\n",
       "up_packet_silences             0.331956\n",
       "up_bytes_silences              0.331956\n",
       "down_bytes_silences            0.318348\n",
       "down_packet_silences           0.318348\n",
       "down_packet_median             0.198868\n",
       "down_packet_perc50             0.198868\n",
       "up_packet_perc50               0.183013\n",
       "up_packet_median               0.183013\n",
       "down_bytes_perc50              0.178715\n",
       "down_bytes_median              0.178715\n",
       "up_bytes_median                0.132959\n",
       "up_bytes_perc50                0.132959\n",
       "down_bytes_mean               -0.006488\n",
       "down_packet_mean              -0.007788\n",
       "up_packet_mean                -0.008220\n",
       "up_bytes_mean                 -0.008907\n",
       "up_bytes_5min_x               -0.057707\n",
       "                                 ...   \n",
       "down_bytes_2max_x             -0.455066\n",
       "up_packet_2max_x              -0.455672\n",
       "up_bytes_2min_x               -0.456682\n",
       "up_packet_1max_x              -0.476059\n",
       "down_packet_2min_x            -0.477088\n",
       "up_bytes_1max_x               -0.478111\n",
       "up_packet_2min_x              -0.480164\n",
       "down_packet_1max_x            -0.485629\n",
       "down_bytes_2min_x             -0.503025\n",
       "down_bytes_2max_y             -0.516712\n",
       "down_bytes_1max_x             -0.518143\n",
       "up_packet_1min_x              -0.519181\n",
       "up_bytes_1min_x               -0.523921\n",
       "down_bytes_2min_y             -0.524426\n",
       "down_packet_2max_y            -0.524474\n",
       "up_bytes_2max_y               -0.525285\n",
       "up_packet_2max_y              -0.526027\n",
       "up_bytes_2min_y               -0.527989\n",
       "down_packet_2min_y            -0.528717\n",
       "up_packet_2min_y              -0.530965\n",
       "down_packet_1min_x            -0.535485\n",
       "up_bytes_1max_y               -0.538109\n",
       "down_packet_1max_y            -0.538228\n",
       "up_packet_1max_y              -0.538368\n",
       "up_bytes_1min_y               -0.538431\n",
       "down_bytes_1min_y             -0.538663\n",
       "down_packet_1min_y            -0.538703\n",
       "up_packet_1min_y              -0.538849\n",
       "down_bytes_1max_y             -0.539248\n",
       "down_bytes_1min_x             -0.570525\n",
       "Name: label, Length: 133, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()['label'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer for NaN\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(dataset)\n",
    "dataset = pd.DataFrame(imputer.transform(dataset), columns=dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.preprocessing import normalize\\n# Normalize data\\ndataset_no_label = dataset.loc[:, dataset.columns != 'label']\\n\\n#dataset = (dataset_no_label - dataset_no_label.mean()) / (dataset_no_label.max() - dataset_no_label.min())\\ndataset_normalized = normalize(dataset_no_label)\\ndataset = pd.DataFrame(dataset_normalized, columns=features)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import normalize\n",
    "# Normalize data\n",
    "dataset_no_label = dataset.loc[:, dataset.columns != 'label']\n",
    "\n",
    "#dataset = (dataset_no_label - dataset_no_label.mean()) / (dataset_no_label.max() - dataset_no_label.min())\n",
    "dataset_normalized = normalize(dataset_no_label)\n",
    "dataset = pd.DataFrame(dataset_normalized, columns=features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components=10)\\nmain_components = pca.fit_transform(dataset)\\ndataset = pd.DataFrame(data = main_components)\\ndataset['label'] = labels\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "main_components = pca.fit_transform(dataset)\n",
    "dataset = pd.DataFrame(data = main_components)\n",
    "dataset['label'] = labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = train['label']\n",
    "x_train = train.drop(columns=['label'])\n",
    "\n",
    "y_test = test['label']\n",
    "x_test = test.drop(columns=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_youtube = (y_train == 0)\n",
    "y_train_netflix = (y_train == 1)\n",
    "y_train_twitch = (y_train == 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_bytes_mean</th>\n",
       "      <th>up_bytes_median</th>\n",
       "      <th>up_bytes_std</th>\n",
       "      <th>up_bytes_var</th>\n",
       "      <th>up_bytes_skew</th>\n",
       "      <th>up_bytes_kurt</th>\n",
       "      <th>up_bytes_perc25</th>\n",
       "      <th>up_bytes_perc50</th>\n",
       "      <th>up_bytes_perc75</th>\n",
       "      <th>up_bytes_perc90</th>\n",
       "      <th>...</th>\n",
       "      <th>down_packet_1min_y</th>\n",
       "      <th>down_packet_2min_y</th>\n",
       "      <th>down_packet_3min_y</th>\n",
       "      <th>down_packet_4min_y</th>\n",
       "      <th>down_packet_5min_y</th>\n",
       "      <th>down_packet_1min_x</th>\n",
       "      <th>down_packet_2min_x</th>\n",
       "      <th>down_packet_3min_x</th>\n",
       "      <th>down_packet_4min_x</th>\n",
       "      <th>down_packet_5min_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10200</th>\n",
       "      <td>-0.079130</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>3.386415e-01</td>\n",
       "      <td>1.146781e-01</td>\n",
       "      <td>5.189428</td>\n",
       "      <td>24.965753</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.008896</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.539757</td>\n",
       "      <td>8.630086</td>\n",
       "      <td>6.800425</td>\n",
       "      <td>3.935542</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13412</th>\n",
       "      <td>-0.107161</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>4.092421e-01</td>\n",
       "      <td>1.674791e-01</td>\n",
       "      <td>2.671848</td>\n",
       "      <td>6.338245</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.092536</td>\n",
       "      <td>0.121833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013330</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.561446</td>\n",
       "      <td>7.336032</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14671</th>\n",
       "      <td>0.226701</td>\n",
       "      <td>-0.174761</td>\n",
       "      <td>9.336432e-01</td>\n",
       "      <td>8.716896e-01</td>\n",
       "      <td>0.789465</td>\n",
       "      <td>-0.249575</td>\n",
       "      <td>-0.601404</td>\n",
       "      <td>-0.174761</td>\n",
       "      <td>0.802801</td>\n",
       "      <td>1.218680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>4.604757</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>-0.578880</td>\n",
       "      <td>-0.706428</td>\n",
       "      <td>2.063335e-01</td>\n",
       "      <td>4.257351e-02</td>\n",
       "      <td>1.247258</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>-0.726231</td>\n",
       "      <td>-0.706428</td>\n",
       "      <td>-0.450378</td>\n",
       "      <td>-0.322992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.814342</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>4.433474</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12451</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>1.446792</td>\n",
       "      <td>1.243354</td>\n",
       "      <td>1.704337e+00</td>\n",
       "      <td>2.904765e+00</td>\n",
       "      <td>1.428606</td>\n",
       "      <td>1.797063</td>\n",
       "      <td>0.027553</td>\n",
       "      <td>1.243354</td>\n",
       "      <td>2.139456</td>\n",
       "      <td>3.579851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>12.745137</td>\n",
       "      <td>8.354190</td>\n",
       "      <td>6.441961</td>\n",
       "      <td>4.199776</td>\n",
       "      <td>2.579672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6824</th>\n",
       "      <td>-0.172218</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>5.152611e-02</td>\n",
       "      <td>2.654940e-03</td>\n",
       "      <td>3.478627</td>\n",
       "      <td>10.112128</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>3.809727</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>-0.106012</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>9.486510e-02</td>\n",
       "      <td>8.999387e-03</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.106012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>0.017239</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>5.482904e-01</td>\n",
       "      <td>3.006224e-01</td>\n",
       "      <td>3.517940</td>\n",
       "      <td>10.605252</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.115549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>-0.052365</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>2.674398e-01</td>\n",
       "      <td>7.152402e-02</td>\n",
       "      <td>3.278747</td>\n",
       "      <td>9.655424</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.091012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.583479</td>\n",
       "      <td>7.140066</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6423</th>\n",
       "      <td>0.096918</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>1.036407e+00</td>\n",
       "      <td>1.074139e+00</td>\n",
       "      <td>2.357437</td>\n",
       "      <td>4.352739</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>1.741536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>1.035536</td>\n",
       "      <td>1.204041</td>\n",
       "      <td>1.126177e+00</td>\n",
       "      <td>1.268274e+00</td>\n",
       "      <td>0.534846</td>\n",
       "      <td>-0.127047</td>\n",
       "      <td>-0.111013</td>\n",
       "      <td>1.204041</td>\n",
       "      <td>1.581805</td>\n",
       "      <td>2.092415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.138872</td>\n",
       "      <td>9.209514</td>\n",
       "      <td>6.004056</td>\n",
       "      <td>2.813152</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>0.744933</td>\n",
       "      <td>0.393486</td>\n",
       "      <td>1.138062e+00</td>\n",
       "      <td>1.295185e+00</td>\n",
       "      <td>1.284652</td>\n",
       "      <td>1.732834</td>\n",
       "      <td>-0.129568</td>\n",
       "      <td>0.393486</td>\n",
       "      <td>1.352132</td>\n",
       "      <td>2.018318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.018744</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>9.259521</td>\n",
       "      <td>5.417022</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>-0.165610</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.672593e-01</td>\n",
       "      <td>7.142754e-02</td>\n",
       "      <td>5.199469</td>\n",
       "      <td>25.034483</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.915094</td>\n",
       "      <td>7.257009</td>\n",
       "      <td>3.648207</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>-0.235707</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>4.944251e-02</td>\n",
       "      <td>2.444562e-03</td>\n",
       "      <td>5.199469</td>\n",
       "      <td>25.034483</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>-0.244888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.009141</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>9.110309</td>\n",
       "      <td>6.168843</td>\n",
       "      <td>4.362031</td>\n",
       "      <td>3.327353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>-0.066480</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>2.760103e-01</td>\n",
       "      <td>7.618166e-02</td>\n",
       "      <td>3.511800</td>\n",
       "      <td>10.430744</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>8.630086</td>\n",
       "      <td>4.705588</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10434</th>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>-0.111469</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>7.586769e-01</td>\n",
       "      <td>5.755906e-01</td>\n",
       "      <td>1.375917</td>\n",
       "      <td>0.486632</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>0.107275</td>\n",
       "      <td>1.351853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.934862</td>\n",
       "      <td>3.002028</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8299</th>\n",
       "      <td>-0.036418</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>4.516375e-01</td>\n",
       "      <td>2.039765e-01</td>\n",
       "      <td>3.644875</td>\n",
       "      <td>11.706503</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.152899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006217</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.819061</td>\n",
       "      <td>7.063154</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12194</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13254</th>\n",
       "      <td>0.021080</td>\n",
       "      <td>-0.278323</td>\n",
       "      <td>5.107240e-01</td>\n",
       "      <td>2.608390e-01</td>\n",
       "      <td>1.529409</td>\n",
       "      <td>0.755960</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.278323</td>\n",
       "      <td>0.093251</td>\n",
       "      <td>1.037905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011902</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.375141</td>\n",
       "      <td>5.687570</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13133</th>\n",
       "      <td>-0.125453</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>4.474782e-01</td>\n",
       "      <td>2.002367e-01</td>\n",
       "      <td>2.388625</td>\n",
       "      <td>4.032556</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>0.398656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009781</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>-0.297805</td>\n",
       "      <td>-0.587556</td>\n",
       "      <td>7.111195e-01</td>\n",
       "      <td>5.056909e-01</td>\n",
       "      <td>2.733056</td>\n",
       "      <td>7.711204</td>\n",
       "      <td>-0.676266</td>\n",
       "      <td>-0.587556</td>\n",
       "      <td>-0.403423</td>\n",
       "      <td>0.498857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10.318687</td>\n",
       "      <td>4.604757</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>0.919651</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>2.930435e+00</td>\n",
       "      <td>8.587452e+00</td>\n",
       "      <td>3.312911</td>\n",
       "      <td>10.782022</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>0.389587</td>\n",
       "      <td>2.315089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.138872</td>\n",
       "      <td>9.061108</td>\n",
       "      <td>4.994404</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>1.232595e-32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>-0.072641</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>2.898558e-01</td>\n",
       "      <td>8.401641e-02</td>\n",
       "      <td>3.874849</td>\n",
       "      <td>13.857175</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>-0.147340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>4.457547</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>-0.046688</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>2.979990e-01</td>\n",
       "      <td>8.880342e-02</td>\n",
       "      <td>3.155160</td>\n",
       "      <td>8.574877</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.140016</td>\n",
       "      <td>-0.089774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>8.771433</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>1.047699</td>\n",
       "      <td>1.115511</td>\n",
       "      <td>1.031005e+00</td>\n",
       "      <td>1.062970e+00</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>-0.763709</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>1.115511</td>\n",
       "      <td>1.736664</td>\n",
       "      <td>2.367135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.823610</td>\n",
       "      <td>5.103767</td>\n",
       "      <td>2.443692</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>8.326673e-17</td>\n",
       "      <td>6.933348e-33</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>-0.200795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>-0.106554</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>8.765227e-01</td>\n",
       "      <td>7.682920e-01</td>\n",
       "      <td>3.630779</td>\n",
       "      <td>11.662082</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.347224</td>\n",
       "      <td>-0.318167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>8.676947</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>-0.024967</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>6.356027e-01</td>\n",
       "      <td>4.039908e-01</td>\n",
       "      <td>3.580091</td>\n",
       "      <td>12.177900</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.122374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>8.491018</td>\n",
       "      <td>7.178836</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>-0.149123</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>3.281929e-01</td>\n",
       "      <td>1.077106e-01</td>\n",
       "      <td>5.141715</td>\n",
       "      <td>24.626476</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>5.048789</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>-0.042417</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>3.901545e-01</td>\n",
       "      <td>1.522206e-01</td>\n",
       "      <td>3.977671</td>\n",
       "      <td>14.858090</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>-0.146523</td>\n",
       "      <td>-0.096588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.404679</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>7.025009</td>\n",
       "      <td>4.043557</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6949</th>\n",
       "      <td>-0.011085</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>9.418646e-01</td>\n",
       "      <td>8.871090e-01</td>\n",
       "      <td>5.199469</td>\n",
       "      <td>25.034483</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.008995</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>8.537123</td>\n",
       "      <td>7.296414</td>\n",
       "      <td>4.433474</td>\n",
       "      <td>3.118009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13803</th>\n",
       "      <td>0.101709</td>\n",
       "      <td>0.175556</td>\n",
       "      <td>6.146590e-01</td>\n",
       "      <td>3.778057e-01</td>\n",
       "      <td>0.632431</td>\n",
       "      <td>0.481458</td>\n",
       "      <td>-0.601404</td>\n",
       "      <td>0.175556</td>\n",
       "      <td>0.559144</td>\n",
       "      <td>0.690269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7.702424</td>\n",
       "      <td>4.132100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10583</th>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>-0.144257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0.071715</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>2.285503e+00</td>\n",
       "      <td>5.223523e+00</td>\n",
       "      <td>4.647858</td>\n",
       "      <td>20.905451</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.409953</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.073315</td>\n",
       "      <td>3.002028</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>0.915837</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>4.873022e+00</td>\n",
       "      <td>2.374634e+01</td>\n",
       "      <td>5.123177</td>\n",
       "      <td>24.503634</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>-0.135179</td>\n",
       "      <td>0.475555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010680</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.204785</td>\n",
       "      <td>8.866947</td>\n",
       "      <td>3.101170</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>-0.156688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11363</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11636</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14423</th>\n",
       "      <td>0.211697</td>\n",
       "      <td>-0.211945</td>\n",
       "      <td>8.647401e-01</td>\n",
       "      <td>7.477754e-01</td>\n",
       "      <td>0.439924</td>\n",
       "      <td>-1.482853</td>\n",
       "      <td>-0.601404</td>\n",
       "      <td>-0.211945</td>\n",
       "      <td>1.129633</td>\n",
       "      <td>1.391295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018997</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.943262</td>\n",
       "      <td>5.300947</td>\n",
       "      <td>2.378414</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>0.645961</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>8.055966e-01</td>\n",
       "      <td>6.489859e-01</td>\n",
       "      <td>0.563289</td>\n",
       "      <td>-0.225174</td>\n",
       "      <td>0.036065</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>1.145215</td>\n",
       "      <td>1.781550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>14.357673</td>\n",
       "      <td>6.069436</td>\n",
       "      <td>2.607763</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>-0.231659</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>4.860568e-01</td>\n",
       "      <td>2.362513e-01</td>\n",
       "      <td>2.544847</td>\n",
       "      <td>4.962687</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>-0.412035</td>\n",
       "      <td>0.343230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.271056</td>\n",
       "      <td>8.630086</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.409919</td>\n",
       "      <td>0.285072</td>\n",
       "      <td>9.661747e-01</td>\n",
       "      <td>9.334936e-01</td>\n",
       "      <td>0.438157</td>\n",
       "      <td>-1.240765</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>0.285072</td>\n",
       "      <td>1.174056</td>\n",
       "      <td>1.875546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.411176</td>\n",
       "      <td>6.202340</td>\n",
       "      <td>3.668016</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0.135417</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>9.367235e-01</td>\n",
       "      <td>8.774509e-01</td>\n",
       "      <td>3.217303</td>\n",
       "      <td>9.663538</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>0.605631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>13.166044</td>\n",
       "      <td>7.101506</td>\n",
       "      <td>3.978398</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>-0.192527</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>6.847011e-02</td>\n",
       "      <td>4.688156e-03</td>\n",
       "      <td>3.154413</td>\n",
       "      <td>8.639927</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.160226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.472035</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.021523</td>\n",
       "      <td>3.668016</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>0.806106</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.507614e+00</td>\n",
       "      <td>6.288129e+00</td>\n",
       "      <td>3.525821</td>\n",
       "      <td>13.080674</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>0.370213</td>\n",
       "      <td>3.237917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.607847</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13418</th>\n",
       "      <td>-0.116688</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>3.896848e-01</td>\n",
       "      <td>1.518542e-01</td>\n",
       "      <td>2.567905</td>\n",
       "      <td>5.847283</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.306905</td>\n",
       "      <td>-0.063953</td>\n",
       "      <td>0.207581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.005751</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10.097579</td>\n",
       "      <td>5.780723</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>7.703720e-34</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.273103</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>4.494021e-01</td>\n",
       "      <td>2.019623e-01</td>\n",
       "      <td>1.360759</td>\n",
       "      <td>0.914041</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>-0.587750</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>0.278606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>11.313708</td>\n",
       "      <td>7.496671</td>\n",
       "      <td>2.874752</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>-0.137634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.724062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12153 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       up_bytes_mean  up_bytes_median  up_bytes_std  up_bytes_var  \\\n",
       "10200      -0.079130        -0.144257  3.386415e-01  1.146781e-01   \n",
       "13412      -0.107161        -0.306905  4.092421e-01  1.674791e-01   \n",
       "14671       0.226701        -0.174761  9.336432e-01  8.716896e-01   \n",
       "4073       -0.578880        -0.706428  2.063335e-01  4.257351e-02   \n",
       "12451       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "8074        1.446792         1.243354  1.704337e+00  2.904765e+00   \n",
       "6824       -0.172218        -0.185985  5.152611e-02  2.654940e-03   \n",
       "7387       -0.106012        -0.137634  9.486510e-02  8.999387e-03   \n",
       "2055        0.017239        -0.135179  5.482904e-01  3.006224e-01   \n",
       "11597       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "2058       -0.052365        -0.135179  2.674398e-01  7.152402e-02   \n",
       "11902       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "6423        0.096918        -0.347224  1.036407e+00  1.074139e+00   \n",
       "4546        1.035536         1.204041  1.126177e+00  1.268274e+00   \n",
       "4755        0.744933         0.393486  1.138062e+00  1.295185e+00   \n",
       "5556       -0.165610        -0.215238  2.672593e-01  7.142754e-02   \n",
       "9564       -0.235707        -0.244888  4.944251e-02  2.444562e-03   \n",
       "3509       -0.066480        -0.140016  2.760103e-01  7.618166e-02   \n",
       "10434      -0.144257        -0.144257  2.775558e-17  7.703720e-34   \n",
       "7337       -0.137634        -0.137634  0.000000e+00  0.000000e+00   \n",
       "483        -0.111469        -0.587750  7.586769e-01  5.755906e-01   \n",
       "8299       -0.036418        -0.156688  4.516375e-01  2.039765e-01   \n",
       "12048       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "12194       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "13254       0.021080        -0.278323  5.107240e-01  2.608390e-01   \n",
       "13133      -0.125453        -0.306905  4.474782e-01  2.002367e-01   \n",
       "4296       -0.297805        -0.587556  7.111195e-01  5.056909e-01   \n",
       "6306        0.919651        -0.211623  2.930435e+00  8.587452e+00   \n",
       "6507       -0.347224        -0.347224  1.110223e-16  1.232595e-32   \n",
       "2984       -0.072641        -0.147340  2.898558e-01  8.401641e-02   \n",
       "...              ...              ...           ...           ...   \n",
       "3385       -0.046688        -0.140016  2.979990e-01  8.880342e-02   \n",
       "4555        1.047699         1.115511  1.031005e+00  1.062970e+00   \n",
       "1184       -0.200795        -0.200795  8.326673e-17  6.933348e-33   \n",
       "6420       -0.106554        -0.347224  8.765227e-01  7.682920e-01   \n",
       "5051       -0.024967        -0.215238  6.356027e-01  4.039908e-01   \n",
       "5311       -0.149123        -0.215238  3.281929e-01  1.077106e-01   \n",
       "2433       -0.042417        -0.146523  3.901545e-01  1.522206e-01   \n",
       "6949       -0.011085        -0.185985  9.418646e-01  8.871090e-01   \n",
       "13803       0.101709         0.175556  6.146590e-01  3.778057e-01   \n",
       "10583      -0.144257        -0.144257  2.775558e-17  7.703720e-34   \n",
       "769         0.071715        -0.587750  2.285503e+00  5.223523e+00   \n",
       "1685        0.915837        -0.135179  4.873022e+00  2.374634e+01   \n",
       "8322       -0.156688        -0.156688  2.775558e-17  7.703720e-34   \n",
       "11111       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "11363       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "11636       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "14423       0.211697        -0.211945  8.647401e-01  7.477754e-01   \n",
       "5578       -0.215238        -0.215238  2.775558e-17  7.703720e-34   \n",
       "4426        0.645961         0.473058  8.055966e-01  6.489859e-01   \n",
       "13526      -0.231659        -0.412035  4.860568e-01  2.362513e-01   \n",
       "466         0.409919         0.285072  9.661747e-01  9.334936e-01   \n",
       "6265        0.135417        -0.211623  9.367235e-01  8.774509e-01   \n",
       "5734       -0.192527        -0.215238  6.847011e-02  4.688156e-03   \n",
       "11284       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "11964       0.000000         0.000000  0.000000e+00  0.000000e+00   \n",
       "5191        0.806106        -0.215238  2.507614e+00  6.288129e+00   \n",
       "13418      -0.116688        -0.306905  3.896848e-01  1.518542e-01   \n",
       "5390       -0.215238        -0.215238  2.775558e-17  7.703720e-34   \n",
       "860        -0.273103        -0.587750  4.494021e-01  2.019623e-01   \n",
       "7270       -0.137634        -0.137634  0.000000e+00  0.000000e+00   \n",
       "\n",
       "       up_bytes_skew  up_bytes_kurt  up_bytes_perc25  up_bytes_perc50  \\\n",
       "10200       5.189428      24.965753        -0.144257        -0.144257   \n",
       "13412       2.671848       6.338245        -0.306905        -0.306905   \n",
       "14671       0.789465      -0.249575        -0.601404        -0.174761   \n",
       "4073        1.247258       0.331092        -0.726231        -0.706428   \n",
       "12451       0.000000      -3.000000         0.000000         0.000000   \n",
       "8074        1.428606       1.797063         0.027553         1.243354   \n",
       "6824        3.478627      10.112128        -0.185985        -0.185985   \n",
       "7387        2.666667       5.111111        -0.137634        -0.137634   \n",
       "2055        3.517940      10.605252        -0.135179        -0.135179   \n",
       "11597       0.000000      -3.000000         0.000000         0.000000   \n",
       "2058        3.278747       9.655424        -0.135179        -0.135179   \n",
       "11902       0.000000      -3.000000         0.000000         0.000000   \n",
       "6423        2.357437       4.352739        -0.347224        -0.347224   \n",
       "4546        0.534846      -0.127047        -0.111013         1.204041   \n",
       "4755        1.284652       1.732834        -0.129568         0.393486   \n",
       "5556        5.199469      25.034483        -0.215238        -0.215238   \n",
       "9564        5.199469      25.034483        -0.244888        -0.244888   \n",
       "3509        3.511800      10.430744        -0.140016        -0.140016   \n",
       "10434       1.000000      -2.000000        -0.144257        -0.144257   \n",
       "7337        0.000000      -3.000000        -0.137634        -0.137634   \n",
       "483         1.375917       0.486632        -0.587750        -0.587750   \n",
       "8299        3.644875      11.706503        -0.156688        -0.156688   \n",
       "12048       0.000000      -3.000000         0.000000         0.000000   \n",
       "12194       0.000000      -3.000000         0.000000         0.000000   \n",
       "13254       1.529409       0.755960        -0.306905        -0.278323   \n",
       "13133       2.388625       4.032556        -0.306905        -0.306905   \n",
       "4296        2.733056       7.711204        -0.676266        -0.587556   \n",
       "6306        3.312911      10.782022        -0.211623        -0.211623   \n",
       "6507        1.000000      -2.000000        -0.347224        -0.347224   \n",
       "2984        3.874849      13.857175        -0.147340        -0.147340   \n",
       "...              ...            ...              ...              ...   \n",
       "3385        3.155160       8.574877        -0.140016        -0.140016   \n",
       "4555        0.058908      -0.763709         0.199106         1.115511   \n",
       "1184       -1.000000      -2.000000        -0.200795        -0.200795   \n",
       "6420        3.630779      11.662082        -0.347224        -0.347224   \n",
       "5051        3.580091      12.177900        -0.215238        -0.215238   \n",
       "5311        5.141715      24.626476        -0.215238        -0.215238   \n",
       "2433        3.977671      14.858090        -0.146523        -0.146523   \n",
       "6949        5.199469      25.034483        -0.185985        -0.185985   \n",
       "13803       0.632431       0.481458        -0.601404         0.175556   \n",
       "10583       1.000000      -2.000000        -0.144257        -0.144257   \n",
       "769         4.647858      20.905451        -0.587750        -0.587750   \n",
       "1685        5.123177      24.503634        -0.135179        -0.135179   \n",
       "8322        1.000000      -2.000000        -0.156688        -0.156688   \n",
       "11111       0.000000      -3.000000         0.000000         0.000000   \n",
       "11363       0.000000      -3.000000         0.000000         0.000000   \n",
       "11636       0.000000      -3.000000         0.000000         0.000000   \n",
       "14423       0.439924      -1.482853        -0.601404        -0.211945   \n",
       "5578        1.000000      -2.000000        -0.215238        -0.215238   \n",
       "4426        0.563289      -0.225174         0.036065         0.473058   \n",
       "13526       2.544847       4.962687        -0.412035        -0.412035   \n",
       "466         0.438157      -1.240765        -0.587750         0.285072   \n",
       "6265        3.217303       9.663538        -0.211623        -0.211623   \n",
       "5734        3.154413       8.639927        -0.215238        -0.215238   \n",
       "11284       0.000000      -3.000000         0.000000         0.000000   \n",
       "11964       0.000000      -3.000000         0.000000         0.000000   \n",
       "5191        3.525821      13.080674        -0.215238        -0.215238   \n",
       "13418       2.567905       5.847283        -0.306905        -0.306905   \n",
       "5390        1.000000      -2.000000        -0.215238        -0.215238   \n",
       "860         1.360759       0.914041        -0.587750        -0.587750   \n",
       "7270        0.000000      -3.000000        -0.137634        -0.137634   \n",
       "\n",
       "       up_bytes_perc75  up_bytes_perc90         ...          \\\n",
       "10200        -0.144257        -0.144257         ...           \n",
       "13412        -0.092536         0.121833         ...           \n",
       "14671         0.802801         1.218680         ...           \n",
       "4073         -0.450378        -0.322992         ...           \n",
       "12451         0.000000         0.000000         ...           \n",
       "8074          2.139456         3.579851         ...           \n",
       "6824         -0.185985        -0.185985         ...           \n",
       "7387         -0.137634        -0.106012         ...           \n",
       "2055         -0.135179        -0.115549         ...           \n",
       "11597         0.000000         0.000000         ...           \n",
       "2058         -0.135179        -0.091012         ...           \n",
       "11902         0.000000         0.000000         ...           \n",
       "6423         -0.347224         1.741536         ...           \n",
       "4546          1.581805         2.092415         ...           \n",
       "4755          1.352132         2.018318         ...           \n",
       "5556         -0.215238        -0.215238         ...           \n",
       "9564         -0.244888        -0.244888         ...           \n",
       "3509         -0.140016        -0.140016         ...           \n",
       "10434        -0.144257        -0.144257         ...           \n",
       "7337         -0.137634        -0.137634         ...           \n",
       "483           0.107275         1.351853         ...           \n",
       "8299         -0.156688        -0.152899         ...           \n",
       "12048         0.000000         0.000000         ...           \n",
       "12194         0.000000         0.000000         ...           \n",
       "13254         0.093251         1.037905         ...           \n",
       "13133        -0.306905         0.398656         ...           \n",
       "4296         -0.403423         0.498857         ...           \n",
       "6306          0.389587         2.315089         ...           \n",
       "6507         -0.347224        -0.347224         ...           \n",
       "2984         -0.147340        -0.147340         ...           \n",
       "...                ...              ...         ...           \n",
       "3385         -0.140016        -0.089774         ...           \n",
       "4555          1.736664         2.367135         ...           \n",
       "1184         -0.200795        -0.200795         ...           \n",
       "6420         -0.347224        -0.318167         ...           \n",
       "5051         -0.215238        -0.122374         ...           \n",
       "5311         -0.215238        -0.215238         ...           \n",
       "2433         -0.146523        -0.096588         ...           \n",
       "6949         -0.185985        -0.185985         ...           \n",
       "13803         0.559144         0.690269         ...           \n",
       "10583        -0.144257        -0.144257         ...           \n",
       "769          -0.409953         0.110507         ...           \n",
       "1685         -0.135179         0.475555         ...           \n",
       "8322         -0.156688        -0.156688         ...           \n",
       "11111         0.000000         0.000000         ...           \n",
       "11363         0.000000         0.000000         ...           \n",
       "11636         0.000000         0.000000         ...           \n",
       "14423         1.129633         1.391295         ...           \n",
       "5578         -0.215238        -0.215238         ...           \n",
       "4426          1.145215         1.781550         ...           \n",
       "13526        -0.412035         0.343230         ...           \n",
       "466           1.174056         1.875546         ...           \n",
       "6265         -0.211623         0.605631         ...           \n",
       "5734         -0.215238        -0.160226         ...           \n",
       "11284         0.000000         0.000000         ...           \n",
       "11964         0.000000         0.000000         ...           \n",
       "5191          0.370213         3.237917         ...           \n",
       "13418        -0.063953         0.207581         ...           \n",
       "5390         -0.215238        -0.215238         ...           \n",
       "860           0.026458         0.278606         ...           \n",
       "7270         -0.137634        -0.137634         ...           \n",
       "\n",
       "       down_packet_1min_y  down_packet_2min_y  down_packet_3min_y  \\\n",
       "10200            0.005860            0.008896            0.008737   \n",
       "13412            0.013330            0.001572           -1.000000   \n",
       "14671            0.005574            0.001867           -1.000000   \n",
       "4073             0.003252            0.008237            0.003024   \n",
       "12451           -1.000000           -1.000000           -1.000000   \n",
       "8074             0.005365            0.009584            0.005290   \n",
       "6824             0.008514            0.011564            0.002802   \n",
       "7387             0.008994            0.012208           -1.000000   \n",
       "2055             0.007299            0.009431           -1.000000   \n",
       "11597           -1.000000           -1.000000           -1.000000   \n",
       "2058             0.009107            0.011621            0.011696   \n",
       "11902           -1.000000           -1.000000           -1.000000   \n",
       "6423             0.010421           -1.000000           -1.000000   \n",
       "4546             0.009793            0.012066            0.007865   \n",
       "4755             0.002496            0.018744            0.005859   \n",
       "5556             0.005173            0.007579            0.008852   \n",
       "9564             0.005896            0.008164            0.008838   \n",
       "3509             0.006175            0.008511            0.009006   \n",
       "10434            0.008994            0.012208           -1.000000   \n",
       "7337             0.008994            0.012208           -1.000000   \n",
       "483              0.001784            0.001820           -1.000000   \n",
       "8299             0.006217            0.008375            0.009037   \n",
       "12048           -1.000000           -1.000000           -1.000000   \n",
       "12194           -1.000000           -1.000000           -1.000000   \n",
       "13254            0.011902            0.003610            0.001850   \n",
       "13133            0.009781           -1.000000           -1.000000   \n",
       "4296             0.001411            0.004333           -1.000000   \n",
       "6306             0.007444            0.009868            0.002316   \n",
       "6507             0.008994            0.012208           -1.000000   \n",
       "2984             0.006574            0.008910            0.008079   \n",
       "...                   ...                 ...                 ...   \n",
       "3385             0.007261            0.011334           -1.000000   \n",
       "4555             0.000537            0.004518            0.001292   \n",
       "1184             0.008994            0.012208           -1.000000   \n",
       "6420             0.008899            0.011104           -1.000000   \n",
       "5051             0.007093            0.009288            0.009181   \n",
       "5311             0.006422            0.008797            0.009226   \n",
       "2433             0.005691            0.008660            0.010063   \n",
       "6949             0.005804            0.008341            0.008488   \n",
       "13803            0.001880            0.000771           -1.000000   \n",
       "10583            0.008994            0.012208           -1.000000   \n",
       "769              0.013393            0.000425           -1.000000   \n",
       "1685             0.010680            0.012899            0.001598   \n",
       "8322             0.008994            0.012208           -1.000000   \n",
       "11111           -1.000000           -1.000000           -1.000000   \n",
       "11363           -1.000000           -1.000000           -1.000000   \n",
       "11636           -1.000000           -1.000000           -1.000000   \n",
       "14423            0.018997            0.001050            0.000263   \n",
       "5578             0.008994            0.012208           -1.000000   \n",
       "4426             0.000967            0.005850            0.001068   \n",
       "13526            0.009898            0.011989           -1.000000   \n",
       "466              0.002531            0.001531            0.001629   \n",
       "6265             0.006037            0.005097            0.006484   \n",
       "5734             0.006585            0.003787            0.007560   \n",
       "11284           -1.000000           -1.000000           -1.000000   \n",
       "11964           -1.000000           -1.000000           -1.000000   \n",
       "5191             0.011995           -1.000000           -1.000000   \n",
       "13418            0.003703            0.005751           -1.000000   \n",
       "5390             0.008994            0.012208           -1.000000   \n",
       "860              0.015683            0.002318            0.000409   \n",
       "7270             0.008994            0.012208           -1.000000   \n",
       "\n",
       "       down_packet_4min_y  down_packet_5min_y  down_packet_1min_x  \\\n",
       "10200            0.008919           -1.000000           12.539757   \n",
       "13412           -1.000000           -1.000000           11.561446   \n",
       "14671           -1.000000           -1.000000           13.454343   \n",
       "4073            -1.000000           -1.000000           12.814342   \n",
       "12451           -1.000000           -1.000000           -1.000000   \n",
       "8074             0.008255            0.008069           12.745137   \n",
       "6824            -1.000000           -1.000000           12.337687   \n",
       "7387            -1.000000           -1.000000           12.337687   \n",
       "2055            -1.000000           -1.000000           12.337687   \n",
       "11597           -1.000000           -1.000000           -1.000000   \n",
       "2058            -1.000000           -1.000000           12.337687   \n",
       "11902           -1.000000           -1.000000           -1.000000   \n",
       "6423            -1.000000           -1.000000           12.472035   \n",
       "4546             0.002583           -1.000000           12.138872   \n",
       "4755            -1.000000           -1.000000           13.454343   \n",
       "5556             0.009160           -1.000000           12.337687   \n",
       "9564             0.009101            0.009141           12.271056   \n",
       "3509            -1.000000           -1.000000           12.404679   \n",
       "10434           -1.000000           -1.000000           12.337687   \n",
       "7337            -1.000000           -1.000000           12.337687   \n",
       "483             -1.000000           -1.000000            9.934862   \n",
       "8299            -1.000000           -1.000000           12.337687   \n",
       "12048           -1.000000           -1.000000           -1.000000   \n",
       "12194           -1.000000           -1.000000           -1.000000   \n",
       "13254           -1.000000           -1.000000           11.375141   \n",
       "13133           -1.000000           -1.000000           12.404679   \n",
       "4296            -1.000000           -1.000000           10.318687   \n",
       "6306            -1.000000           -1.000000           12.138872   \n",
       "6507            -1.000000           -1.000000           12.337687   \n",
       "2984            -1.000000           -1.000000           12.337687   \n",
       "...                   ...                 ...                 ...   \n",
       "3385            -1.000000           -1.000000           12.472035   \n",
       "4555            -1.000000           -1.000000           13.823610   \n",
       "1184            -1.000000           -1.000000           12.337687   \n",
       "6420            -1.000000           -1.000000           12.271056   \n",
       "5051            -1.000000           -1.000000           12.404679   \n",
       "5311            -1.000000           -1.000000           12.271056   \n",
       "2433             0.008434           -1.000000           12.404679   \n",
       "6949             0.008995            0.009132           12.472035   \n",
       "13803           -1.000000           -1.000000            7.702424   \n",
       "10583           -1.000000           -1.000000           12.337687   \n",
       "769             -1.000000           -1.000000           12.073315   \n",
       "1685            -1.000000           -1.000000           12.204785   \n",
       "8322            -1.000000           -1.000000           12.337687   \n",
       "11111           -1.000000           -1.000000           -1.000000   \n",
       "11363           -1.000000           -1.000000           -1.000000   \n",
       "11636           -1.000000           -1.000000           -1.000000   \n",
       "14423           -1.000000           -1.000000           11.943262   \n",
       "5578            -1.000000           -1.000000           12.337687   \n",
       "4426            -1.000000           -1.000000           14.357673   \n",
       "13526           -1.000000           -1.000000           12.271056   \n",
       "466             -1.000000           -1.000000            9.411176   \n",
       "6265            -1.000000           -1.000000           13.166044   \n",
       "5734             0.007705           -1.000000           12.472035   \n",
       "11284           -1.000000           -1.000000           -1.000000   \n",
       "11964           -1.000000           -1.000000           -1.000000   \n",
       "5191            -1.000000           -1.000000           12.607847   \n",
       "13418           -1.000000           -1.000000           10.097579   \n",
       "5390            -1.000000           -1.000000           12.337687   \n",
       "860             -1.000000           -1.000000           11.313708   \n",
       "7270            -1.000000           -1.000000           12.337687   \n",
       "\n",
       "       down_packet_2min_x  down_packet_3min_x  down_packet_4min_x  \\\n",
       "10200            8.630086            6.800425            3.935542   \n",
       "13412            7.336032           -1.000000           -1.000000   \n",
       "14671            4.604757           -1.000000           -1.000000   \n",
       "4073             8.963502            4.433474           -1.000000   \n",
       "12451           -1.000000           -1.000000           -1.000000   \n",
       "8074             8.354190            6.441961            4.199776   \n",
       "6824             8.724062            3.809727           -1.000000   \n",
       "7387             8.724062           -1.000000           -1.000000   \n",
       "2055             8.724062           -1.000000           -1.000000   \n",
       "11597           -1.000000           -1.000000           -1.000000   \n",
       "2058             8.583479            7.140066           -1.000000   \n",
       "11902           -1.000000           -1.000000           -1.000000   \n",
       "6423            -1.000000           -1.000000           -1.000000   \n",
       "4546             9.209514            6.004056            2.813152   \n",
       "4755             9.259521            5.417022           -1.000000   \n",
       "5556             8.915094            7.257009            3.648207   \n",
       "9564             9.110309            6.168843            4.362031   \n",
       "3509             8.630086            4.705588           -1.000000   \n",
       "10434            8.724062           -1.000000           -1.000000   \n",
       "7337             8.724062           -1.000000           -1.000000   \n",
       "483              3.002028           -1.000000           -1.000000   \n",
       "8299             8.819061            7.063154           -1.000000   \n",
       "12048           -1.000000           -1.000000           -1.000000   \n",
       "12194           -1.000000           -1.000000           -1.000000   \n",
       "13254            5.687570            2.828427           -1.000000   \n",
       "13133           -1.000000           -1.000000           -1.000000   \n",
       "4296             4.604757           -1.000000           -1.000000   \n",
       "6306             9.061108            4.994404           -1.000000   \n",
       "6507             8.724062           -1.000000           -1.000000   \n",
       "2984             8.724062            4.457547           -1.000000   \n",
       "...                   ...                 ...                 ...   \n",
       "3385             8.771433           -1.000000           -1.000000   \n",
       "4555             5.103767            2.443692           -1.000000   \n",
       "1184             8.724062           -1.000000           -1.000000   \n",
       "6420             8.676947           -1.000000           -1.000000   \n",
       "5051             8.491018            7.178836           -1.000000   \n",
       "5311             8.963502            5.048789           -1.000000   \n",
       "2433             8.963502            7.025009            4.043557   \n",
       "6949             8.537123            7.296414            4.433474   \n",
       "13803            4.132100           -1.000000           -1.000000   \n",
       "10583            8.724062           -1.000000           -1.000000   \n",
       "769              3.002028           -1.000000           -1.000000   \n",
       "1685             8.866947            3.101170           -1.000000   \n",
       "8322             8.724062           -1.000000           -1.000000   \n",
       "11111           -1.000000           -1.000000           -1.000000   \n",
       "11363           -1.000000           -1.000000           -1.000000   \n",
       "11636           -1.000000           -1.000000           -1.000000   \n",
       "14423            5.300947            2.378414           -1.000000   \n",
       "5578             8.724062           -1.000000           -1.000000   \n",
       "4426             6.069436            2.607763           -1.000000   \n",
       "13526            8.630086           -1.000000           -1.000000   \n",
       "466              6.202340            3.668016           -1.000000   \n",
       "6265             7.101506            3.978398           -1.000000   \n",
       "5734             8.000000            5.021523            3.668016   \n",
       "11284           -1.000000           -1.000000           -1.000000   \n",
       "11964           -1.000000           -1.000000           -1.000000   \n",
       "5191            -1.000000           -1.000000           -1.000000   \n",
       "13418            5.780723           -1.000000           -1.000000   \n",
       "5390             8.724062           -1.000000           -1.000000   \n",
       "860              7.496671            2.874752           -1.000000   \n",
       "7270             8.724062           -1.000000           -1.000000   \n",
       "\n",
       "       down_packet_5min_x  \n",
       "10200           -1.000000  \n",
       "13412           -1.000000  \n",
       "14671           -1.000000  \n",
       "4073            -1.000000  \n",
       "12451           -1.000000  \n",
       "8074             2.579672  \n",
       "6824            -1.000000  \n",
       "7387            -1.000000  \n",
       "2055            -1.000000  \n",
       "11597           -1.000000  \n",
       "2058            -1.000000  \n",
       "11902           -1.000000  \n",
       "6423            -1.000000  \n",
       "4546            -1.000000  \n",
       "4755            -1.000000  \n",
       "5556            -1.000000  \n",
       "9564             3.327353  \n",
       "3509            -1.000000  \n",
       "10434           -1.000000  \n",
       "7337            -1.000000  \n",
       "483             -1.000000  \n",
       "8299            -1.000000  \n",
       "12048           -1.000000  \n",
       "12194           -1.000000  \n",
       "13254           -1.000000  \n",
       "13133           -1.000000  \n",
       "4296            -1.000000  \n",
       "6306            -1.000000  \n",
       "6507            -1.000000  \n",
       "2984            -1.000000  \n",
       "...                   ...  \n",
       "3385            -1.000000  \n",
       "4555            -1.000000  \n",
       "1184            -1.000000  \n",
       "6420            -1.000000  \n",
       "5051            -1.000000  \n",
       "5311            -1.000000  \n",
       "2433            -1.000000  \n",
       "6949             3.118009  \n",
       "13803           -1.000000  \n",
       "10583           -1.000000  \n",
       "769             -1.000000  \n",
       "1685            -1.000000  \n",
       "8322            -1.000000  \n",
       "11111           -1.000000  \n",
       "11363           -1.000000  \n",
       "11636           -1.000000  \n",
       "14423           -1.000000  \n",
       "5578            -1.000000  \n",
       "4426            -1.000000  \n",
       "13526           -1.000000  \n",
       "466             -1.000000  \n",
       "6265            -1.000000  \n",
       "5734            -1.000000  \n",
       "11284           -1.000000  \n",
       "11964           -1.000000  \n",
       "5191            -1.000000  \n",
       "13418           -1.000000  \n",
       "5390            -1.000000  \n",
       "860             -1.000000  \n",
       "7270            -1.000000  \n",
       "\n",
       "[12153 rows x 132 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\\nimport matplotlib.pyplot as plt\\n\\nrandom_forest = RandomForestClassifier(random_state=42)\\ny_probas_forest_y = cross_val_predict(random_forest, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\\ny_probas_forest_n = cross_val_predict(random_forest, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\\ny_probas_forest_t = cross_val_predict(random_forest, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\\ny_probas_forest = cross_val_predict(random_forest, x_train, y_train, cv=10, method=\"predict_proba\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest_y = cross_val_predict(random_forest, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest_n = cross_val_predict(random_forest, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest_t = cross_val_predict(random_forest, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest = cross_val_predict(random_forest, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_scores_forest_y = y_probas_forest_y[:, 1]\\ny_scores_forest_n = y_probas_forest_n[:, 1]\\ny_scores_forest_t = y_probas_forest_t[:, 1]\\n\\nfpr_forest_y, tpr_forest_y, thresholds_forest_y = roc_curve(y_train_youtube, y_scores_forest_y)\\nfpr_forest_n, tpr_forest_n, thresholds_forest_n = roc_curve(y_train_netflix, y_scores_forest_n)\\nfpr_forest_t, tpr_forest_t, thresholds_forest_t = roc_curve(y_train_twitch, y_scores_forest_t)\\n#fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_probas_forest)\\n\\n\\nplot_roc_curve(fpr_forest_y, tpr_forest_y, \"YouTube\")\\nplot_roc_curve(fpr_forest_n, tpr_forest_n, \"Netflix\")\\nplot_roc_curve(fpr_forest_t, tpr_forest_t, \"Twitch\")\\n#plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\\n\\nplt.legend(loc=\"lower right\")\\nplt.show()\\n#cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring=\\'accuracy\\')\\n#accuracy = sum(cvs)/len(cvs)\\n#print(\"Accuracy: \" + str(accuracy))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "y_scores_forest_y = y_probas_forest_y[:, 1]\n",
    "y_scores_forest_n = y_probas_forest_n[:, 1]\n",
    "y_scores_forest_t = y_probas_forest_t[:, 1]\n",
    "\n",
    "fpr_forest_y, tpr_forest_y, thresholds_forest_y = roc_curve(y_train_youtube, y_scores_forest_y)\n",
    "fpr_forest_n, tpr_forest_n, thresholds_forest_n = roc_curve(y_train_netflix, y_scores_forest_n)\n",
    "fpr_forest_t, tpr_forest_t, thresholds_forest_t = roc_curve(y_train_twitch, y_scores_forest_t)\n",
    "#fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_probas_forest)\n",
    "\n",
    "\n",
    "plot_roc_curve(fpr_forest_y, tpr_forest_y, \"YouTube\")\n",
    "plot_roc_curve(fpr_forest_n, tpr_forest_n, \"Netflix\")\n",
    "plot_roc_curve(fpr_forest_t, tpr_forest_t, \"Twitch\")\n",
    "#plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "#cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "#accuracy = sum(cvs)/len(cvs)\n",
    "#print(\"Accuracy: \" + str(accuracy))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrandom_forest.fit(x_train, y_train)\\npredictions = random_forest.predict(x_test)\\nconf_mx = confusion_matrix(y_test, predictions)\\nplt.matshow(conf_mx, cmap=plt.cm.gray)\\nconf_mx\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "random_forest.fit(x_train, y_train)\n",
    "predictions = random_forest.predict(x_test)\n",
    "conf_mx = confusion_matrix(y_test, predictions)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "conf_mx\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search_acc = GridSearchCV(random_forest, params, cv=10, scoring='accuracy')\n",
    "#grid_search_acc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best2 = grid_search_acc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(zip(grid_search.best_estimator_.feature_importances_, basic_stats), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Final evaluation\\nrf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\\'entropy\\', max_depth=9, max_features=\\'log2\\', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=42, verbose=0, warm_start=False)\\nrf.fit(x_train, y_train)\\nprint(\"Accuracy train set: \" + str(sum(rf.predict(x_train) == y_train)/float(len(y_train))))\\nprint(\"Accuracy test set: \" + str(sum(rf.predict(x_test) == y_test)/float(len(y_test))))\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Final evaluation\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "rf.fit(x_train, y_train)\n",
    "print(\"Accuracy train set: \" + str(sum(rf.predict(x_train) == y_train)/float(len(y_train))))\n",
    "print(\"Accuracy test set: \" + str(sum(rf.predict(x_test) == y_test)/float(len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncrossval = cross_val_predict(rf, x_train, y_train, cv=10, method=\"predict_proba\")\\ncrossvalscore = cross_val_score(rf, x_train, y_train, cv=10, scoring=\"accuracy\")\\nprint(\"\\tCrossValScore: \" + str(sum(crossvalscore)/len(cv_yt)))\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "crossval = cross_val_predict(rf, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "crossvalscore = cross_val_score(rf, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "print(\"\\tCrossValScore: \" + str(sum(crossvalscore)/len(cv_yt)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_test_forest(model):\n",
    "    '''\n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    \n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    '''\n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    '''\n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "    '''\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def do_test_svm(model):    \n",
    "    '''\n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "    \n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    '''\n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    '''\n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    '''\n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test_knn(model):\n",
    "    \n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "    \n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    \n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    \n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Random Forest------\n",
      "[[ 436    0   16    0]\n",
      " [   0  922    3    0]\n",
      " [   0    1 1160    0]\n",
      " [   0    0    1  500]]\n",
      "\n",
      "Accuracy train set: 0.9994240105323788\n",
      "Accuracy test set: 0.9930898321816387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"-----OvO Classifier Random Forest------\")\\n\\nrf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\\'entropy\\', max_depth=9,\\n                            max_features=\\'log2\\', max_leaf_nodes=None, min_impurity_decrease=0.0,\\n                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\\n                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\\n                            random_state=42, verbose=0, warm_start=False)\\nrf = OneVsOneClassifier(rf)\\ndo_test_forest(rf)\\n\\nprint(\"----------SVM-----------\")\\nsvm = SVC(random_state=42, probability=True)\\ndo_test_svm(svm)\\n\\nprint(\"----------Knn-----------\")\\nknn = KNeighborsClassifier()\\ndo_test_knn(knn)\\n\\nprint(\"-----Neural Network-----\")\\nnn = MLPClassifier()\\ndo_test_knn(nn)\\n\\n\\nprint(\"--------AdaBoost Random Forest--------\")\\n\\nrf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\\'entropy\\', max_depth=9,\\n                            max_features=\\'log2\\', max_leaf_nodes=None, min_impurity_decrease=0.0,\\n                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\\n                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\\n                            random_state=42, verbose=0, warm_start=False)\\n\\nada_clf = AdaBoostClassifier(\\n    rf,\\n    algorithm=\"SAMME.R\", learning_rate=0.5\\n)\\ndo_test_forest(ada_clf)\\n\\nprint(\"---------Decision Tree------\")\\ndt = DecisionTreeClassifier(random_state=42)\\n\\ndo_test_forest(dt)\\n\\nprint(\"--------AdaBoost Decision Tree--------\")\\n\\ndt = DecisionTreeClassifier(random_state=42)\\n\\nada_clf = AdaBoostClassifier(\\n    dt,\\n    algorithm=\"SAMME.R\", learning_rate=0.5\\n)\\ndo_test_forest(ada_clf)\\n\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"-----Random Forest------\")\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=9, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "do_test_forest(rf)\n",
    "\n",
    "'''\n",
    "print(\"-----OvO Classifier Random Forest------\")\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9,\n",
    "                            max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\n",
    "                            random_state=42, verbose=0, warm_start=False)\n",
    "rf = OneVsOneClassifier(rf)\n",
    "do_test_forest(rf)\n",
    "\n",
    "print(\"----------SVM-----------\")\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "do_test_svm(svm)\n",
    "\n",
    "print(\"----------Knn-----------\")\n",
    "knn = KNeighborsClassifier()\n",
    "do_test_knn(knn)\n",
    "\n",
    "print(\"-----Neural Network-----\")\n",
    "nn = MLPClassifier()\n",
    "do_test_knn(nn)\n",
    "\n",
    "\n",
    "print(\"--------AdaBoost Random Forest--------\")\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9,\n",
    "                            max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\n",
    "                            random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    rf,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n",
    "\n",
    "print(\"---------Decision Tree------\")\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "do_test_forest(dt)\n",
    "\n",
    "print(\"--------AdaBoost Decision Tree--------\")\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    dt,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = dict()\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "params['forest'] = {\n",
    "    'max_depth' : [6,7,8,9],\n",
    "    'n_estimators': [30,100,300],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf' : [1, 2, 4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 3, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(random_forest, params['forest'], cv=10, scoring='accuracy')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "\n",
    "predictions = best_random_forest.predict(x_test)\n",
    "conf_mx = confusion_matrix(y_test, predictions)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "print(conf_mx)\n",
    "print(\"\\nAccuracy train set: \" + str(sum(best_random_forest.predict(x_train) == y_train)/float(len(y_train))))\n",
    "print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Random Forest------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3c6869eccbb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             warm_start=False)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdo_test_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-b767a046dc29>\u001b[0m in \u001b[0;36mdo_test_forest\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcv_nf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_netflix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcv_tw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_twitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mcv_mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     '''\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YouTube: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"-----Random Forest------\")\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=9, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "do_test_forest(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/rf_model_01s.sav']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf, '../models/rf_model_01s.sav') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.65946734 -1.73477919 -1.2318347  -4.22145108]\n",
      " [-0.34487931 -4.11526472 -1.37651383 -3.77594527]\n",
      " [-0.16042623 -3.82198385 -2.0832087  -6.31718965]\n",
      " [-0.49440497 -2.76821807 -1.18112638 -3.8941985 ]\n",
      " [-0.13421028 -4.403363   -2.19051664 -6.50311376]\n",
      " [-0.30400206 -3.16321299 -1.53384557 -5.48550383]\n",
      " [-0.17031451 -4.36427963 -1.9777458  -5.20376361]\n",
      " [-0.1332101  -4.05725409 -2.246051   -6.43394457]\n",
      " [-0.10791961 -3.83569323 -2.52406313 -7.45159518]\n",
      " [-0.25609308 -3.09667795 -1.73380679 -5.49186039]\n",
      " [-0.06928215 -4.4673744  -2.91726127 -6.58740486]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for path, subdirs, files in os.walk(base_folder):\n",
    "    for name in files:\n",
    "        if name == 'cap3.csv':\n",
    "            test = pd.read_csv(os.path.join(str(path), str(name)))\n",
    "test = test.drop(columns=['Unnamed: 0']).reset_index()\n",
    "test.drop(columns=['index', 'up_packet_silence_mean', 'down_packet_silence_mean',\n",
    "                      'down_packet_longest_silence', 'down_packet_shortest_silence','label'], inplace=True)\n",
    "# Imputer for NaN\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(test)\n",
    "test = pd.DataFrame(imputer.transform(test), columns=test.columns)\n",
    "print(rf.predict_log_proba(test))\n",
    "print(rf.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "rf = joblib.load('../models/rf_model_01s.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acestream\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prevlabel[4000])\n",
    "dataset['label'][4000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_bytes_mean</th>\n",
       "      <th>up_bytes_median</th>\n",
       "      <th>up_bytes_std</th>\n",
       "      <th>up_bytes_var</th>\n",
       "      <th>up_bytes_skew</th>\n",
       "      <th>up_bytes_kurt</th>\n",
       "      <th>up_bytes_perc25</th>\n",
       "      <th>up_bytes_perc50</th>\n",
       "      <th>up_bytes_perc75</th>\n",
       "      <th>up_bytes_perc90</th>\n",
       "      <th>...</th>\n",
       "      <th>down_packet_1min_y</th>\n",
       "      <th>down_packet_2min_y</th>\n",
       "      <th>down_packet_3min_y</th>\n",
       "      <th>down_packet_4min_y</th>\n",
       "      <th>down_packet_5min_y</th>\n",
       "      <th>down_packet_1min_x</th>\n",
       "      <th>down_packet_2min_x</th>\n",
       "      <th>down_packet_3min_x</th>\n",
       "      <th>down_packet_4min_x</th>\n",
       "      <th>down_packet_5min_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.049954</td>\n",
       "      <td>-0.403884</td>\n",
       "      <td>0.903263</td>\n",
       "      <td>0.815885</td>\n",
       "      <td>0.936888</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.777905</td>\n",
       "      <td>-0.403884</td>\n",
       "      <td>0.496817</td>\n",
       "      <td>1.335181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.071279</td>\n",
       "      <td>3.978398</td>\n",
       "      <td>2.497202</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.555438</td>\n",
       "      <td>-1.038336</td>\n",
       "      <td>0.953788</td>\n",
       "      <td>0.909712</td>\n",
       "      <td>2.042770</td>\n",
       "      <td>3.353170</td>\n",
       "      <td>-1.076309</td>\n",
       "      <td>-1.038336</td>\n",
       "      <td>-0.688625</td>\n",
       "      <td>0.728165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015163</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.814609</td>\n",
       "      <td>6.727171</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.429697</td>\n",
       "      <td>-0.797330</td>\n",
       "      <td>1.049213</td>\n",
       "      <td>1.100848</td>\n",
       "      <td>1.963251</td>\n",
       "      <td>2.620611</td>\n",
       "      <td>-1.040970</td>\n",
       "      <td>-0.797330</td>\n",
       "      <td>-0.604283</td>\n",
       "      <td>1.413431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10.262960</td>\n",
       "      <td>3.220981</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.290295</td>\n",
       "      <td>-0.852862</td>\n",
       "      <td>1.105632</td>\n",
       "      <td>1.222422</td>\n",
       "      <td>1.520949</td>\n",
       "      <td>1.379564</td>\n",
       "      <td>-1.076309</td>\n",
       "      <td>-0.852862</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>1.490957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.676306</td>\n",
       "      <td>7.336032</td>\n",
       "      <td>4.808627</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.490188</td>\n",
       "      <td>0.728495</td>\n",
       "      <td>1.230044</td>\n",
       "      <td>1.513008</td>\n",
       "      <td>0.312373</td>\n",
       "      <td>-0.971798</td>\n",
       "      <td>-0.726433</td>\n",
       "      <td>0.728495</td>\n",
       "      <td>1.389561</td>\n",
       "      <td>1.860390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.669475</td>\n",
       "      <td>4.087589</td>\n",
       "      <td>2.650473</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.412808</td>\n",
       "      <td>0.602065</td>\n",
       "      <td>0.860344</td>\n",
       "      <td>0.740192</td>\n",
       "      <td>-0.417224</td>\n",
       "      <td>-1.152910</td>\n",
       "      <td>-0.262419</td>\n",
       "      <td>0.602065</td>\n",
       "      <td>1.011425</td>\n",
       "      <td>1.422233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.237534</td>\n",
       "      <td>6.036658</td>\n",
       "      <td>2.524395</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.350947</td>\n",
       "      <td>0.470368</td>\n",
       "      <td>0.871598</td>\n",
       "      <td>0.759682</td>\n",
       "      <td>0.216610</td>\n",
       "      <td>-0.865017</td>\n",
       "      <td>-0.559836</td>\n",
       "      <td>0.470368</td>\n",
       "      <td>0.849711</td>\n",
       "      <td>1.411894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.006488</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.674702</td>\n",
       "      <td>7.140066</td>\n",
       "      <td>3.768688</td>\n",
       "      <td>2.538102</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.268645</td>\n",
       "      <td>-0.620746</td>\n",
       "      <td>1.062333</td>\n",
       "      <td>1.128552</td>\n",
       "      <td>1.905095</td>\n",
       "      <td>3.268406</td>\n",
       "      <td>-1.053646</td>\n",
       "      <td>-0.620746</td>\n",
       "      <td>-0.213910</td>\n",
       "      <td>1.118297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.660826</td>\n",
       "      <td>3.291511</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.061178</td>\n",
       "      <td>-0.247054</td>\n",
       "      <td>0.810107</td>\n",
       "      <td>0.656273</td>\n",
       "      <td>0.315406</td>\n",
       "      <td>-1.275492</td>\n",
       "      <td>-0.870641</td>\n",
       "      <td>-0.247054</td>\n",
       "      <td>0.627636</td>\n",
       "      <td>1.170910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.988808</td>\n",
       "      <td>5.358670</td>\n",
       "      <td>2.497202</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.236846</td>\n",
       "      <td>0.320672</td>\n",
       "      <td>0.891537</td>\n",
       "      <td>0.794837</td>\n",
       "      <td>-0.037296</td>\n",
       "      <td>-1.229062</td>\n",
       "      <td>-0.767698</td>\n",
       "      <td>0.320672</td>\n",
       "      <td>0.949033</td>\n",
       "      <td>1.227913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>6.949335</td>\n",
       "      <td>3.118009</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.144833</td>\n",
       "      <td>0.340536</td>\n",
       "      <td>0.743961</td>\n",
       "      <td>0.553478</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-1.121968</td>\n",
       "      <td>-0.596491</td>\n",
       "      <td>0.340536</td>\n",
       "      <td>0.604644</td>\n",
       "      <td>1.026987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.600851</td>\n",
       "      <td>7.063154</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.087574</td>\n",
       "      <td>0.163513</td>\n",
       "      <td>0.785633</td>\n",
       "      <td>0.617219</td>\n",
       "      <td>-0.085402</td>\n",
       "      <td>-1.405019</td>\n",
       "      <td>-0.775600</td>\n",
       "      <td>0.163513</td>\n",
       "      <td>0.701606</td>\n",
       "      <td>1.137678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>5.812112</td>\n",
       "      <td>3.151962</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    up_bytes_mean  up_bytes_median  up_bytes_std  up_bytes_var  up_bytes_skew  \\\n",
       "0       -0.049954        -0.403884      0.903263      0.815885       0.936888   \n",
       "1       -0.555438        -1.038336      0.953788      0.909712       2.042770   \n",
       "2       -0.429697        -0.797330      1.049213      1.100848       1.963251   \n",
       "3       -0.290295        -0.852862      1.105632      1.222422       1.520949   \n",
       "4        0.490188         0.728495      1.230044      1.513008       0.312373   \n",
       "5        0.412808         0.602065      0.860344      0.740192      -0.417224   \n",
       "6        0.350947         0.470368      0.871598      0.759682       0.216610   \n",
       "7       -0.268645        -0.620746      1.062333      1.128552       1.905095   \n",
       "8       -0.061178        -0.247054      0.810107      0.656273       0.315406   \n",
       "9        0.236846         0.320672      0.891537      0.794837      -0.037296   \n",
       "10       0.144833         0.340536      0.743961      0.553478      -0.272444   \n",
       "11       0.087574         0.163513      0.785633      0.617219      -0.085402   \n",
       "\n",
       "    up_bytes_kurt  up_bytes_perc25  up_bytes_perc50  up_bytes_perc75  \\\n",
       "0       -0.152483        -0.777905        -0.403884         0.496817   \n",
       "1        3.353170        -1.076309        -1.038336        -0.688625   \n",
       "2        2.620611        -1.040970        -0.797330        -0.604283   \n",
       "3        1.379564        -1.076309        -0.852862         0.033406   \n",
       "4       -0.971798        -0.726433         0.728495         1.389561   \n",
       "5       -1.152910        -0.262419         0.602065         1.011425   \n",
       "6       -0.865017        -0.559836         0.470368         0.849711   \n",
       "7        3.268406        -1.053646        -0.620746        -0.213910   \n",
       "8       -1.275492        -0.870641        -0.247054         0.627636   \n",
       "9       -1.229062        -0.767698         0.320672         0.949033   \n",
       "10      -1.121968        -0.596491         0.340536         0.604644   \n",
       "11      -1.405019        -0.775600         0.163513         0.701606   \n",
       "\n",
       "    up_bytes_perc90         ...          down_packet_1min_y  \\\n",
       "0          1.335181         ...                    0.002346   \n",
       "1          0.728165         ...                    0.015163   \n",
       "2          1.413431         ...                    0.000837   \n",
       "3          1.490957         ...                    0.013611   \n",
       "4          1.860390         ...                    0.001233   \n",
       "5          1.422233         ...                    0.008492   \n",
       "6          1.411894         ...                    0.003404   \n",
       "7          1.118297         ...                    0.002405   \n",
       "8          1.170910         ...                    0.006532   \n",
       "9          1.227913         ...                    0.016661   \n",
       "10         1.026987         ...                    0.001585   \n",
       "11         1.137678         ...                    0.001604   \n",
       "\n",
       "    down_packet_2min_y  down_packet_3min_y  down_packet_4min_y  \\\n",
       "0             0.001808            0.001815           -1.000000   \n",
       "1             0.000547           -1.000000           -1.000000   \n",
       "2             0.002284           -1.000000           -1.000000   \n",
       "3             0.002952            0.002953           -1.000000   \n",
       "4             0.004270            0.003137           -1.000000   \n",
       "5             0.002504            0.000737           -1.000000   \n",
       "6             0.006488            0.001413            0.003516   \n",
       "7             0.002918           -1.000000           -1.000000   \n",
       "8             0.004598            0.000632           -1.000000   \n",
       "9             0.001908            0.000587           -1.000000   \n",
       "10            0.002463           -1.000000           -1.000000   \n",
       "11            0.004513            0.010919           -1.000000   \n",
       "\n",
       "    down_packet_5min_y  down_packet_1min_x  down_packet_2min_x  \\\n",
       "0                 -1.0           11.071279            3.978398   \n",
       "1                 -1.0           11.814609            6.727171   \n",
       "2                 -1.0           10.262960            3.220981   \n",
       "3                 -1.0           12.676306            7.336032   \n",
       "4                 -1.0            9.669475            4.087589   \n",
       "5                 -1.0           13.237534            6.036658   \n",
       "6                 -1.0           13.674702            7.140066   \n",
       "7                 -1.0            7.660826            3.291511   \n",
       "8                 -1.0            9.988808            5.358670   \n",
       "9                 -1.0           12.337687            6.949335   \n",
       "10                -1.0           13.600851            7.063154   \n",
       "11                -1.0           13.454343            5.812112   \n",
       "\n",
       "    down_packet_3min_x  down_packet_4min_x  down_packet_5min_x  \n",
       "0             2.497202           -1.000000                -1.0  \n",
       "1            -1.000000           -1.000000                -1.0  \n",
       "2            -1.000000           -1.000000                -1.0  \n",
       "3             4.808627           -1.000000                -1.0  \n",
       "4             2.650473           -1.000000                -1.0  \n",
       "5             2.524395           -1.000000                -1.0  \n",
       "6             3.768688            2.538102                -1.0  \n",
       "7            -1.000000           -1.000000                -1.0  \n",
       "8             2.497202           -1.000000                -1.0  \n",
       "9             3.118009           -1.000000                -1.0  \n",
       "10           -1.000000           -1.000000                -1.0  \n",
       "11            3.151962           -1.000000                -1.0  \n",
       "\n",
       "[12 rows x 132 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
