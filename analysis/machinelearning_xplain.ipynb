{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_number(s):\n",
    "    return list(filter(None, re.split(r'(\\d+)', s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_stats = [\n",
    "    'up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var', 'up_bytes_skew', 'up_bytes_kurt',\n",
    "    'up_bytes_perc25', 'up_bytes_perc50', 'up_bytes_perc75', 'up_bytes_perc90',\n",
    "    'up_packet_mean', 'up_packet_median', 'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
    "    'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75', 'up_packet_perc90',\n",
    "    'down_bytes_mean', 'down_bytes_median', 'down_bytes_std', 'down_bytes_var', 'down_bytes_skew', 'down_bytes_kurt',\n",
    "    'down_bytes_perc25', 'down_bytes_perc50', 'down_bytes_perc75', 'down_bytes_perc90',\n",
    "    'down_packet_mean', 'down_packet_median', 'down_packet_std', 'down_packet_var', 'down_packet_skew', 'down_packet_kurt',\n",
    "    'down_packet_perc25', 'down_packet_perc50', 'down_packet_perc75', 'down_packet_perc90']\n",
    "\n",
    "silences = ['down_bytes_silences', 'down_bytes_silence_mean', 'down_bytes_longest_silence', 'down_bytes_shortest_silence',\n",
    "           'up_bytes_silences', 'up_bytes_silence_mean', 'up_bytes_longest_silence', 'up_bytes_shortest_silence']\n",
    "\n",
    "scalogram = ['up_bytes_1max_y', 'up_bytes_2max_y', 'up_bytes_3max_y', 'up_bytes_4max_y', 'up_bytes_5max_y',\n",
    "    'up_bytes_1max_x', 'up_bytes_2max_x', 'up_bytes_3max_x', 'up_bytes_4max_x', 'up_bytes_5max_x',\n",
    "    'up_bytes_1min_y', 'up_bytes_2min_y', 'up_bytes_3min_y', 'up_bytes_4min_y', 'up_bytes_5min_y',\n",
    "    'up_bytes_1min_x', 'up_bytes_2min_x', 'up_bytes_3min_x', 'up_bytes_4min_x', 'up_bytes_5min_x',\n",
    "    'up_packet_1max_y', 'up_packet_2max_y', 'up_packet_3max_y', 'up_packet_4max_y', 'up_packet_5max_y',\n",
    "    'up_packet_1max_x', 'up_packet_2max_x', 'up_packet_3max_x', 'up_packet_4max_x', 'up_packet_5max_x',\n",
    "    'up_packet_1min_y', 'up_packet_2min_y', 'up_packet_2min_y', 'up_packet_4min_y', 'up_packet_5min_y',\n",
    "    'up_packet_1min_x', 'up_packet_2min_x', 'up_packet_3min_x', 'up_packet_4min_x', 'up_packet_5min_x',\n",
    "    'down_bytes_1max_y', 'down_bytes_2max_y', 'down_bytes_3max_y', 'down_bytes_4max_y', 'down_bytes_5max_y',\n",
    "    'down_bytes_1max_x', 'down_bytes_2max_x', 'down_bytes_3max_x', 'down_bytes_4max_x', 'down_bytes_5max_x',\n",
    "    'down_bytes_1min_y', 'down_bytes_2min_y', 'down_bytes_3min_y', 'down_bytes_4min_y', 'down_bytes_5min_y',\n",
    "    'down_bytes_1min_x', 'down_bytes_2min_x', 'down_bytes_3min_x', 'down_bytes_4min_x', 'down_bytes_5min_x',\n",
    "    'down_packet_1max_y', 'down_packet_2max_y', 'down_packet_3max_y', 'down_packet_4max_y', 'down_packet_5max_y',\n",
    "    'down_packet_1max_x', 'down_packet_2max_x', 'down_packet_3max_x', 'down_packet_4max_x', 'down_packet_5max_x',\n",
    "    'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_2min_y', 'down_packet_4min_y', 'down_packet_5min_y',\n",
    "    'down_packet_1min_x', 'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'csv/all_30s_01s_128.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aa28e0573121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all_30s_01s_128.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'csv/all_30s_01s_128.csv' does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "base_folder = \"csv/\"\n",
    "file_name = 'all_30s_01s_128.csv'\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(str(base_folder), str(file_name)))\n",
    "\n",
    "dataset = dataset.drop(columns=['Unnamed: 0']).reset_index()\n",
    "dataset.drop(columns=['index', 'up_packet_silence_mean', 'down_packet_silence_mean',\n",
    "                      'down_packet_longest_silence', 'down_packet_shortest_silence'], inplace=True)\n",
    "features = dataset.columns\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video        6969\n",
       "browsing     1774\n",
       "acestream    1258\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset['label'] == 'netflix-ssh', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'youtube-ssh', 'label'] = 'youtube'\n",
    "dataset.loc[dataset['label'] == 'twitch-ssh', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'netflix-openvpn', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'youtube-openvpn', 'label'] = 'youtube'\n",
    "dataset.loc[dataset['label'] == 'twitch-openvpn', 'label'] = 'twitch'\n",
    "dataset.loc[dataset['label'] == 'acestream-openvpn', 'label'] = 'acestream'\n",
    "dataset.loc[dataset['label'] == 'twitch-openvpn', 'label'] = 'twitch'\n",
    "dataset.loc[dataset['label'] == 'netflix-openvpn', 'label'] = 'netflix'\n",
    "dataset.loc[dataset['label'] == 'youtube-openvpn', 'label'] = 'youtube'\n",
    "dataset.loc[dataset['label'] == 'reddit', 'label'] = 'browsing'\n",
    "dataset.loc[dataset['label'] == 'facebook', 'label'] = 'browsing'\n",
    "dataset.loc[dataset['label'] == 'netflix', 'label'] = 'video'\n",
    "dataset.loc[dataset['label'] == 'youtube', 'label'] = 'video'\n",
    "dataset.loc[dataset['label'] == 'twitch', 'label'] = 'video'\n",
    "\n",
    "\n",
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video       6969\n",
       "browsing    1774\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[dataset.label != 'acestream']\n",
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var',\n",
       "       'up_bytes_skew', 'up_bytes_kurt', 'up_bytes_perc25', 'up_bytes_perc50',\n",
       "       'up_bytes_perc75', 'up_bytes_perc90',\n",
       "       ...\n",
       "       'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_3min_y',\n",
       "       'down_packet_4min_y', 'down_packet_5min_y', 'down_packet_1min_x',\n",
       "       'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x',\n",
       "       'down_packet_5min_x'],\n",
       "      dtype='object', length=132)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test some stuff\n",
    "#dataset.drop(columns=silences, inplace=True)\n",
    "dataset.columns\n",
    "features = dataset.columns[:-1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "5        0\n",
       "6        0\n",
       "7        0\n",
       "8        0\n",
       "9        0\n",
       "10       0\n",
       "11       0\n",
       "12       0\n",
       "13       0\n",
       "14       0\n",
       "15       0\n",
       "16       0\n",
       "17       0\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       0\n",
       "29       0\n",
       "        ..\n",
       "9971     0\n",
       "9972     0\n",
       "9973     0\n",
       "9974     0\n",
       "9975     0\n",
       "9976     0\n",
       "9977     0\n",
       "9978     0\n",
       "9979     0\n",
       "9980     0\n",
       "9981     0\n",
       "9982     0\n",
       "9983     0\n",
       "9984     0\n",
       "9985     0\n",
       "9986     0\n",
       "9987     0\n",
       "9988     0\n",
       "9989     0\n",
       "9990     0\n",
       "9991     0\n",
       "9992     0\n",
       "9993     0\n",
       "9994     0\n",
       "9995     0\n",
       "9996     0\n",
       "9997     0\n",
       "9998     0\n",
       "9999     0\n",
       "10000    0\n",
       "Name: label, Length: 8743, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevlabel = dataset['label']\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "labels = dataset['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                          1.000000\n",
       "down_packet_4min_y             0.165449\n",
       "down_packet_4max_y             0.160212\n",
       "down_packet_5min_y             0.150855\n",
       "down_packet_5max_y             0.150648\n",
       "up_packet_4min_y               0.148293\n",
       "up_packet_4max_y               0.144364\n",
       "up_packet_5min_y               0.143634\n",
       "up_packet_5max_y               0.143189\n",
       "down_bytes_4min_y              0.140899\n",
       "down_bytes_4max_y              0.135313\n",
       "up_bytes_4min_y                0.113051\n",
       "down_bytes_5max_y              0.112165\n",
       "down_bytes_5min_y              0.110683\n",
       "down_packet_5min_x             0.110231\n",
       "up_bytes_5min_y                0.110131\n",
       "up_bytes_5max_y                0.108363\n",
       "up_packet_5min_x               0.106762\n",
       "up_bytes_4max_y                0.104665\n",
       "down_packet_5max_x             0.099368\n",
       "up_packet_5max_x               0.096579\n",
       "down_packet_4min_x             0.094900\n",
       "up_bytes_5min_x                0.091211\n",
       "down_bytes_skew                0.089639\n",
       "up_bytes_5max_x                0.081405\n",
       "down_packet_3min_y             0.080379\n",
       "up_packet_4min_x               0.076998\n",
       "up_bytes_4min_x                0.075800\n",
       "down_bytes_kurt                0.071962\n",
       "down_packet_3max_y             0.069789\n",
       "                                 ...   \n",
       "up_bytes_perc90               -0.084700\n",
       "down_packet_var               -0.085319\n",
       "down_packet_mean              -0.088782\n",
       "up_bytes_mean                 -0.090286\n",
       "down_packet_perc90            -0.092880\n",
       "up_packet_shortest_silence    -0.105083\n",
       "up_bytes_shortest_silence     -0.105083\n",
       "down_bytes_perc75             -0.112421\n",
       "down_bytes_1max_x             -0.118402\n",
       "up_packet_1min_x              -0.123551\n",
       "down_bytes_1min_x             -0.130256\n",
       "up_bytes_std                  -0.130738\n",
       "down_bytes_shortest_silence   -0.134007\n",
       "down_packet_std               -0.151200\n",
       "up_packet_1max_x              -0.155037\n",
       "down_bytes_var                -0.161548\n",
       "down_bytes_perc90             -0.162270\n",
       "up_packet_longest_silence     -0.164389\n",
       "up_bytes_longest_silence      -0.164389\n",
       "up_bytes_silence_mean         -0.172586\n",
       "up_bytes_silences             -0.172929\n",
       "up_packet_silences            -0.172929\n",
       "down_bytes_mean               -0.174168\n",
       "down_bytes_longest_silence    -0.188410\n",
       "down_bytes_silences           -0.197142\n",
       "down_packet_silences          -0.197142\n",
       "down_bytes_silence_mean       -0.202695\n",
       "up_packet_1max_y              -0.238405\n",
       "down_bytes_1max_y             -0.264569\n",
       "down_bytes_std                -0.303665\n",
       "Name: label, Length: 133, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()['label'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer for NaN\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(dataset)\n",
    "dataset = pd.DataFrame(imputer.transform(dataset), columns=dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.preprocessing import normalize\\n# Normalize data\\ndataset_no_label = dataset.loc[:, dataset.columns != 'label']\\n\\n#dataset = (dataset_no_label - dataset_no_label.mean()) / (dataset_no_label.max() - dataset_no_label.min())\\ndataset_normalized = normalize(dataset_no_label)\\ndataset = pd.DataFrame(dataset_normalized, columns=features)\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import normalize\n",
    "# Normalize data\n",
    "dataset_no_label = dataset.loc[:, dataset.columns != 'label']\n",
    "\n",
    "#dataset = (dataset_no_label - dataset_no_label.mean()) / (dataset_no_label.max() - dataset_no_label.min())\n",
    "dataset_normalized = normalize(dataset_no_label)\n",
    "dataset = pd.DataFrame(dataset_normalized, columns=features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components=20)\\nmain_components = pca.fit_transform(dataset)\\ndataset = pd.DataFrame(data = main_components)\\ndataset['label'] = labels\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "main_components = pca.fit_transform(dataset)\n",
    "dataset = pd.DataFrame(data = main_components)\n",
    "dataset['label'] = labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = train['label']\n",
    "x_train = train.drop(columns=['label'])\n",
    "\n",
    "y_test = test['label']\n",
    "x_test = test.drop(columns=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_youtube = (y_train == 0)\n",
    "y_train_netflix = (y_train == 1)\n",
    "y_train_twitch = (y_train == 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_bytes_mean</th>\n",
       "      <th>up_bytes_median</th>\n",
       "      <th>up_bytes_std</th>\n",
       "      <th>up_bytes_var</th>\n",
       "      <th>up_bytes_skew</th>\n",
       "      <th>up_bytes_kurt</th>\n",
       "      <th>up_bytes_perc25</th>\n",
       "      <th>up_bytes_perc50</th>\n",
       "      <th>up_bytes_perc75</th>\n",
       "      <th>up_bytes_perc90</th>\n",
       "      <th>...</th>\n",
       "      <th>down_packet_1min_y</th>\n",
       "      <th>down_packet_2min_y</th>\n",
       "      <th>down_packet_3min_y</th>\n",
       "      <th>down_packet_4min_y</th>\n",
       "      <th>down_packet_5min_y</th>\n",
       "      <th>down_packet_1min_x</th>\n",
       "      <th>down_packet_2min_x</th>\n",
       "      <th>down_packet_3min_x</th>\n",
       "      <th>down_packet_4min_x</th>\n",
       "      <th>down_packet_5min_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.095683</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>2.750500e-04</td>\n",
       "      <td>5.210940</td>\n",
       "      <td>25.196312</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.809358</td>\n",
       "      <td>17.166958</td>\n",
       "      <td>14.993341</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6972</th>\n",
       "      <td>-0.098676</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>4.205620e-07</td>\n",
       "      <td>7.550957</td>\n",
       "      <td>55.016949</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>24.944070</td>\n",
       "      <td>17.166958</td>\n",
       "      <td>9.411176</td>\n",
       "      <td>6.654706</td>\n",
       "      <td>5.048789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>-0.096516</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.016245</td>\n",
       "      <td>2.638908e-04</td>\n",
       "      <td>7.507542</td>\n",
       "      <td>54.569792</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.809358</td>\n",
       "      <td>17.353894</td>\n",
       "      <td>9.827844</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>-0.098061</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>2.204939e-05</td>\n",
       "      <td>7.440762</td>\n",
       "      <td>53.873398</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>27.054797</td>\n",
       "      <td>16.982036</td>\n",
       "      <td>11.943262</td>\n",
       "      <td>7.063154</td>\n",
       "      <td>3.589418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>-0.098136</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>2.245781e-05</td>\n",
       "      <td>7.550226</td>\n",
       "      <td>55.009690</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>17.927004</td>\n",
       "      <td>14.514017</td>\n",
       "      <td>8.175177</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>-0.097817</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>2.561027e-05</td>\n",
       "      <td>7.008363</td>\n",
       "      <td>49.165454</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>28.252615</td>\n",
       "      <td>17.638121</td>\n",
       "      <td>11.943262</td>\n",
       "      <td>7.140066</td>\n",
       "      <td>4.913922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>-0.098441</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>1.243981e-06</td>\n",
       "      <td>3.400699</td>\n",
       "      <td>9.860083</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.675373</td>\n",
       "      <td>17.353894</td>\n",
       "      <td>3.169077</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7947</th>\n",
       "      <td>-0.098703</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>1.928032e-07</td>\n",
       "      <td>7.550957</td>\n",
       "      <td>55.016949</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>24.675373</td>\n",
       "      <td>17.733895</td>\n",
       "      <td>14.435633</td>\n",
       "      <td>10.431050</td>\n",
       "      <td>7.537377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>-0.098430</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>1.447403e-06</td>\n",
       "      <td>4.159958</td>\n",
       "      <td>18.458041</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.628684</td>\n",
       "      <td>15.321652</td>\n",
       "      <td>9.259521</td>\n",
       "      <td>6.338153</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>-0.098261</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>1.522747e-06</td>\n",
       "      <td>3.379797</td>\n",
       "      <td>12.314956</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098694</td>\n",
       "      <td>-0.097296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>22.383675</td>\n",
       "      <td>14.203012</td>\n",
       "      <td>3.789152</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229</th>\n",
       "      <td>-0.098319</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>7.116874e-07</td>\n",
       "      <td>3.501761</td>\n",
       "      <td>16.064495</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098101</td>\n",
       "      <td>-0.097453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>23.248448</td>\n",
       "      <td>17.927004</td>\n",
       "      <td>8.399553</td>\n",
       "      <td>6.036658</td>\n",
       "      <td>3.978398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>-0.098543</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>7.649286e-07</td>\n",
       "      <td>4.175876</td>\n",
       "      <td>16.360306</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.215694</td>\n",
       "      <td>15.488494</td>\n",
       "      <td>9.411176</td>\n",
       "      <td>6.690841</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.390637</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>2.179158</td>\n",
       "      <td>4.748730e+00</td>\n",
       "      <td>4.428907</td>\n",
       "      <td>18.547860</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>27.497909</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>7.744247</td>\n",
       "      <td>5.505744</td>\n",
       "      <td>4.291739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7943</th>\n",
       "      <td>-0.098523</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>9.420894e-07</td>\n",
       "      <td>4.383612</td>\n",
       "      <td>18.469829</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.016225</td>\n",
       "      <td>11.878761</td>\n",
       "      <td>3.589418</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>-0.097947</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>3.174221e-06</td>\n",
       "      <td>2.026324</td>\n",
       "      <td>2.714708</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.095287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>28.871267</td>\n",
       "      <td>13.309413</td>\n",
       "      <td>7.660826</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>-0.098644</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>4.057377e-07</td>\n",
       "      <td>5.967456</td>\n",
       "      <td>35.730811</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>18.024346</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>-0.098576</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>8.692301e-07</td>\n",
       "      <td>6.010193</td>\n",
       "      <td>37.360815</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>18.024346</td>\n",
       "      <td>5.780723</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6203</th>\n",
       "      <td>-0.098035</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>1.921365e-06</td>\n",
       "      <td>2.514278</td>\n",
       "      <td>6.540374</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.097969</td>\n",
       "      <td>-0.095538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.146631</td>\n",
       "      <td>15.913591</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>4.887384</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6808</th>\n",
       "      <td>-0.098646</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>3.747109e-07</td>\n",
       "      <td>5.199502</td>\n",
       "      <td>25.034939</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>18.220618</td>\n",
       "      <td>12.138872</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6204</th>\n",
       "      <td>-0.098216</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>1.091658e-06</td>\n",
       "      <td>1.989992</td>\n",
       "      <td>2.617154</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098166</td>\n",
       "      <td>-0.096551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>27.054797</td>\n",
       "      <td>14.993341</td>\n",
       "      <td>9.061108</td>\n",
       "      <td>2.921836</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>-0.098756</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5.607555e-10</td>\n",
       "      <td>5.199469</td>\n",
       "      <td>25.034483</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>18.519041</td>\n",
       "      <td>5.812112</td>\n",
       "      <td>3.493535</td>\n",
       "      <td>2.497202</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>-0.098617</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>4.006178e-07</td>\n",
       "      <td>4.460745</td>\n",
       "      <td>18.724994</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.409570</td>\n",
       "      <td>18.319554</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>-0.098403</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>4.826664e-06</td>\n",
       "      <td>7.349601</td>\n",
       "      <td>52.913714</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>22.262790</td>\n",
       "      <td>9.360350</td>\n",
       "      <td>5.535640</td>\n",
       "      <td>2.607763</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>-0.097204</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>9.778300e-06</td>\n",
       "      <td>2.159034</td>\n",
       "      <td>3.599427</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.097774</td>\n",
       "      <td>-0.093114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006418</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.146631</td>\n",
       "      <td>8.445161</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0.316421</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.707595</td>\n",
       "      <td>5.006907e-01</td>\n",
       "      <td>1.583234</td>\n",
       "      <td>1.095705</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.414580</td>\n",
       "      <td>1.606421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>15.657153</td>\n",
       "      <td>9.565315</td>\n",
       "      <td>4.481751</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8068</th>\n",
       "      <td>-0.097277</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>5.825424e-06</td>\n",
       "      <td>1.676156</td>\n",
       "      <td>1.753348</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.096528</td>\n",
       "      <td>-0.093885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>27.349404</td>\n",
       "      <td>12.676306</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>-0.098433</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>4.735200e-06</td>\n",
       "      <td>7.499549</td>\n",
       "      <td>54.497916</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>26.048439</td>\n",
       "      <td>10.601893</td>\n",
       "      <td>6.372569</td>\n",
       "      <td>4.506086</td>\n",
       "      <td>2.621922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>0.535097</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>1.046461</td>\n",
       "      <td>1.095080e+00</td>\n",
       "      <td>1.730875</td>\n",
       "      <td>2.235337</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.883282</td>\n",
       "      <td>2.070659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>27.497909</td>\n",
       "      <td>11.071279</td>\n",
       "      <td>6.800425</td>\n",
       "      <td>2.767820</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7724</th>\n",
       "      <td>-0.097264</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>1.046426e-05</td>\n",
       "      <td>1.883696</td>\n",
       "      <td>1.829649</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.091297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>20.749433</td>\n",
       "      <td>11.624224</td>\n",
       "      <td>4.994404</td>\n",
       "      <td>2.417369</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3903</th>\n",
       "      <td>-0.098715</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>9.962479e-08</td>\n",
       "      <td>7.438148</td>\n",
       "      <td>53.827408</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>24.277745</td>\n",
       "      <td>18.122216</td>\n",
       "      <td>6.512110</td>\n",
       "      <td>3.872124</td>\n",
       "      <td>2.738005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658</th>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>-0.098306</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>8.951100e-07</td>\n",
       "      <td>2.778403</td>\n",
       "      <td>8.266963</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098082</td>\n",
       "      <td>-0.097023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.146631</td>\n",
       "      <td>18.619598</td>\n",
       "      <td>11.943262</td>\n",
       "      <td>2.636159</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>-0.098495</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>7.768117e-07</td>\n",
       "      <td>3.201202</td>\n",
       "      <td>8.686314</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.490274</td>\n",
       "      <td>15.321652</td>\n",
       "      <td>9.110309</td>\n",
       "      <td>6.583022</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.653240</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>3.135262</td>\n",
       "      <td>9.829867e+00</td>\n",
       "      <td>5.124875</td>\n",
       "      <td>27.359054</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>27.349404</td>\n",
       "      <td>12.539757</td>\n",
       "      <td>7.537377</td>\n",
       "      <td>5.215525</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>-0.098658</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>2.760417e-07</td>\n",
       "      <td>5.167532</td>\n",
       "      <td>24.811283</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.675373</td>\n",
       "      <td>17.927004</td>\n",
       "      <td>7.619453</td>\n",
       "      <td>4.579889</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>-0.093850</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.019766</td>\n",
       "      <td>3.906840e-04</td>\n",
       "      <td>4.392169</td>\n",
       "      <td>19.786261</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.352612</td>\n",
       "      <td>15.488494</td>\n",
       "      <td>9.411176</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>-0.098754</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.566906e-09</td>\n",
       "      <td>7.550957</td>\n",
       "      <td>55.016949</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.675373</td>\n",
       "      <td>17.542865</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>-0.098564</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>8.031105e-07</td>\n",
       "      <td>5.603006</td>\n",
       "      <td>31.739193</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>17.830188</td>\n",
       "      <td>13.974139</td>\n",
       "      <td>10.152408</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8666</th>\n",
       "      <td>-0.097298</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>6.382512e-06</td>\n",
       "      <td>1.578070</td>\n",
       "      <td>1.069757</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.096851</td>\n",
       "      <td>-0.092309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>20.525920</td>\n",
       "      <td>11.943262</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>-0.097763</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>1.285935e-05</td>\n",
       "      <td>6.612344</td>\n",
       "      <td>45.441615</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098101</td>\n",
       "      <td>-0.095631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>22.022976</td>\n",
       "      <td>14.912368</td>\n",
       "      <td>9.061108</td>\n",
       "      <td>7.257009</td>\n",
       "      <td>2.565740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>-0.097255</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.003192</td>\n",
       "      <td>1.018911e-05</td>\n",
       "      <td>2.411743</td>\n",
       "      <td>5.732093</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098096</td>\n",
       "      <td>-0.091907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>22.998016</td>\n",
       "      <td>15.156608</td>\n",
       "      <td>9.934862</td>\n",
       "      <td>5.387767</td>\n",
       "      <td>2.890362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>-0.095482</td>\n",
       "      <td>-0.098417</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>5.111163e-05</td>\n",
       "      <td>3.285242</td>\n",
       "      <td>10.700534</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098417</td>\n",
       "      <td>-0.095800</td>\n",
       "      <td>-0.089827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>26.618825</td>\n",
       "      <td>14.280133</td>\n",
       "      <td>7.913824</td>\n",
       "      <td>5.300947</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>0.730474</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>1.013414</td>\n",
       "      <td>1.027009e+00</td>\n",
       "      <td>0.830880</td>\n",
       "      <td>-0.649556</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>1.508217</td>\n",
       "      <td>2.329561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>23.629218</td>\n",
       "      <td>6.949335</td>\n",
       "      <td>2.524395</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>-0.098198</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>1.133523e-06</td>\n",
       "      <td>2.028986</td>\n",
       "      <td>2.734725</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098199</td>\n",
       "      <td>-0.096420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.767845</td>\n",
       "      <td>15.742170</td>\n",
       "      <td>8.219568</td>\n",
       "      <td>4.731139</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>-0.098660</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>1.060843e-07</td>\n",
       "      <td>4.138048</td>\n",
       "      <td>17.829683</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>21.904039</td>\n",
       "      <td>20.195158</td>\n",
       "      <td>10.262960</td>\n",
       "      <td>6.690841</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>-0.098351</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>1.946410e-06</td>\n",
       "      <td>4.636848</td>\n",
       "      <td>23.055167</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.097731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>19.869725</td>\n",
       "      <td>12.337687</td>\n",
       "      <td>8.963502</td>\n",
       "      <td>4.043557</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6949</th>\n",
       "      <td>-0.098472</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>2.837054e-06</td>\n",
       "      <td>6.313410</td>\n",
       "      <td>40.174655</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.215694</td>\n",
       "      <td>15.321652</td>\n",
       "      <td>9.209514</td>\n",
       "      <td>6.800425</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>-0.096328</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.018682</td>\n",
       "      <td>3.490233e-04</td>\n",
       "      <td>7.550957</td>\n",
       "      <td>55.016949</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>18.122216</td>\n",
       "      <td>11.750803</td>\n",
       "      <td>7.956795</td>\n",
       "      <td>6.102393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>-0.098604</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>5.390880e-07</td>\n",
       "      <td>5.153330</td>\n",
       "      <td>26.554464</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.944070</td>\n",
       "      <td>17.260173</td>\n",
       "      <td>14.831833</td>\n",
       "      <td>8.771433</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>-0.096456</td>\n",
       "      <td>-0.098289</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>7.335076e-06</td>\n",
       "      <td>0.737477</td>\n",
       "      <td>-0.908407</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098289</td>\n",
       "      <td>-0.094710</td>\n",
       "      <td>-0.092191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>26.048439</td>\n",
       "      <td>18.519041</td>\n",
       "      <td>10.717340</td>\n",
       "      <td>3.935542</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>-0.098702</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>5.037009e-08</td>\n",
       "      <td>4.599257</td>\n",
       "      <td>22.488102</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.079514</td>\n",
       "      <td>15.238907</td>\n",
       "      <td>9.827844</td>\n",
       "      <td>4.433474</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>-0.097517</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>1.172581e-05</td>\n",
       "      <td>3.866568</td>\n",
       "      <td>15.345484</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098104</td>\n",
       "      <td>-0.094631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>22.262790</td>\n",
       "      <td>18.220618</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>6.547470</td>\n",
       "      <td>2.969690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>-0.081177</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.106231</td>\n",
       "      <td>1.128512e-02</td>\n",
       "      <td>6.641254</td>\n",
       "      <td>44.370614</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.944070</td>\n",
       "      <td>17.927004</td>\n",
       "      <td>9.513657</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>-0.098311</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>9.414314e-07</td>\n",
       "      <td>2.415571</td>\n",
       "      <td>4.556483</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098496</td>\n",
       "      <td>-0.097401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>24.542112</td>\n",
       "      <td>12.676306</td>\n",
       "      <td>2.890362</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>-0.098732</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>2.499712e-08</td>\n",
       "      <td>5.618832</td>\n",
       "      <td>30.876006</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>24.675373</td>\n",
       "      <td>17.542865</td>\n",
       "      <td>10.262960</td>\n",
       "      <td>7.217817</td>\n",
       "      <td>5.476010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>-0.098156</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>4.985306e-06</td>\n",
       "      <td>5.095229</td>\n",
       "      <td>25.154675</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098417</td>\n",
       "      <td>-0.097731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>21.203786</td>\n",
       "      <td>8.175177</td>\n",
       "      <td>5.417022</td>\n",
       "      <td>3.169077</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-0.091000</td>\n",
       "      <td>-0.097434</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>1.300796e-04</td>\n",
       "      <td>1.687381</td>\n",
       "      <td>2.229178</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.097434</td>\n",
       "      <td>-0.084458</td>\n",
       "      <td>-0.076884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>27.647220</td>\n",
       "      <td>19.443958</td>\n",
       "      <td>8.399553</td>\n",
       "      <td>4.433474</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.740396</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>0.937652</td>\n",
       "      <td>8.791920e-01</td>\n",
       "      <td>0.588471</td>\n",
       "      <td>-1.221603</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>1.597494</td>\n",
       "      <td>2.070659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>28.100035</td>\n",
       "      <td>11.878761</td>\n",
       "      <td>7.296414</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>-0.097757</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>5.973974e-06</td>\n",
       "      <td>2.214621</td>\n",
       "      <td>3.290036</td>\n",
       "      <td>-0.09876</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.098760</td>\n",
       "      <td>-0.093536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>28.560266</td>\n",
       "      <td>13.454343</td>\n",
       "      <td>7.660826</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6994 rows  132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      up_bytes_mean  up_bytes_median  up_bytes_std  up_bytes_var  \\\n",
       "2002      -0.095683        -0.098760      0.016585  2.750500e-04   \n",
       "6972      -0.098676        -0.098760      0.000649  4.205620e-07   \n",
       "1916      -0.096516        -0.098760      0.016245  2.638908e-04   \n",
       "1550      -0.098061        -0.098760      0.004696  2.204939e-05   \n",
       "2358      -0.098136        -0.098760      0.004739  2.245781e-05   \n",
       "6726      -0.097817        -0.098760      0.005061  2.561027e-05   \n",
       "3884      -0.098441        -0.098760      0.001115  1.243981e-06   \n",
       "7947      -0.098703        -0.098760      0.000439  1.928032e-07   \n",
       "1739      -0.098430        -0.098760      0.001203  1.447403e-06   \n",
       "4071      -0.098261        -0.098760      0.001234  1.522747e-06   \n",
       "8229      -0.098319        -0.098760      0.000844  7.116874e-07   \n",
       "1650      -0.098543        -0.098760      0.000875  7.649286e-07   \n",
       "73         0.390637        -0.098760      2.179158  4.748730e+00   \n",
       "7943      -0.098523        -0.098760      0.000971  9.420894e-07   \n",
       "6681      -0.097947        -0.098760      0.001782  3.174221e-06   \n",
       "3404      -0.098644        -0.098760      0.000637  4.057377e-07   \n",
       "3082      -0.098576        -0.098760      0.000932  8.692301e-07   \n",
       "6203      -0.098035        -0.098760      0.001386  1.921365e-06   \n",
       "6808      -0.098646        -0.098760      0.000612  3.747109e-07   \n",
       "6204      -0.098216        -0.098760      0.001045  1.091658e-06   \n",
       "7935      -0.098756        -0.098760      0.000024  5.607555e-10   \n",
       "2510      -0.098617        -0.098760      0.000633  4.006178e-07   \n",
       "5441      -0.098403        -0.098760      0.002197  4.826664e-06   \n",
       "8550      -0.097204        -0.098760      0.003127  9.778300e-06   \n",
       "1126       0.316421        -0.098760      0.707595  5.006907e-01   \n",
       "8068      -0.097277        -0.098760      0.002414  5.825424e-06   \n",
       "5883      -0.098433        -0.098760      0.002176  4.735200e-06   \n",
       "1203       0.535097        -0.098760      1.046461  1.095080e+00   \n",
       "7724      -0.097264        -0.098760      0.003235  1.046426e-05   \n",
       "3903      -0.098715        -0.098760      0.000316  9.962479e-08   \n",
       "...             ...              ...           ...           ...   \n",
       "4658      -0.098760        -0.098760      0.000000  0.000000e+00   \n",
       "3005      -0.098306        -0.098760      0.000946  8.951100e-07   \n",
       "2734      -0.098495        -0.098760      0.000881  7.768117e-07   \n",
       "189        0.653240        -0.098760      3.135262  9.829867e+00   \n",
       "2747      -0.098658        -0.098760      0.000525  2.760417e-07   \n",
       "2047      -0.093850        -0.098760      0.019766  3.906840e-04   \n",
       "7849      -0.098754        -0.098760      0.000051  2.566906e-09   \n",
       "2558      -0.098564        -0.098760      0.000896  8.031105e-07   \n",
       "8666      -0.097298        -0.098760      0.002526  6.382512e-06   \n",
       "6396      -0.097763        -0.098760      0.003586  1.285935e-05   \n",
       "3385      -0.097255        -0.098760      0.003192  1.018911e-05   \n",
       "4555      -0.095482        -0.098417      0.007149  5.111163e-05   \n",
       "1184       0.730474        -0.009484      1.013414  1.027009e+00   \n",
       "6420      -0.098198        -0.098760      0.001065  1.133523e-06   \n",
       "5051      -0.098660        -0.098760      0.000326  1.060843e-07   \n",
       "5311      -0.098351        -0.098760      0.001395  1.946410e-06   \n",
       "2433      -0.098760        -0.098760      0.000000  0.000000e+00   \n",
       "6949      -0.098472        -0.098760      0.001684  2.837054e-06   \n",
       "769       -0.096328        -0.098760      0.018682  3.490233e-04   \n",
       "1685      -0.098604        -0.098760      0.000734  5.390880e-07   \n",
       "8322      -0.096456        -0.098289      0.002708  7.335076e-06   \n",
       "5578      -0.098702        -0.098760      0.000224  5.037009e-08   \n",
       "4426      -0.097517        -0.098760      0.003424  1.172581e-05   \n",
       "466       -0.081177        -0.098760      0.106231  1.128512e-02   \n",
       "6265      -0.098311        -0.098760      0.000970  9.414314e-07   \n",
       "5734      -0.098732        -0.098760      0.000158  2.499712e-08   \n",
       "5191      -0.098156        -0.098760      0.002233  4.985306e-06   \n",
       "5390      -0.091000        -0.097434      0.011405  1.300796e-04   \n",
       "860        0.740396        -0.009484      0.937652  8.791920e-01   \n",
       "7270      -0.097757        -0.098760      0.002444  5.973974e-06   \n",
       "\n",
       "      up_bytes_skew  up_bytes_kurt  up_bytes_perc25  up_bytes_perc50  \\\n",
       "2002       5.210940      25.196312         -0.09876        -0.098760   \n",
       "6972       7.550957      55.016949         -0.09876        -0.098760   \n",
       "1916       7.507542      54.569792         -0.09876        -0.098760   \n",
       "1550       7.440762      53.873398         -0.09876        -0.098760   \n",
       "2358       7.550226      55.009690         -0.09876        -0.098760   \n",
       "6726       7.008363      49.165454         -0.09876        -0.098760   \n",
       "3884       3.400699       9.860083         -0.09876        -0.098760   \n",
       "7947       7.550957      55.016949         -0.09876        -0.098760   \n",
       "1739       4.159958      18.458041         -0.09876        -0.098760   \n",
       "4071       3.379797      12.314956         -0.09876        -0.098760   \n",
       "8229       3.501761      16.064495         -0.09876        -0.098760   \n",
       "1650       4.175876      16.360306         -0.09876        -0.098760   \n",
       "73         4.428907      18.547860         -0.09876        -0.098760   \n",
       "7943       4.383612      18.469829         -0.09876        -0.098760   \n",
       "6681       2.026324       2.714708         -0.09876        -0.098760   \n",
       "3404       5.967456      35.730811         -0.09876        -0.098760   \n",
       "3082       6.010193      37.360815         -0.09876        -0.098760   \n",
       "6203       2.514278       6.540374         -0.09876        -0.098760   \n",
       "6808       5.199502      25.034939         -0.09876        -0.098760   \n",
       "6204       1.989992       2.617154         -0.09876        -0.098760   \n",
       "7935       5.199469      25.034483         -0.09876        -0.098760   \n",
       "2510       4.460745      18.724994         -0.09876        -0.098760   \n",
       "5441       7.349601      52.913714         -0.09876        -0.098760   \n",
       "8550       2.159034       3.599427         -0.09876        -0.098760   \n",
       "1126       1.583234       1.095705         -0.09876        -0.098760   \n",
       "8068       1.676156       1.753348         -0.09876        -0.098760   \n",
       "5883       7.499549      54.497916         -0.09876        -0.098760   \n",
       "1203       1.730875       2.235337         -0.09876        -0.098760   \n",
       "7724       1.883696       1.829649         -0.09876        -0.098760   \n",
       "3903       7.438148      53.827408         -0.09876        -0.098760   \n",
       "...             ...            ...              ...              ...   \n",
       "4658       0.000000      -3.000000         -0.09876        -0.098760   \n",
       "3005       2.778403       8.266963         -0.09876        -0.098760   \n",
       "2734       3.201202       8.686314         -0.09876        -0.098760   \n",
       "189        5.124875      27.359054         -0.09876        -0.098760   \n",
       "2747       5.167532      24.811283         -0.09876        -0.098760   \n",
       "2047       4.392169      19.786261         -0.09876        -0.098760   \n",
       "7849       7.550957      55.016949         -0.09876        -0.098760   \n",
       "2558       5.603006      31.739193         -0.09876        -0.098760   \n",
       "8666       1.578070       1.069757         -0.09876        -0.098760   \n",
       "6396       6.612344      45.441615         -0.09876        -0.098760   \n",
       "3385       2.411743       5.732093         -0.09876        -0.098760   \n",
       "4555       3.285242      10.700534         -0.09876        -0.098417   \n",
       "1184       0.830880      -0.649556         -0.09876        -0.009484   \n",
       "6420       2.028986       2.734725         -0.09876        -0.098760   \n",
       "5051       4.138048      17.829683         -0.09876        -0.098760   \n",
       "5311       4.636848      23.055167         -0.09876        -0.098760   \n",
       "2433       0.000000      -3.000000         -0.09876        -0.098760   \n",
       "6949       6.313410      40.174655         -0.09876        -0.098760   \n",
       "769        7.550957      55.016949         -0.09876        -0.098760   \n",
       "1685       5.153330      26.554464         -0.09876        -0.098760   \n",
       "8322       0.737477      -0.908407         -0.09876        -0.098289   \n",
       "5578       4.599257      22.488102         -0.09876        -0.098760   \n",
       "4426       3.866568      15.345484         -0.09876        -0.098760   \n",
       "466        6.641254      44.370614         -0.09876        -0.098760   \n",
       "6265       2.415571       4.556483         -0.09876        -0.098760   \n",
       "5734       5.618832      30.876006         -0.09876        -0.098760   \n",
       "5191       5.095229      25.154675         -0.09876        -0.098760   \n",
       "5390       1.687381       2.229178         -0.09876        -0.097434   \n",
       "860        0.588471      -1.221603         -0.09876        -0.009484   \n",
       "7270       2.214621       3.290036         -0.09876        -0.098760   \n",
       "\n",
       "      up_bytes_perc75  up_bytes_perc90         ...          \\\n",
       "2002        -0.098760        -0.098760         ...           \n",
       "6972        -0.098760        -0.098760         ...           \n",
       "1916        -0.098760        -0.098760         ...           \n",
       "1550        -0.098760        -0.098760         ...           \n",
       "2358        -0.098760        -0.098760         ...           \n",
       "6726        -0.098760        -0.098243         ...           \n",
       "3884        -0.098760        -0.098747         ...           \n",
       "7947        -0.098760        -0.098760         ...           \n",
       "1739        -0.098760        -0.098760         ...           \n",
       "4071        -0.098694        -0.097296         ...           \n",
       "8229        -0.098101        -0.097453         ...           \n",
       "1650        -0.098760        -0.098760         ...           \n",
       "73          -0.098760        -0.098760         ...           \n",
       "7943        -0.098760        -0.098760         ...           \n",
       "6681        -0.098760        -0.095287         ...           \n",
       "3404        -0.098760        -0.098760         ...           \n",
       "3082        -0.098760        -0.098760         ...           \n",
       "6203        -0.097969        -0.095538         ...           \n",
       "6808        -0.098760        -0.098760         ...           \n",
       "6204        -0.098166        -0.096551         ...           \n",
       "7935        -0.098760        -0.098760         ...           \n",
       "2510        -0.098760        -0.098760         ...           \n",
       "5441        -0.098760        -0.098726         ...           \n",
       "8550        -0.097774        -0.093114         ...           \n",
       "1126         0.414580         1.606421         ...           \n",
       "8068        -0.096528        -0.093885         ...           \n",
       "5883        -0.098760        -0.098496         ...           \n",
       "1203         0.883282         2.070659         ...           \n",
       "7724        -0.098760        -0.091297         ...           \n",
       "3903        -0.098760        -0.098760         ...           \n",
       "...               ...              ...         ...           \n",
       "4658        -0.098760        -0.098760         ...           \n",
       "3005        -0.098082        -0.097023         ...           \n",
       "2734        -0.098760        -0.098734         ...           \n",
       "189         -0.098760        -0.098760         ...           \n",
       "2747        -0.098760        -0.098760         ...           \n",
       "2047        -0.098760        -0.098760         ...           \n",
       "7849        -0.098760        -0.098760         ...           \n",
       "2558        -0.098760        -0.098616         ...           \n",
       "8666        -0.096851        -0.092309         ...           \n",
       "6396        -0.098101        -0.095631         ...           \n",
       "3385        -0.098096        -0.091907         ...           \n",
       "4555        -0.095800        -0.089827         ...           \n",
       "1184         1.508217         2.329561         ...           \n",
       "6420        -0.098199        -0.096420         ...           \n",
       "5051        -0.098760        -0.098417         ...           \n",
       "5311        -0.098760        -0.097731         ...           \n",
       "2433        -0.098760        -0.098760         ...           \n",
       "6949        -0.098760        -0.098760         ...           \n",
       "769         -0.098760        -0.098760         ...           \n",
       "1685        -0.098760        -0.098760         ...           \n",
       "8322        -0.094710        -0.092191         ...           \n",
       "5578        -0.098760        -0.098760         ...           \n",
       "4426        -0.098104        -0.094631         ...           \n",
       "466         -0.098760        -0.098760         ...           \n",
       "6265        -0.098496        -0.097401         ...           \n",
       "5734        -0.098760        -0.098760         ...           \n",
       "5191        -0.098417        -0.097731         ...           \n",
       "5390        -0.084458        -0.076884         ...           \n",
       "860          1.597494         2.070659         ...           \n",
       "7270        -0.098760        -0.093536         ...           \n",
       "\n",
       "      down_packet_1min_y  down_packet_2min_y  down_packet_3min_y  \\\n",
       "2002            0.002965            0.004055            0.004195   \n",
       "6972            0.002176            0.003042            0.003324   \n",
       "1916            0.002341            0.003236            0.003478   \n",
       "1550            0.000928            0.005393            0.001574   \n",
       "2358            0.002255            0.003140            0.003704   \n",
       "6726            0.000267            0.005086            0.001918   \n",
       "3884            0.004488            0.005749            0.000093   \n",
       "7947            0.002196            0.003044            0.003364   \n",
       "1739            0.002696            0.002079            0.003317   \n",
       "4071            0.004778            0.000310            0.000779   \n",
       "8229            0.004439            0.003905            0.002276   \n",
       "1650            0.003272            0.001275            0.002945   \n",
       "73              0.000644            0.001625            0.002522   \n",
       "7943            0.004400            0.000967            0.000905   \n",
       "6681            0.000312            0.001118            0.002576   \n",
       "3404            0.003005            0.004064           -1.000000   \n",
       "3082            0.002365            0.003338            0.003487   \n",
       "6203            0.003978            0.001656            0.002519   \n",
       "6808            0.002323            0.003209            0.003466   \n",
       "6204            0.001857            0.003744            0.002573   \n",
       "7935            0.000400            0.001543            0.003101   \n",
       "2510            0.002875            0.003920            0.004365   \n",
       "5441            0.000345            0.001216            0.001924   \n",
       "8550            0.006418            0.000378           -1.000000   \n",
       "1126            0.006813            0.004491            0.002512   \n",
       "8068            0.001812            0.002002           -1.000000   \n",
       "5883            0.002726            0.001204            0.002295   \n",
       "1203            0.001450            0.002290            0.001594   \n",
       "7724            0.000802            0.004027            0.000682   \n",
       "3903            0.001398            0.001772            0.002769   \n",
       "...                  ...                 ...                 ...   \n",
       "4658           -1.000000           -1.000000           -1.000000   \n",
       "3005            0.003037            0.003738            0.002699   \n",
       "2734            0.002746            0.001598            0.003119   \n",
       "189             0.000980            0.001610            0.002747   \n",
       "2747            0.002039            0.002938            0.003204   \n",
       "2047            0.002786            0.001345            0.003416   \n",
       "7849            0.002695            0.003807           -1.000000   \n",
       "2558            0.002873            0.003767            0.004180   \n",
       "8666            0.000725            0.003255           -1.000000   \n",
       "6396            0.004959            0.003027            0.000853   \n",
       "3385            0.003990            0.003021            0.002409   \n",
       "4555            0.002181            0.001862            0.002920   \n",
       "1184            0.002200            0.000716            0.000113   \n",
       "6420            0.003219            0.003028            0.001978   \n",
       "5051            0.002220            0.002205            0.003301   \n",
       "5311            0.001403            0.001551            0.002835   \n",
       "2433           -1.000000           -1.000000           -1.000000   \n",
       "6949            0.002604            0.001796            0.002534   \n",
       "769             0.002185            0.003031            0.003334   \n",
       "1685            0.002711            0.003902            0.004032   \n",
       "8322            0.002961            0.009163            0.002278   \n",
       "5578            0.003780            0.001658            0.002177   \n",
       "4426            0.001967            0.001668            0.001324   \n",
       "466             0.002504            0.004069            0.003974   \n",
       "6265            0.003810            0.002094            0.000892   \n",
       "5734            0.002196            0.003041            0.003303   \n",
       "5191            0.001063            0.001964            0.002251   \n",
       "5390            0.000477            0.006146            0.001870   \n",
       "860             0.001299            0.001437            0.000370   \n",
       "7270            0.000444            0.000960            0.002582   \n",
       "\n",
       "      down_packet_4min_y  down_packet_5min_y  down_packet_1min_x  \\\n",
       "2002           -1.000000           -1.000000           24.809358   \n",
       "6972            0.003368            0.003374           24.944070   \n",
       "1916           -1.000000           -1.000000           24.809358   \n",
       "1550            0.002838            0.002571           27.054797   \n",
       "2358            0.003421           -1.000000           24.542112   \n",
       "6726            0.003033            0.003236           28.252615   \n",
       "3884           -1.000000           -1.000000           24.675373   \n",
       "7947            0.003310            0.003365           24.675373   \n",
       "1739            0.003492           -1.000000           25.628684   \n",
       "4071           -1.000000           -1.000000           22.383675   \n",
       "8229            0.001420            0.000646           23.248448   \n",
       "1650            0.002973           -1.000000           25.215694   \n",
       "73              0.003341            0.003353           27.497909   \n",
       "7943           -1.000000           -1.000000           24.016225   \n",
       "6681           -1.000000           -1.000000           28.871267   \n",
       "3404           -1.000000           -1.000000           24.542112   \n",
       "3082           -1.000000           -1.000000           24.542112   \n",
       "6203            0.001637           -1.000000           24.146631   \n",
       "6808           -1.000000           -1.000000           24.542112   \n",
       "6204            0.000455           -1.000000           27.054797   \n",
       "7935            0.003946           -1.000000           18.519041   \n",
       "2510           -1.000000           -1.000000           24.409570   \n",
       "5441            0.003715           -1.000000           22.262790   \n",
       "8550           -1.000000           -1.000000           24.146631   \n",
       "1126            0.000117           -1.000000           24.542112   \n",
       "8068           -1.000000           -1.000000           27.349404   \n",
       "5883            0.003716            0.001711           26.048439   \n",
       "1203            0.000067           -1.000000           27.497909   \n",
       "7724            0.000397           -1.000000           20.749433   \n",
       "3903            0.003510            0.003744           24.277745   \n",
       "...                  ...                 ...                 ...   \n",
       "4658           -1.000000           -1.000000           -1.000000   \n",
       "3005            0.001158           -1.000000           24.146631   \n",
       "2734            0.003282           -1.000000           25.490274   \n",
       "189             0.003144           -1.000000           27.349404   \n",
       "2747            0.003282           -1.000000           24.675373   \n",
       "2047           -1.000000           -1.000000           25.352612   \n",
       "7849           -1.000000           -1.000000           24.675373   \n",
       "2558            0.003791           -1.000000           24.542112   \n",
       "8666           -1.000000           -1.000000           20.525920   \n",
       "6396            0.000800            0.000651           22.022976   \n",
       "3385            0.002251            0.001354           22.998016   \n",
       "4555            0.001273           -1.000000           26.618825   \n",
       "1184           -1.000000           -1.000000           23.629218   \n",
       "6420            0.001781           -1.000000           25.767845   \n",
       "5051            0.001893           -1.000000           21.904039   \n",
       "5311            0.001527           -1.000000           19.869725   \n",
       "2433           -1.000000           -1.000000           -1.000000   \n",
       "6949            0.002869           -1.000000           25.215694   \n",
       "769             0.003370            0.003374           24.542112   \n",
       "1685            0.003843           -1.000000           24.944070   \n",
       "8322            0.000079           -1.000000           26.048439   \n",
       "5578            0.001672           -1.000000           25.079514   \n",
       "4426            0.002048            0.002089           22.262790   \n",
       "466            -1.000000           -1.000000           24.944070   \n",
       "6265           -1.000000           -1.000000           24.542112   \n",
       "5734            0.003366            0.003373           24.675373   \n",
       "5191            0.002688           -1.000000           21.203786   \n",
       "5390            0.000361           -1.000000           27.647220   \n",
       "860            -1.000000           -1.000000           28.100035   \n",
       "7270           -1.000000           -1.000000           28.560266   \n",
       "\n",
       "      down_packet_2min_x  down_packet_3min_x  down_packet_4min_x  \\\n",
       "2002           17.166958           14.993341           -1.000000   \n",
       "6972           17.166958            9.411176            6.654706   \n",
       "1916           17.353894            9.827844           -1.000000   \n",
       "1550           16.982036           11.943262            7.063154   \n",
       "2358           17.927004           14.514017            8.175177   \n",
       "6726           17.638121           11.943262            7.140066   \n",
       "3884           17.353894            3.169077           -1.000000   \n",
       "7947           17.733895           14.435633           10.431050   \n",
       "1739           15.321652            9.259521            6.338153   \n",
       "4071           14.203012            3.789152           -1.000000   \n",
       "8229           17.927004            8.399553            6.036658   \n",
       "1650           15.488494            9.411176            6.690841   \n",
       "73             13.454343            7.744247            5.505744   \n",
       "7943           11.878761            3.589418           -1.000000   \n",
       "6681           13.309413            7.660826           -1.000000   \n",
       "3404           18.024346           -1.000000           -1.000000   \n",
       "3082           18.024346            5.780723           -1.000000   \n",
       "6203           15.913591            8.963502            4.887384   \n",
       "6808           18.220618           12.138872           -1.000000   \n",
       "6204           14.993341            9.061108            2.921836   \n",
       "7935            5.812112            3.493535            2.497202   \n",
       "2510           18.319554           12.337687           -1.000000   \n",
       "5441            9.360350            5.535640            2.607763   \n",
       "8550            8.445161           -1.000000           -1.000000   \n",
       "1126           15.657153            9.565315            4.481751   \n",
       "8068           12.676306           -1.000000           -1.000000   \n",
       "5883           10.601893            6.372569            4.506086   \n",
       "1203           11.071279            6.800425            2.767820   \n",
       "7724           11.624224            4.994404            2.417369   \n",
       "3903           18.122216            6.512110            3.872124   \n",
       "...                  ...                 ...                 ...   \n",
       "4658           -1.000000           -1.000000           -1.000000   \n",
       "3005           18.619598           11.943262            2.636159   \n",
       "2734           15.321652            9.110309            6.583022   \n",
       "189            12.539757            7.537377            5.215525   \n",
       "2747           17.927004            7.619453            4.579889   \n",
       "2047           15.488494            9.411176           -1.000000   \n",
       "7849           17.542865           -1.000000           -1.000000   \n",
       "2558           17.830188           13.974139           10.152408   \n",
       "8666           11.943262           -1.000000           -1.000000   \n",
       "6396           14.912368            9.061108            7.257009   \n",
       "3385           15.156608            9.934862            5.387767   \n",
       "4555           14.280133            7.913824            5.300947   \n",
       "1184            6.949335            2.524395           -1.000000   \n",
       "6420           15.742170            8.219568            4.731139   \n",
       "5051           20.195158           10.262960            6.690841   \n",
       "5311           12.337687            8.963502            4.043557   \n",
       "2433           -1.000000           -1.000000           -1.000000   \n",
       "6949           15.321652            9.209514            6.800425   \n",
       "769            18.122216           11.750803            7.956795   \n",
       "1685           17.260173           14.831833            8.771433   \n",
       "8322           18.519041           10.717340            3.935542   \n",
       "5578           15.238907            9.827844            4.433474   \n",
       "4426           18.220618           13.454343            6.547470   \n",
       "466            17.927004            9.513657           -1.000000   \n",
       "6265           12.676306            2.890362           -1.000000   \n",
       "5734           17.542865           10.262960            7.217817   \n",
       "5191            8.175177            5.417022            3.169077   \n",
       "5390           19.443958            8.399553            4.433474   \n",
       "860            11.878761            7.296414           -1.000000   \n",
       "7270           13.454343            7.660826           -1.000000   \n",
       "\n",
       "      down_packet_5min_x  \n",
       "2002           -1.000000  \n",
       "6972            5.048789  \n",
       "1916           -1.000000  \n",
       "1550            3.589418  \n",
       "2358           -1.000000  \n",
       "6726            4.913922  \n",
       "3884           -1.000000  \n",
       "7947            7.537377  \n",
       "1739           -1.000000  \n",
       "4071           -1.000000  \n",
       "8229            3.978398  \n",
       "1650           -1.000000  \n",
       "73              4.291739  \n",
       "7943           -1.000000  \n",
       "6681           -1.000000  \n",
       "3404           -1.000000  \n",
       "3082           -1.000000  \n",
       "6203           -1.000000  \n",
       "6808           -1.000000  \n",
       "6204           -1.000000  \n",
       "7935           -1.000000  \n",
       "2510           -1.000000  \n",
       "5441           -1.000000  \n",
       "8550           -1.000000  \n",
       "1126           -1.000000  \n",
       "8068           -1.000000  \n",
       "5883            2.621922  \n",
       "1203           -1.000000  \n",
       "7724           -1.000000  \n",
       "3903            2.738005  \n",
       "...                  ...  \n",
       "4658           -1.000000  \n",
       "3005           -1.000000  \n",
       "2734           -1.000000  \n",
       "189            -1.000000  \n",
       "2747           -1.000000  \n",
       "2047           -1.000000  \n",
       "7849           -1.000000  \n",
       "2558           -1.000000  \n",
       "8666           -1.000000  \n",
       "6396            2.565740  \n",
       "3385            2.890362  \n",
       "4555           -1.000000  \n",
       "1184           -1.000000  \n",
       "6420           -1.000000  \n",
       "5051           -1.000000  \n",
       "5311           -1.000000  \n",
       "2433           -1.000000  \n",
       "6949           -1.000000  \n",
       "769             6.102393  \n",
       "1685           -1.000000  \n",
       "8322           -1.000000  \n",
       "5578           -1.000000  \n",
       "4426            2.969690  \n",
       "466            -1.000000  \n",
       "6265           -1.000000  \n",
       "5734            5.476010  \n",
       "5191           -1.000000  \n",
       "5390           -1.000000  \n",
       "860            -1.000000  \n",
       "7270           -1.000000  \n",
       "\n",
       "[6994 rows x 132 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\\nimport matplotlib.pyplot as plt\\n\\nrandom_forest = RandomForestClassifier(random_state=42)\\ny_probas_forest_y = cross_val_predict(random_forest, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\\ny_probas_forest_n = cross_val_predict(random_forest, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\\ny_probas_forest_t = cross_val_predict(random_forest, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\\ny_probas_forest = cross_val_predict(random_forest, x_train, y_train, cv=10, method=\"predict_proba\")\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest_y = cross_val_predict(random_forest, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest_n = cross_val_predict(random_forest, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest_t = cross_val_predict(random_forest, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "y_probas_forest = cross_val_predict(random_forest, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_scores_forest_y = y_probas_forest_y[:, 1]\\ny_scores_forest_n = y_probas_forest_n[:, 1]\\ny_scores_forest_t = y_probas_forest_t[:, 1]\\n\\nfpr_forest_y, tpr_forest_y, thresholds_forest_y = roc_curve(y_train_youtube, y_scores_forest_y)\\nfpr_forest_n, tpr_forest_n, thresholds_forest_n = roc_curve(y_train_netflix, y_scores_forest_n)\\nfpr_forest_t, tpr_forest_t, thresholds_forest_t = roc_curve(y_train_twitch, y_scores_forest_t)\\n#fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_probas_forest)\\n\\n\\nplot_roc_curve(fpr_forest_y, tpr_forest_y, \"YouTube\")\\nplot_roc_curve(fpr_forest_n, tpr_forest_n, \"Netflix\")\\nplot_roc_curve(fpr_forest_t, tpr_forest_t, \"Twitch\")\\n#plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\\n\\nplt.legend(loc=\"lower right\")\\nplt.show()\\n#cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring=\\'accuracy\\')\\n#accuracy = sum(cvs)/len(cvs)\\n#print(\"Accuracy: \" + str(accuracy))\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "y_scores_forest_y = y_probas_forest_y[:, 1]\n",
    "y_scores_forest_n = y_probas_forest_n[:, 1]\n",
    "y_scores_forest_t = y_probas_forest_t[:, 1]\n",
    "\n",
    "fpr_forest_y, tpr_forest_y, thresholds_forest_y = roc_curve(y_train_youtube, y_scores_forest_y)\n",
    "fpr_forest_n, tpr_forest_n, thresholds_forest_n = roc_curve(y_train_netflix, y_scores_forest_n)\n",
    "fpr_forest_t, tpr_forest_t, thresholds_forest_t = roc_curve(y_train_twitch, y_scores_forest_t)\n",
    "#fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_probas_forest)\n",
    "\n",
    "\n",
    "plot_roc_curve(fpr_forest_y, tpr_forest_y, \"YouTube\")\n",
    "plot_roc_curve(fpr_forest_n, tpr_forest_n, \"Netflix\")\n",
    "plot_roc_curve(fpr_forest_t, tpr_forest_t, \"Twitch\")\n",
    "#plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "#cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "#accuracy = sum(cvs)/len(cvs)\n",
    "#print(\"Accuracy: \" + str(accuracy))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrandom_forest.fit(x_train, y_train)\\npredictions = random_forest.predict(x_test)\\nconf_mx = confusion_matrix(y_test, predictions)\\nplt.matshow(conf_mx, cmap=plt.cm.gray)\\nconf_mx\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "random_forest.fit(x_train, y_train)\n",
    "predictions = random_forest.predict(x_test)\n",
    "conf_mx = confusion_matrix(y_test, predictions)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "conf_mx\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search_acc = GridSearchCV(random_forest, params, cv=10, scoring='accuracy')\n",
    "#grid_search_acc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best2 = grid_search_acc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(zip(grid_search.best_estimator_.feature_importances_, basic_stats), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Final evaluation\\nrf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\\'entropy\\', max_depth=9, max_features=\\'log2\\', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=42, verbose=0, warm_start=False)\\nrf.fit(x_train, y_train)\\nprint(\"Accuracy train set: \" + str(sum(rf.predict(x_train) == y_train)/float(len(y_train))))\\nprint(\"Accuracy test set: \" + str(sum(rf.predict(x_test) == y_test)/float(len(y_test))))\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Final evaluation\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "rf.fit(x_train, y_train)\n",
    "print(\"Accuracy train set: \" + str(sum(rf.predict(x_train) == y_train)/float(len(y_train))))\n",
    "print(\"Accuracy test set: \" + str(sum(rf.predict(x_test) == y_test)/float(len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncrossval = cross_val_predict(rf, x_train, y_train, cv=10, method=\"predict_proba\")\\ncrossvalscore = cross_val_score(rf, x_train, y_train, cv=10, scoring=\"accuracy\")\\nprint(\"\\tCrossValScore: \" + str(sum(crossvalscore)/len(cv_yt)))\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "crossval = cross_val_predict(rf, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "crossvalscore = cross_val_score(rf, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "print(\"\\tCrossValScore: \" + str(sum(crossvalscore)/len(cv_yt)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_test_forest(model):\n",
    "    '''\n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    \n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    '''\n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    '''\n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "    '''\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def do_test_svm(model):    \n",
    "    '''\n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "    \n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    '''\n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    '''\n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    '''\n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test_knn(model):\n",
    "    \n",
    "    y_probas_y = cross_val_predict(model, x_train, y_train_youtube, cv=10, method=\"predict_proba\")\n",
    "    y_probas_n = cross_val_predict(model, x_train, y_train_netflix, cv=10, method=\"predict_proba\")\n",
    "    y_probas_t = cross_val_predict(model, x_train, y_train_twitch, cv=10, method=\"predict_proba\")\n",
    "    y_probas = cross_val_predict(model, x_train, y_train, cv=10, method=\"predict_proba\")\n",
    "    y_scores_y = y_probas_y[:, 1]\n",
    "    y_scores_n = y_probas_n[:, 1]\n",
    "    y_scores_t = y_probas_t[:, 1]\n",
    "    \n",
    "\n",
    "    fpr_y, tpr_y, thresholds_y = roc_curve(y_train_youtube, y_scores_y)\n",
    "    fpr_n, tpr_n, thresholds_n = roc_curve(y_train_netflix, y_scores_n)\n",
    "    fpr_t, tpr_t, thresholds_t = roc_curve(y_train_twitch, y_scores_t)\n",
    "    plot_roc_curve(fpr_y, tpr_y, \"YouTube\")\n",
    "    plot_roc_curve(fpr_n, tpr_n, \"Netflix\")\n",
    "    plot_roc_curve(fpr_t, tpr_t, \"Twitch\")\n",
    "    #plot_roc_curve(fpr_forest, tpr_forest, \"Multiclass\")\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #cvs = cross_val_score(random_forest, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    #accuracy = sum(cvs)/len(cvs)\n",
    "    #print(\"Accuracy: \" + str(accuracy))\n",
    "    \n",
    "    cv_yt = cross_val_score(model, x_train, y_train_youtube, cv=10, scoring=\"accuracy\")\n",
    "    cv_nf = cross_val_score(model, x_train, y_train_netflix, cv=10, scoring=\"accuracy\")\n",
    "    cv_tw = cross_val_score(model, x_train, y_train_twitch, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    \n",
    "    print(\"YouTube: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_yt)/len(cv_yt)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_youtube, y_scores_y)))\n",
    "    print(\"Netflix: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_nf)/len(cv_nf)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_netflix, y_scores_n)))\n",
    "    print(\"Twitch: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_tw)/len(cv_tw)) + \"\\n\\tRocAucScore:  \" + str(roc_auc_score(y_train_twitch, y_scores_t)))\n",
    "    \n",
    "    print(\"Multiclass: \")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Random Forest------\n",
      "[[1372   13]\n",
      " [  74  290]]\n",
      "\n",
      "Accuracy train set: 0.9887046039462396\n",
      "Accuracy test set: 0.9502572898799314\n",
      "-----OvO Classifier Random Forest------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "OneVsOneClassifier can not be fit when only one class is present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c7308d3c41ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsOneClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdo_test_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m '''\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-b767a046dc29>\u001b[0m in \u001b[0;36mdo_test_forest\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcv_yt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_youtube\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcv_nf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_netflix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcv_tw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_twitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mcv_mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     '''\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(\"OneVsOneClassifier can not be fit when only one\"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              \" class is present.\")\n\u001b[1;32m    500\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: OneVsOneClassifier can not be fit when only one class is present."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABZZJREFUeJzt2zGLXeUexeH1vxOE9JlK5Y6FSFIHP0OsbE0tpPID+EVsUgQ7xdJCsLWxcNJpRAjCxdg4F7s0Iry3cIrcak7i2XNmXM9T5RwObxbs+bH3ZCaz1grQ5V+HHgBcPuFDIeFDIeFDIeFDIeFDIeG/hJm5NzM/zczTmfn40HvY3cw8mpnfZub7Q2+5CoS/o5k5SvJJkveS3Elyf2buHHYVL+HTJPcOPeKqEP7u3k3ydK3181rrjySfJ3n/wJvY0VrrmyS/H3rHVSH83b2e5JcXXj87fw+uHeFDIeHv7tckb77w+o3z9+DaEf7uvkvy9sy8NTOvJfkgyZcH3gSvRPg7Wmv9meSjJF8n+THJF2utHw67il3NzGdJvk3yzsw8m5kPD73pkMZ/y4U+7vhQSPhQSPhQSPhQSPhQSPgvaWYeHHoDr871+4vwX54vnOvN9YvwodImv8Bz69atdXJysvdzr4Kzs7McHx8fesamHj9+fOgJ/A1rrbnoMze2+ItPTk5yenq6xdFcgpkLv2645jzqQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQyHhQ6Gdwp+ZezPz08w8nZmPtx4FbOvC8GfmKMknSd5LcifJ/Zm5s/UwYDu73PHfTfJ0rfXzWuuPJJ8neX/bWcCWdgn/9SS/vPD62fl7wDW1t3/cm5kHM3M6M6dnZ2f7OhbYwC7h/5rkzRdev3H+3v9Zaz1ca91da909Pj7e1z5gA7uE/12St2fmrZl5LckHSb7cdhawpRsXfWCt9efMfJTk6yRHSR6ttX7YfBmwmQvDT5K11ldJvtp4C3BJ/OYeFBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FBI+FJq11t4PPTo6Wjdv3tz7uVyO27dvH3oCr+jJkyd5/vz5XPQ5d3woJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwoJHwodGH4M/NoZn6bme8vYxCwvV3u+J8mubfxDuASXRj+WuubJL9fwhbgkvgeHwrd2NdBM/MgyYPzP+/rWGADewt/rfUwycMkOTo6Wvs6F9g/j/pQaJcf532W5Nsk78zMs5n5cPtZwJYufNRfa92/jCHA5fGoD4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4WED4VmrbX/Q2fOkvxn7wdfDbeS/PfQI3hl//Tr9++11vFFH9ok/H+ymTlda9099A5ejev3F4/6UEj4UEj4L+/hoQfwt7h+8T0+VHLHh0LCh0LCh0LCh0LCh0L/Azdxq0m0stYsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"-----Random Forest------\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "do_test_forest(rf)\n",
    "\n",
    "print(\"-----OvO Classifier Random Forest------\")\n",
    "\n",
    "\n",
    "\n",
    "rf = OneVsOneClassifier(rf)\n",
    "do_test_forest(rf)\n",
    "'''\n",
    "\n",
    "print(\"----------SVM-----------\")\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "do_test_svm(svm)\n",
    "\n",
    "print(\"----------Knn-----------\")\n",
    "knn = KNeighborsClassifier()\n",
    "do_test_knn(knn)\n",
    "\n",
    "print(\"-----Neural Network-----\")\n",
    "nn = MLPClassifier()\n",
    "do_test_knn(nn)\n",
    "\n",
    "\n",
    "print(\"--------AdaBoost Random Forest--------\")\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9,\n",
    "                            max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\n",
    "                            random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    rf,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n",
    "\n",
    "print(\"---------Decision Tree------\")\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "do_test_forest(dt)\n",
    "\n",
    "print(\"--------AdaBoost Decision Tree--------\")\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    dt,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nparams = dict()\\n\\nrandom_forest = RandomForestClassifier()\\n\\nparams[\\'forest\\'] = {\\n    \\'max_depth\\' : [6,7,8,9],\\n    \\'n_estimators\\': [30,100,300],\\n    \\'criterion\\': [\\'gini\\',\\'entropy\\'],\\n    \\'max_features\\': [\\'auto\\', \\'sqrt\\', \\'log2\\'],\\n    \\'min_samples_leaf\\' : [1, 2, 4, 6, 8, 10],\\n    \\'min_samples_split\\': [2, 3, 10],\\n}\\n\\ngrid_search = GridSearchCV(random_forest, params[\\'forest\\'], cv=10, scoring=\\'accuracy\\')\\ngrid_search.fit(x_train, y_train)\\n\\nbest_random_forest = grid_search.best_estimator_\\n\\npredictions = best_random_forest.predict(x_test)\\nconf_mx = confusion_matrix(y_test, predictions)\\nplt.matshow(conf_mx, cmap=plt.cm.gray)\\nprint(conf_mx)\\nprint(\"\\nAccuracy train set: \" + str(sum(best_random_forest.predict(x_train) == y_train)/float(len(y_train))))\\nprint(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = dict()\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "params['forest'] = {\n",
    "    'max_depth' : [6,7,8,9],\n",
    "    'n_estimators': [30,100,300],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf' : [1, 2, 4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 3, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(random_forest, params['forest'], cv=10, scoring='accuracy')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "\n",
    "predictions = best_random_forest.predict(x_test)\n",
    "conf_mx = confusion_matrix(y_test, predictions)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "print(conf_mx)\n",
    "print(\"\\nAccuracy train set: \" + str(sum(best_random_forest.predict(x_train) == y_train)/float(len(y_train))))\n",
    "print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Random Forest------\n",
      "[[63  1  0  0]\n",
      " [ 1 52  0  2]\n",
      " [ 0  0 19  0]\n",
      " [ 1  5  0 31]]\n",
      "\n",
      "Accuracy train set: 1.0\n",
      "Accuracy test set: 0.9428571428571428\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"-----Random Forest------\")\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=9, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "do_test_forest(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/rf_model_all_60s_1s_64.csv.sav']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf, '../models/rf_model_' + file_name + '.sav') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.85775709 -1.36511737 -1.90905858 -1.75841844]\n",
      " [-1.30671931 -1.27479909 -3.90819891 -0.84459922]\n",
      " [-1.65517134 -0.77969983 -5.01063529 -1.06789769]\n",
      " [-0.80793102 -1.54991106 -2.71487541 -1.28829499]\n",
      " [-1.13787692 -0.89622223 -5.01063529 -1.32905958]\n",
      " [-1.46798092 -1.17841638 -5.70378247 -0.77977156]\n",
      " [-0.87102305 -1.15403153 -3.38469001 -1.46004475]\n",
      " [-1.55410649 -0.60344535 -4.60517019 -1.46232137]\n",
      " [-0.98766641 -0.94960912 -4.07990388 -1.4972086 ]\n",
      " [-1.48947032 -1.4780981  -4.60517019 -0.62280635]\n",
      " [-1.24363479 -0.71502763 -5.70378247 -1.51798154]]\n",
      "[0. 3. 1. 0. 1. 3. 0. 1. 1. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "for path, subdirs, files in os.walk(base_folder):\n",
    "    for name in files:\n",
    "        if name == 'cap3.csv':\n",
    "            test = pd.read_csv(os.path.join(str(path), str(name)))\n",
    "test = test.drop(columns=['Unnamed: 0']).reset_index()\n",
    "test.drop(columns=['index', 'up_packet_silence_mean', 'down_packet_silence_mean',\n",
    "                      'down_packet_longest_silence', 'down_packet_shortest_silence','label'], inplace=True)\n",
    "# Imputer for NaN\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(test)\n",
    "test = pd.DataFrame(imputer.transform(test), columns=test.columns)\n",
    "print(rf.predict_log_proba(test))\n",
    "print(rf.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "rf = joblib.load('../models/rf_model_01s.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c6f8f2dd61c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/thesis/env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2560\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4000"
     ]
    }
   ],
   "source": [
    "print(prevlabel[4000])\n",
    "dataset['label'][4000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
