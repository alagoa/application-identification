{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "\n",
    "def calc_scalogram(data, scales):\n",
    "    S,scales= scalogramCWT(data,scales)\n",
    "    return S\n",
    "def show_scalo(data, scales, colors):\n",
    "    for i in range (0, len(data)):\n",
    "        plt.plot(scales, data[i], colors[i], lw=3)\n",
    "    plt.show()\n",
    "    \n",
    "# Get top X spikes from scalogram, sorted by value\n",
    "def get_spikes(scalo, comparator):\n",
    "    len(scalo)\n",
    "    spikes = deque([(-1,-1)] * 5, maxlen=5)\n",
    "    #aux = argrelextrema(scalo, comparator, order=int(len(scalo)/10))\n",
    "    aux = argrelextrema(scalo, comparator)\n",
    "    if aux[0].size:\n",
    "        for x in np.nditer(aux) or []:\n",
    "            spikes.append((scalo[x], scales[x]))\n",
    "    ordered = sorted(spikes, key=lambda x: x[1], reverse=True)\n",
    "    values = np.hstack(zip(*ordered))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from scipy import stats\n",
    "def get_stats_numpy(data, zero):\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    std = np.std(data)\n",
    "    var = np.var(data)\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    pc = [25,50,75,90]\n",
    "    percentiles = np.array(np.percentile(data, pc))\n",
    "    silences = np.count_nonzero(np.asarray(data)==zero)\n",
    "    silence_mean = np.mean(list(sum(1 for _ in g) for k, g in groupby(data) if k==zero))\n",
    "    longest_silence = max(sum(1 for _ in g) for k, g in groupby(data) if k==zero) if silences > 0 else 0\n",
    "    shortest_silence = min(sum(1 for _ in g) for k, g in groupby(data) if k==zero) if silences > 0 else 0\n",
    "    \n",
    "    #print(\"Mean: \" + str(mean))\n",
    "    #print(\"Media: \" + str(median))\n",
    "    #print(\"StdDev: \" + str(std))\n",
    "    #print(\"Variance: \" + str(var))\n",
    "    #print(\"Skewness: \" + str(skew))\n",
    "    #print(\"Kurtosis: \" + str(kurt))\n",
    "    #print(\"Pc25: \" + str(percentiles[0]))\n",
    "    #print(\"Pc50: \" + str(percentiles[1]))\n",
    "    #print(\"Pc75: \" + str(percentiles[2]))\n",
    "    \n",
    "    features = np.hstack((mean, median, std, var, skew, kurt, percentiles, silences, silence_mean, longest_silence, shortest_silence))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all on a numpy array\n",
    "def get_features_numpy(info, local_max_up_bytes, local_min_up_bytes, local_max_up_packet, local_min_up_packet, \n",
    "                       local_max_down_bytes, local_min_down_bytes, local_max_down_packet, local_min_down_packet, zeros):\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for idx in range(0, len(info)):\n",
    "        result.append(\n",
    "            np.hstack(\n",
    "                (\n",
    "                 get_stats_numpy(info[idx]['up']['byte_count'], zeros[1]),\n",
    "                 get_stats_numpy(info[idx]['up']['packet_count'], zeros[0]),\n",
    "                 local_max_up_bytes[idx], local_min_up_bytes[idx],\n",
    "                 local_max_up_packet[idx], local_min_up_packet[idx],\n",
    "                 get_stats_numpy(info[idx]['down']['byte_count'], zeros[3]),\n",
    "                 get_stats_numpy(info[idx]['down']['packet_count'], zeros[2]),\n",
    "                 local_max_down_bytes[idx], local_min_down_bytes[idx],\n",
    "                 local_max_down_packet[idx], local_min_down_packet[idx],\n",
    "\n",
    "            ))\n",
    "        )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(data):\n",
    "    info = []\n",
    "    up = defaultdict(list)\n",
    "    down = defaultdict(list)\n",
    "    result = {}\n",
    "    count = 0\n",
    "    for second in data:\n",
    "        up['packet_count'].append(second[0])\n",
    "        up['byte_count'].append(second[1])\n",
    "        down['packet_count'].append(second[2])\n",
    "        down['byte_count'].append(second[3])\n",
    "        count+=1\n",
    "        if count >= sample_size:\n",
    "            result['up'] = up\n",
    "            result['down'] = down\n",
    "            info.append(result)\n",
    "            up = defaultdict(list)\n",
    "            down = defaultdict(list)\n",
    "            result = {}\n",
    "            count = 0\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_number(s):\n",
    "    return list(filter(None, re.split(r'(\\d+)', s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2acb7edf8205>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2acb7edf8205>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "from scalogram import *\n",
    "import pandas as pd\n",
    "\n",
    "base_url = '../../../shared/'\n",
    "sample_size = 30 # number of intervals for each sample\n",
    "\n",
    "for path, subdirs, files in os.walk(base_url):\n",
    "    for name in files:\n",
    "        print(name)\n",
    "        if os.path.basename(path) != 'dat':\n",
    "            continue\n",
    "        \n",
    "        print(os.path.join(path, name))\n",
    "        \n",
    "        #[up_n_packets, up_n_bytes, up_flag, down_n_packets, down_n_bytes, down_flag]\n",
    "        data = np.loadtxt(os.path.join(path, name))\n",
    "        data = np.delete(data, [2,5], 1)\n",
    "\n",
    "\n",
    "        # Normalize\n",
    "        data = np.vstack([data,[0,0,0,0]]) # put zeros\n",
    "        \n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        \n",
    "        zeros = data[-1]\n",
    "        data = data[:-1]\n",
    "        \n",
    "        # Split data into categories\n",
    "        info = get_info(data)\n",
    "\n",
    "        #Scalogram\n",
    "        N = sample_size\n",
    "        dj=1/128\n",
    "        s0=2\n",
    "        J=1/dj * np.log2(0.5*N/s0)\n",
    "        scales=s0*2**(np.arange(J)*dj)\n",
    "\n",
    "        scalos_up = []\n",
    "        scalos_down = []\n",
    "        for idx, sample in enumerate(info):\n",
    "                scalos_up.append(\n",
    "                    (calc_scalogram(np.asarray(sample['up']['byte_count']), scales),\n",
    "                     calc_scalogram(np.asarray(sample['up']['packet_count']), scales))\n",
    "                )\n",
    "                scalos_down.append(\n",
    "                    (calc_scalogram(np.asarray(sample['down']['byte_count']), scales),\n",
    "                     calc_scalogram(np.asarray(sample['down']['packet_count']), scales))\n",
    "                )\n",
    "\n",
    "        local_max_up_bytes = []\n",
    "        local_min_up_bytes = []\n",
    "        local_max_up_packet = []\n",
    "        local_min_up_packet = []\n",
    "        local_max_down_bytes = []\n",
    "        local_min_down_bytes = []\n",
    "        local_max_down_packet = []\n",
    "        local_min_down_packet = []\n",
    "\n",
    "        for scalo in scalos_up:\n",
    "            local_max_up_bytes.append(get_spikes(scalo[0], np.greater))\n",
    "            local_min_up_bytes.append(get_spikes(scalo[0], np.less))\n",
    "            local_max_up_packet.append(get_spikes(scalo[1], np.greater))\n",
    "            local_min_up_packet.append(get_spikes(scalo[1], np.less))\n",
    "\n",
    "        for scalo in scalos_down:\n",
    "            local_max_down_bytes.append(get_spikes(scalo[0], np.greater))\n",
    "            local_min_down_bytes.append(get_spikes(scalo[0], np.less))\n",
    "            local_max_down_packet.append(get_spikes(scalo[1], np.greater))\n",
    "            local_min_down_packet.append(get_spikes(scalo[1], np.less))\n",
    "\n",
    "        # Put it in a pandas Dataframe and save it to csv\n",
    "        samples = get_features_numpy(info, local_max_up_bytes, local_min_up_bytes, local_max_up_packet, local_min_up_packet,\n",
    "                                    local_max_down_bytes, local_min_down_bytes, local_max_down_packet, local_min_down_packet,\n",
    "                                    zeros)\n",
    "\n",
    "\n",
    "        names = [\n",
    "            'up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var', 'up_bytes_skew', 'up_bytes_kurt',\n",
    "            'up_bytes_perc25', 'up_bytes_perc50', 'up_bytes_perc75', 'up_bytes_perc90',\n",
    "            'up_bytes_silences', 'up_bytes_silence_mean', 'up_bytes_longest_silence', 'up_bytes_shortest_silence',\n",
    "            'up_packet_mean', 'up_packet_median', 'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
    "            'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75', 'up_packet_perc90',\n",
    "            'up_packet_silences', 'up_packet_silence_mean', 'up_packet_longest_silence', 'up_packet_shortest_silence',\n",
    "            'up_bytes_1max_y', 'up_bytes_2max_y', 'up_bytes_3max_y', 'up_bytes_4max_y', 'up_bytes_5max_y',\n",
    "            'up_bytes_1max_x', 'up_bytes_2max_x', 'up_bytes_3max_x', 'up_bytes_4max_x', 'up_bytes_5max_x',\n",
    "            'up_bytes_1min_y', 'up_bytes_2min_y', 'up_bytes_3min_y', 'up_bytes_4min_y', 'up_bytes_5min_y',\n",
    "            'up_bytes_1min_x', 'up_bytes_2min_x', 'up_bytes_3min_x', 'up_bytes_4min_x', 'up_bytes_5min_x',\n",
    "            'up_packet_1max_y', 'up_packet_2max_y', 'up_packet_3max_y', 'up_packet_4max_y', 'up_packet_5max_y',\n",
    "            'up_packet_1max_x', 'up_packet_2max_x', 'up_packet_3max_x', 'up_packet_4max_x', 'up_packet_5max_x',\n",
    "            'up_packet_1min_y', 'up_packet_2min_y', 'up_packet_3min_y', 'up_packet_4min_y', 'up_packet_5min_y',\n",
    "            'up_packet_1min_x', 'up_packet_2min_x', 'up_packet_3min_x', 'up_packet_4min_x', 'up_packet_5min_x',\n",
    "\n",
    "            'down_bytes_mean', 'down_bytes_median', 'down_bytes_std', 'down_bytes_var', 'down_bytes_skew', 'down_bytes_kurt',\n",
    "            'down_bytes_perc25', 'down_bytes_perc50', 'down_bytes_perc75', 'down_bytes_perc90',\n",
    "            'down_bytes_silences', 'down_bytes_silence_mean', 'down_bytes_longest_silence', 'down_bytes_shortest_silence',\n",
    "            'down_packet_mean', 'down_packet_median', 'down_packet_std', 'down_packet_var', 'down_packet_skew', 'down_packet_kurt',\n",
    "            'down_packet_perc25', 'down_packet_perc50', 'down_packet_perc75', 'down_packet_perc90',\n",
    "            'down_packet_silences', 'down_packet_silence_mean', 'down_packet_longest_silence', 'down_packet_shortest_silence',\n",
    "            'down_bytes_1max_y', 'down_bytes_2max_y', 'down_bytes_3max_y', 'down_bytes_4max_y', 'down_bytes_5max_y',\n",
    "            'down_bytes_1max_x', 'down_bytes_2max_x', 'down_bytes_3max_x', 'down_bytes_4max_x', 'down_bytes_5max_x',\n",
    "            'down_bytes_1min_y', 'down_bytes_2min_y', 'down_bytes_3min_y', 'down_bytes_4min_y', 'down_bytes_5min_y',\n",
    "            'down_bytes_1min_x', 'down_bytes_2min_x', 'down_bytes_3min_x', 'down_bytes_4min_x', 'down_bytes_5min_x',\n",
    "            'down_packet_1max_y', 'down_packet_2max_y', 'down_packet_3max_y', 'down_packet_4max_y', 'down_packet_5max_y',\n",
    "            'down_packet_1max_x', 'down_packet_2max_x', 'down_packet_3max_x', 'down_packet_4max_x', 'down_packet_5max_x',\n",
    "            'down_packet_1min_y', 'down_packet_2min_y', 'down_packet_3min_y', 'down_packet_4min_y', 'down_packet_5min_y',\n",
    "            'down_packet_1min_x', 'down_packet_2min_x', 'down_packet_3min_x', 'down_packet_4min_x', 'down_packet_5min_x'\n",
    "        ]\n",
    "    \n",
    "        df = pd.DataFrame(samples, columns=names)\n",
    "        outdir = 'csv/' + str(sample_size) + 's' + '.'.join(name.split('_')[1].split('.')[:-1]) + '/' + split_number(name)[0] + '/'        \n",
    "        outname =  name.split('.')[0] + '.csv'\n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        df['label'] = name.split('/')[-1].split('.')[0]\n",
    "        print(name.split('/')[-1].split('.')[0])\n",
    "        df.to_csv(os.path.join(outdir, outname), sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
