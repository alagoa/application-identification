{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool creates ML models with the given datasets to detect video vs. regular browsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_number(s):\n",
    "    return list(filter(None, re.split(r'(\\d+)', s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels, separated by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_stats = [\n",
    "    'up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var', 'up_bytes_skew', 'up_bytes_kurt',\n",
    "    'up_bytes_perc25', 'up_bytes_perc50', 'up_bytes_perc75', 'up_bytes_perc90',\n",
    "    'up_packet_mean', 'up_packet_median', 'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
    "    'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75', 'up_packet_perc90',\n",
    "    'down_bytes_mean', 'down_bytes_median', 'down_bytes_std', 'down_bytes_var', 'down_bytes_skew', 'down_bytes_kurt',\n",
    "    'down_bytes_perc25', 'down_bytes_perc50', 'down_bytes_perc75', 'down_bytes_perc90',\n",
    "    'down_packet_mean', 'down_packet_median', 'down_packet_std', 'down_packet_var', 'down_packet_skew', 'down_packet_kurt',\n",
    "    'down_packet_perc25', 'down_packet_perc50', 'down_packet_perc75', 'down_packet_perc90']\n",
    "\n",
    "silences = ['down_bytes_silences', 'down_bytes_silence_mean', 'down_bytes_longest_silence', 'down_bytes_shortest_silence',\n",
    "           'up_bytes_silences', 'up_bytes_silence_mean', 'up_bytes_longest_silence', 'up_bytes_shortest_silence']\n",
    "\n",
    "\n",
    "scalogram_1 = ['up_bytes_1max_y', 'up_bytes_1max_x', 'up_bytes_1min_y', 'up_bytes_1min_x',  \n",
    "    'up_packet_1max_y', 'up_packet_1max_x', 'up_packet_1min_y', 'up_packet_1min_x', 'down_bytes_1max_y',\n",
    "    'down_bytes_1max_x',    'down_bytes_1min_y', 'down_bytes_1min_x', 'down_packet_1max_y',\n",
    "    'down_packet_1max_x', 'down_packet_1min_y', 'down_packet_1min_x']\n",
    "\n",
    "scalogram_2 = ['up_bytes_2max_y', 'up_bytes_2max_x', 'up_bytes_2min_y', 'up_bytes_2min_x',  \n",
    "    'up_packet_2max_y', 'up_packet_2max_x', 'up_packet_2min_y', 'up_packet_2min_x', 'down_bytes_2max_y',\n",
    "    'down_bytes_2max_x',    'down_bytes_2min_y', 'down_bytes_2min_x', 'down_packet_2max_y',\n",
    "    'down_packet_2max_x','down_packet_2min_y','down_packet_2min_x']\n",
    "\n",
    "scalogram_3 = ['up_bytes_3max_y', 'up_bytes_3max_x', 'up_bytes_3min_y', 'up_bytes_3min_x',  \n",
    "    'up_packet_3max_y', 'up_packet_3max_x', 'up_packet_3min_y', 'up_packet_3min_x', 'up_packet_3min_y',\n",
    "    'down_bytes_3max_y', 'down_packet_3min_y', 'down_bytes_3max_x', 'down_bytes_3min_y',\n",
    "    'down_bytes_3min_x', 'down_packet_3max_y','down_packet_3max_x','down_packet_3min_y','down_packet_3min_x']\n",
    "scalogram_4 = ['up_bytes_4max_y', 'up_bytes_4max_x', 'up_bytes_4min_y', 'up_bytes_4min_x',  \n",
    "    'up_packet_4max_y', 'up_packet_4max_x', 'up_packet_4min_y', 'up_packet_4min_x', 'down_bytes_4max_y',\n",
    "    'down_bytes_4max_x',    'down_bytes_4min_y', 'down_bytes_4min_x', 'down_packet_4max_y',\n",
    "    'down_packet_4max_x','down_packet_4min_y','down_packet_4min_x']\n",
    "scalogram_5 = ['up_bytes_5max_y', 'up_bytes_5max_x', 'up_bytes_5min_y', 'up_bytes_5min_x',  \n",
    "    'up_packet_5max_y', 'up_packet_5max_x', 'up_packet_5min_y', 'up_packet_5min_x', 'down_bytes_5max_y',\n",
    "    'down_bytes_5max_x',    'down_bytes_5min_y', 'down_bytes_5min_x', 'down_packet_5max_y',\n",
    "    'down_packet_5max_x','down_packet_5min_y','down_packet_5min_x']\n",
    "\n",
    "scalogram = scalogram_1 + scalogram_2 + scalogram_3 + scalogram_4 + scalogram_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['up_bytes_mean', 'up_bytes_median', 'up_bytes_std', 'up_bytes_var',\n",
      "       'up_bytes_skew', 'up_bytes_kurt', 'up_bytes_perc25', 'up_bytes_perc50',\n",
      "       'up_bytes_perc75', 'up_bytes_perc90', 'up_bytes_silences',\n",
      "       'up_bytes_silence_mean', 'up_bytes_longest_silence',\n",
      "       'up_bytes_shortest_silence', 'up_packet_mean', 'up_packet_median',\n",
      "       'up_packet_std', 'up_packet_var', 'up_packet_skew', 'up_packet_kurt',\n",
      "       'up_packet_perc25', 'up_packet_perc50', 'up_packet_perc75',\n",
      "       'up_packet_perc90', 'up_packet_silences', 'up_packet_longest_silence',\n",
      "       'up_packet_shortest_silence', 'down_bytes_mean', 'down_bytes_median',\n",
      "       'down_bytes_std', 'down_bytes_var', 'down_bytes_skew',\n",
      "       'down_bytes_kurt', 'down_bytes_perc25', 'down_bytes_perc50',\n",
      "       'down_bytes_perc75', 'down_bytes_perc90', 'down_bytes_silences',\n",
      "       'down_bytes_silence_mean', 'down_bytes_longest_silence',\n",
      "       'down_bytes_shortest_silence', 'down_packet_mean', 'down_packet_median',\n",
      "       'down_packet_std', 'down_packet_var', 'down_packet_skew',\n",
      "       'down_packet_kurt', 'down_packet_perc25', 'down_packet_perc50',\n",
      "       'down_packet_perc75', 'down_packet_perc90', 'down_packet_silences',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "base_folder = \"csv/\"\n",
    "file_name = 'all_10s_01s_32.csv'\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(str(base_folder), str(file_name)))\n",
    "\n",
    "dataset = dataset.drop(columns=['Unnamed: 0']).reset_index()\n",
    "dataset.drop(columns=['index', 'up_packet_silence_mean', 'down_packet_silence_mean',\n",
    "                      'down_packet_longest_silence', 'down_packet_shortest_silence'] + scalogram, inplace=True)\n",
    "features = dataset.columns\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the labels to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41069\n",
      "video       23293\n",
      "browsing    17776\n",
      "Name: label, dtype: int64\n",
      "37224\n",
      "video       23059\n",
      "browsing    14165\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset.loc[dataset['label'] == 'reddit', 'label'] = 'browsing'\n",
    "dataset.loc[dataset['label'] == 'facebook', 'label'] = 'browsing'\n",
    "\n",
    "dataset_tunnel = dataset[dataset['label'] == 'netflix-openvpn']\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'netflix-ssh'])\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'youtube-openvpn'])\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'youtube-ssh'])\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'twitch-ssh'])\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'twitch-openvpn'])\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'acestream-openvpn'])\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'browsing-openvpn'])\n",
    "dataset_tunnel = dataset_tunnel.append(dataset[dataset['label'] == 'browsing'])\n",
    "\n",
    "# Prepare tunnel dataset for testing\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'netflix-ssh', 'label'] = 'netflix'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'youtube-ssh', 'label'] = 'youtube'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'twitch-ssh', 'label'] = 'twitch'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'netflix-openvpn', 'label'] = 'netflix'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'youtube-openvpn', 'label'] = 'youtube'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'twitch-openvpn', 'label'] = 'twitch'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'acestream-openvpn', 'label'] = 'acestream'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'browsing-openvpn', 'label'] = 'browsing'\n",
    "\n",
    "\n",
    "# Join applications from different capture settings\n",
    "dataset_normal = dataset[dataset['label'] == 'netflix']\n",
    "dataset_normal = dataset_normal.append(dataset[dataset['label'] == 'youtube'])\n",
    "dataset_normal = dataset_normal.append(dataset[dataset['label'] == 'twitch'])\n",
    "dataset_normal = dataset_normal.append(dataset[dataset['label'] == 'acestream'])\n",
    "dataset_normal = dataset_normal.append(dataset[dataset['label'] == 'browsing'])\n",
    "\n",
    "# Video vs. Rest\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'netflix', 'label'] = 'video'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'youtube', 'label'] = 'video'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'twitch', 'label'] = 'video'\n",
    "dataset_tunnel.loc[dataset_tunnel['label'] == 'acestream', 'label'] = 'video'\n",
    "\n",
    "\n",
    "dataset_normal.loc[dataset_normal['label'] == 'netflix', 'label'] = 'video'\n",
    "dataset_normal.loc[dataset_normal['label'] == 'youtube', 'label'] = 'video'\n",
    "dataset_normal.loc[dataset_normal['label'] == 'twitch', 'label'] = 'video'\n",
    "dataset_normal.loc[dataset_normal['label'] == 'acestream', 'label'] = 'video'\n",
    "\n",
    "\n",
    "print(dataset_tunnel.shape[0])\n",
    "print(dataset_tunnel['label'].value_counts())\n",
    "print(dataset_normal.shape[0])\n",
    "print(dataset_normal['label'].value_counts())\n",
    "number_training_samples = dataset_normal.shape[0]\n",
    "dataset = dataset_normal.append(dataset_tunnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "browsing = dataset[dataset['label'] == 'browsing']\n",
    "video = dataset[dataset['label'] == 'video']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorize the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    46352\n",
       "1    31941\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevlabel = dataset['label']\n",
    "dataset['label'] = pd.factorize(dataset['label'])[0]\n",
    "labels = dataset['label']\n",
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                          1.000000\n",
       "up_bytes_skew                  0.130569\n",
       "down_bytes_skew                0.120367\n",
       "up_bytes_kurt                  0.117951\n",
       "down_bytes_kurt                0.102173\n",
       "down_packet_skew               0.101037\n",
       "up_packet_skew                 0.092136\n",
       "down_packet_kurt               0.083791\n",
       "up_bytes_silences              0.080436\n",
       "up_packet_silences             0.080436\n",
       "down_bytes_silences            0.076310\n",
       "down_packet_silences           0.076310\n",
       "up_packet_kurt                 0.073049\n",
       "up_bytes_longest_silence      -0.005094\n",
       "up_packet_longest_silence     -0.005094\n",
       "down_bytes_longest_silence    -0.011284\n",
       "down_packet_var               -0.022064\n",
       "up_bytes_shortest_silence     -0.065642\n",
       "up_packet_shortest_silence    -0.065642\n",
       "down_bytes_perc25             -0.066676\n",
       "up_bytes_perc25               -0.074144\n",
       "up_packet_var                 -0.074500\n",
       "down_bytes_shortest_silence   -0.076258\n",
       "up_bytes_var                  -0.078099\n",
       "up_bytes_median               -0.094541\n",
       "up_bytes_perc50               -0.094541\n",
       "down_bytes_median             -0.095187\n",
       "down_bytes_perc50             -0.095187\n",
       "up_bytes_perc75               -0.116847\n",
       "down_bytes_var                -0.116958\n",
       "down_packet_median            -0.119252\n",
       "down_packet_perc50            -0.119252\n",
       "up_bytes_mean                 -0.124064\n",
       "down_packet_perc25            -0.124700\n",
       "down_packet_std               -0.125183\n",
       "up_bytes_perc90               -0.132843\n",
       "down_packet_perc75            -0.132988\n",
       "up_packet_perc25              -0.140366\n",
       "up_packet_perc50              -0.141069\n",
       "up_packet_median              -0.141069\n",
       "up_bytes_std                  -0.141910\n",
       "up_packet_std                 -0.145310\n",
       "down_packet_mean              -0.145424\n",
       "up_packet_perc75              -0.147907\n",
       "down_packet_perc90            -0.148155\n",
       "up_packet_mean                -0.154335\n",
       "up_packet_perc90              -0.155524\n",
       "down_bytes_perc75             -0.157036\n",
       "down_bytes_mean               -0.183272\n",
       "down_bytes_perc90             -0.220364\n",
       "up_bytes_silence_mean         -0.223409\n",
       "down_bytes_silence_mean       -0.236633\n",
       "down_bytes_std                -0.252198\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()['label'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputer for Nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(dataset)\n",
    "dataset = pd.DataFrame(imputer.transform(dataset), columns=dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "labels = dataset['label']\n",
    "dataset_no_label = dataset.loc[:, dataset.columns != 'label']\n",
    "to_scale = dataset_no_label\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(to_scale)\n",
    "\n",
    "dataset = pd.DataFrame(scaled)\n",
    "#joblib.dump(scaler, '../models/std_scaler_' + sampling_interval + '_' + str(sample_size) + '.sav')\n",
    "dataset['label'] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "labels = dataset['label']\n",
    "dataset_no_label = dataset.loc[:, dataset.columns != 'label']\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "main_components = pca.fit_transform(dataset_no_label)\n",
    "dataset = pd.DataFrame(data = main_components)\n",
    "dataset['label'] = labels\n",
    "\n",
    "dataset['label'].isna().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and testing set randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.iloc[:number_training_samples, :]\n",
    "test = dataset.iloc[number_training_samples:, :]\n",
    "\n",
    "y_train = train['label']\n",
    "x_train = train.drop(columns=['label'])\n",
    "\n",
    "y_test = test['label']\n",
    "x_test = test.drop(columns=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video vs. Video\n",
    "y_train_video = (y_train == 0)\n",
    "y_train_browsing = (y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_test_forest(model):\n",
    "    \n",
    "    y_probas_video = cross_val_predict(model, x_train, y_train_video, cv=10, method=\"predict_proba\")\n",
    "    y_probas_browsing = cross_val_predict(model, x_train, y_train_browsing, cv=10, method=\"predict_proba\")\n",
    "    y_scores_video = y_probas_video[:, 1]\n",
    "    y_scores_browsing = y_probas_browsing[:, 1]\n",
    "    \n",
    "    fpr_v, tpr_v, thresholds_y = roc_curve(y_train_video, y_scores_video)\n",
    "    fpr_b, tpr_b, thresholds_n = roc_curve(y_train_browsing, y_scores_browsing)\n",
    "    plot_roc_curve(fpr_v, tpr_v, \"Video\")\n",
    "    plot_roc_curve(fpr_b, tpr_b, \"Browsing\")\n",
    "\n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))    \n",
    "  \n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    \n",
    "    print(conf_mx)\n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def do_test_svm(model):    \n",
    "    \n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))    \n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    \n",
    "    print(conf_mx)\n",
    "    \n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test_forest_ovo(model):\n",
    "\n",
    "    cv_mc = cross_val_score(model, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    print(\"\\tCrossValScore: \" + str(sum(cv_mc)/len(cv_mc)))    \n",
    "  \n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    conf_mx = confusion_matrix(y_test, predictions)\n",
    "    plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "    \n",
    "    print(conf_mx)    \n",
    "    print(\"\\nAccuracy train set: \" + str(sum(model.predict(x_train) == y_train)/float(len(y_train))))\n",
    "    print(\"Accuracy test set: \" + str(sum(predictions == y_test)/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Random Forest------\n",
      "\tCrossValScore: 0.8031581608332001\n",
      "[[17871  5422]\n",
      " [ 6086 11690]]\n",
      "\n",
      "Accuracy train set: 0.888217279174726\n",
      "Accuracy test set: 0.7197886483722515\n",
      "-----OvO Classifier Random Forest------\n",
      "\tCrossValScore: 0.5708314307989382\n",
      "[[ 3393 19900]\n",
      " [   66 17710]]\n",
      "\n",
      "Accuracy train set: 0.6502793896410918\n",
      "Accuracy test set: 0.5138425576468869\n",
      "-----OvR Classifier Random Forest------\n",
      "\tCrossValScore: 0.5708314307989382\n",
      "[[ 3393 19900]\n",
      " [   66 17710]]\n",
      "\n",
      "Accuracy train set: 0.6502793896410918\n",
      "Accuracy test set: 0.5138425576468869\n",
      "----------SVM-----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(\"-----Random Forest------\")\n",
    "rf = RandomForestClassifier(random_state=13)\n",
    "\n",
    "do_test_forest(rf)\n",
    "\n",
    "print(\"-----OvO Classifier Random Forest------\")\n",
    "\n",
    "rf = OneVsOneClassifier(rf)\n",
    "do_test_forest_ovo(rf)\n",
    "\n",
    "print(\"-----OvR Classifier Random Forest------\")\n",
    "\n",
    "rf = OneVsRestClassifier(rf)\n",
    "do_test_forest_ovo(rf)\n",
    "\n",
    "print(\"----------SVM-----------\")\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "do_test_svm(svm)\n",
    "\n",
    "print(\"----------Knn-----------\")\n",
    "knn = KNeighborsClassifier()\n",
    "do_test_svm(knn)\n",
    "\n",
    "print(\"-----Neural Network-----\")\n",
    "nn = MLPClassifier(max_iter=1000)\n",
    "do_test_svm(nn)\n",
    "\n",
    "print(\"--------AdaBoost Random Forest--------\")\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_depth=9,\n",
    "                            max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False,\n",
    "                            random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    rf,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n",
    "\n",
    "print(\"---------Decision Tree------\")\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "do_test_forest(dt)\n",
    "\n",
    "print(\"--------AdaBoost Decision Tree--------\")\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    dt,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "do_test_forest(ada_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
